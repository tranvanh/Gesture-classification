{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caring-species",
   "metadata": {
    "id": "caring-species"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "dir_path = './' #local\n",
    "# dir_path = './drive/MyDrive/BAKA/' #colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "KstfPbJfzIAV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "KstfPbJfzIAV",
    "outputId": "848f73d9-3924-4c3b-8750-969e0d7184c8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>152.028</td>\n",
       "      <td>174.270</td>\n",
       "      <td>114.872</td>\n",
       "      <td>135.012</td>\n",
       "      <td>115.821</td>\n",
       "      <td>169.357</td>\n",
       "      <td>108.251</td>\n",
       "      <td>86.5737</td>\n",
       "      <td>86.4014</td>\n",
       "      <td>60.6628</td>\n",
       "      <td>...</td>\n",
       "      <td>19.15310</td>\n",
       "      <td>37.6444</td>\n",
       "      <td>296.277</td>\n",
       "      <td>5.277580</td>\n",
       "      <td>5.69833</td>\n",
       "      <td>303.563</td>\n",
       "      <td>-4.53744</td>\n",
       "      <td>7.62196</td>\n",
       "      <td>12.33870</td>\n",
       "      <td>35.1608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106.969</td>\n",
       "      <td>174.270</td>\n",
       "      <td>138.093</td>\n",
       "      <td>122.560</td>\n",
       "      <td>112.017</td>\n",
       "      <td>151.941</td>\n",
       "      <td>139.409</td>\n",
       "      <td>67.5126</td>\n",
       "      <td>93.4567</td>\n",
       "      <td>141.7770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.97681</td>\n",
       "      <td>54.9755</td>\n",
       "      <td>295.547</td>\n",
       "      <td>-61.058000</td>\n",
       "      <td>8.98806</td>\n",
       "      <td>317.394</td>\n",
       "      <td>-4.13179</td>\n",
       "      <td>85.07790</td>\n",
       "      <td>5.64757</td>\n",
       "      <td>43.2827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115.931</td>\n",
       "      <td>174.270</td>\n",
       "      <td>114.917</td>\n",
       "      <td>134.286</td>\n",
       "      <td>175.119</td>\n",
       "      <td>127.521</td>\n",
       "      <td>177.996</td>\n",
       "      <td>58.0159</td>\n",
       "      <td>69.8662</td>\n",
       "      <td>178.0840</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.39748</td>\n",
       "      <td>82.3194</td>\n",
       "      <td>336.737</td>\n",
       "      <td>-44.982000</td>\n",
       "      <td>13.94950</td>\n",
       "      <td>337.090</td>\n",
       "      <td>-7.44972</td>\n",
       "      <td>63.64330</td>\n",
       "      <td>21.32290</td>\n",
       "      <td>29.9727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>129.774</td>\n",
       "      <td>90.000</td>\n",
       "      <td>151.379</td>\n",
       "      <td>164.207</td>\n",
       "      <td>147.675</td>\n",
       "      <td>133.193</td>\n",
       "      <td>153.299</td>\n",
       "      <td>53.3455</td>\n",
       "      <td>64.2682</td>\n",
       "      <td>55.6463</td>\n",
       "      <td>...</td>\n",
       "      <td>6.15326</td>\n",
       "      <td>45.3346</td>\n",
       "      <td>348.796</td>\n",
       "      <td>1.379250</td>\n",
       "      <td>19.70390</td>\n",
       "      <td>370.834</td>\n",
       "      <td>-9.76106</td>\n",
       "      <td>19.74900</td>\n",
       "      <td>17.69820</td>\n",
       "      <td>71.4496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>168.765</td>\n",
       "      <td>174.270</td>\n",
       "      <td>156.177</td>\n",
       "      <td>141.628</td>\n",
       "      <td>146.640</td>\n",
       "      <td>128.245</td>\n",
       "      <td>177.981</td>\n",
       "      <td>80.4555</td>\n",
       "      <td>66.8833</td>\n",
       "      <td>71.7721</td>\n",
       "      <td>...</td>\n",
       "      <td>7.91760</td>\n",
       "      <td>45.8886</td>\n",
       "      <td>362.047</td>\n",
       "      <td>13.636300</td>\n",
       "      <td>27.27890</td>\n",
       "      <td>388.642</td>\n",
       "      <td>-8.96836</td>\n",
       "      <td>15.10440</td>\n",
       "      <td>19.16930</td>\n",
       "      <td>97.3733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>155.025</td>\n",
       "      <td>173.721</td>\n",
       "      <td>137.026</td>\n",
       "      <td>141.175</td>\n",
       "      <td>142.413</td>\n",
       "      <td>144.205</td>\n",
       "      <td>171.451</td>\n",
       "      <td>71.6357</td>\n",
       "      <td>76.7435</td>\n",
       "      <td>77.4298</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.88642</td>\n",
       "      <td>50.2178</td>\n",
       "      <td>384.405</td>\n",
       "      <td>1.723980</td>\n",
       "      <td>18.24680</td>\n",
       "      <td>398.022</td>\n",
       "      <td>-10.44550</td>\n",
       "      <td>15.77150</td>\n",
       "      <td>9.59649</td>\n",
       "      <td>110.1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>156.901</td>\n",
       "      <td>173.395</td>\n",
       "      <td>136.497</td>\n",
       "      <td>140.909</td>\n",
       "      <td>141.700</td>\n",
       "      <td>144.869</td>\n",
       "      <td>170.816</td>\n",
       "      <td>71.1895</td>\n",
       "      <td>76.4634</td>\n",
       "      <td>77.2505</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.63851</td>\n",
       "      <td>50.7962</td>\n",
       "      <td>385.537</td>\n",
       "      <td>0.938872</td>\n",
       "      <td>18.37180</td>\n",
       "      <td>398.392</td>\n",
       "      <td>-10.57410</td>\n",
       "      <td>15.82180</td>\n",
       "      <td>9.31662</td>\n",
       "      <td>109.5280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>158.522</td>\n",
       "      <td>173.625</td>\n",
       "      <td>136.929</td>\n",
       "      <td>141.031</td>\n",
       "      <td>141.552</td>\n",
       "      <td>145.678</td>\n",
       "      <td>170.873</td>\n",
       "      <td>71.2914</td>\n",
       "      <td>76.3473</td>\n",
       "      <td>76.4080</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.09323</td>\n",
       "      <td>50.9274</td>\n",
       "      <td>386.211</td>\n",
       "      <td>0.622400</td>\n",
       "      <td>18.48320</td>\n",
       "      <td>398.925</td>\n",
       "      <td>-10.66650</td>\n",
       "      <td>15.72430</td>\n",
       "      <td>9.22443</td>\n",
       "      <td>109.0020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>158.859</td>\n",
       "      <td>174.598</td>\n",
       "      <td>137.177</td>\n",
       "      <td>140.760</td>\n",
       "      <td>141.265</td>\n",
       "      <td>147.142</td>\n",
       "      <td>171.260</td>\n",
       "      <td>71.2260</td>\n",
       "      <td>76.3613</td>\n",
       "      <td>76.1615</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.47433</td>\n",
       "      <td>51.2412</td>\n",
       "      <td>386.845</td>\n",
       "      <td>0.361023</td>\n",
       "      <td>18.55280</td>\n",
       "      <td>399.310</td>\n",
       "      <td>-10.78070</td>\n",
       "      <td>15.59450</td>\n",
       "      <td>8.98942</td>\n",
       "      <td>109.1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>159.968</td>\n",
       "      <td>175.177</td>\n",
       "      <td>138.385</td>\n",
       "      <td>139.917</td>\n",
       "      <td>140.353</td>\n",
       "      <td>148.704</td>\n",
       "      <td>171.572</td>\n",
       "      <td>71.8511</td>\n",
       "      <td>76.6114</td>\n",
       "      <td>75.7387</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.88371</td>\n",
       "      <td>51.7601</td>\n",
       "      <td>387.757</td>\n",
       "      <td>0.181574</td>\n",
       "      <td>18.65410</td>\n",
       "      <td>399.774</td>\n",
       "      <td>-10.76800</td>\n",
       "      <td>15.30470</td>\n",
       "      <td>8.98171</td>\n",
       "      <td>109.3330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1        2        3        4        5        6        7   \\\n",
       "0   152.028  174.270  114.872  135.012  115.821  169.357  108.251  86.5737   \n",
       "1   106.969  174.270  138.093  122.560  112.017  151.941  139.409  67.5126   \n",
       "2   115.931  174.270  114.917  134.286  175.119  127.521  177.996  58.0159   \n",
       "3   129.774   90.000  151.379  164.207  147.675  133.193  153.299  53.3455   \n",
       "4   168.765  174.270  156.177  141.628  146.640  128.245  177.981  80.4555   \n",
       "..      ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "95  155.025  173.721  137.026  141.175  142.413  144.205  171.451  71.6357   \n",
       "96  156.901  173.395  136.497  140.909  141.700  144.869  170.816  71.1895   \n",
       "97  158.522  173.625  136.929  141.031  141.552  145.678  170.873  71.2914   \n",
       "98  158.859  174.598  137.177  140.760  141.265  147.142  171.260  71.2260   \n",
       "99  159.968  175.177  138.385  139.917  140.353  148.704  171.572  71.8511   \n",
       "\n",
       "         8         9   ...        21       22       23         24        25  \\\n",
       "0   86.4014   60.6628  ...  19.15310  37.6444  296.277   5.277580   5.69833   \n",
       "1   93.4567  141.7770  ...   0.97681  54.9755  295.547 -61.058000   8.98806   \n",
       "2   69.8662  178.0840  ...  -2.39748  82.3194  336.737 -44.982000  13.94950   \n",
       "3   64.2682   55.6463  ...   6.15326  45.3346  348.796   1.379250  19.70390   \n",
       "4   66.8833   71.7721  ...   7.91760  45.8886  362.047  13.636300  27.27890   \n",
       "..      ...       ...  ...       ...      ...      ...        ...       ...   \n",
       "95  76.7435   77.4298  ...  -5.88642  50.2178  384.405   1.723980  18.24680   \n",
       "96  76.4634   77.2505  ...  -6.63851  50.7962  385.537   0.938872  18.37180   \n",
       "97  76.3473   76.4080  ...  -7.09323  50.9274  386.211   0.622400  18.48320   \n",
       "98  76.3613   76.1615  ...  -7.47433  51.2412  386.845   0.361023  18.55280   \n",
       "99  76.6114   75.7387  ...  -7.88371  51.7601  387.757   0.181574  18.65410   \n",
       "\n",
       "         26        27        28        29        30  \n",
       "0   303.563  -4.53744   7.62196  12.33870   35.1608  \n",
       "1   317.394  -4.13179  85.07790   5.64757   43.2827  \n",
       "2   337.090  -7.44972  63.64330  21.32290   29.9727  \n",
       "3   370.834  -9.76106  19.74900  17.69820   71.4496  \n",
       "4   388.642  -8.96836  15.10440  19.16930   97.3733  \n",
       "..      ...       ...       ...       ...       ...  \n",
       "95  398.022 -10.44550  15.77150   9.59649  110.1400  \n",
       "96  398.392 -10.57410  15.82180   9.31662  109.5280  \n",
       "97  398.925 -10.66650  15.72430   9.22443  109.0020  \n",
       "98  399.310 -10.78070  15.59450   8.98942  109.1230  \n",
       "99  399.774 -10.76800  15.30470   8.98171  109.3330  \n",
       "\n",
       "[100 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uncomment in colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# pd.read_csv(dir_path + 'DataCollection' + '/' + '1' + '/' + '0.txt', sep=' ', index_col=False )\n",
    "# test = np.genfromtxt(dir_path + 'DataCollection' + '/' + '1' + '/' + '0.txt', delimiter=' ',dtype='float64')\n",
    "df = pd.read_csv(dir_path + 'DataCollection' + '/' + '1' + '/' + '0.txt', header=None, sep=' ')\n",
    "df = df.drop(columns=[31])\n",
    "sp_df = np.array_split(df, 2)\n",
    "sp_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "signal-friendly",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "signal-friendly",
    "outputId": "1a67d662-4d4b-4a31-ea83-913c969c7c85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5', '6']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(dir_path + 'DataCollection')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "annoying-rotation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "annoying-rotation",
    "outputId": "3be4c00e-aa79-4bd7-9613-eab0876051eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301   60\n",
      "256   51\n",
      "151   30\n",
      "205   41\n",
      "211   42\n",
      "151   30\n"
     ]
    }
   ],
   "source": [
    "x_train = [];\n",
    "y_train = [];\n",
    "\n",
    "x_test = [];\n",
    "y_test = [];\n",
    "for i in files:\n",
    "    samples = os.listdir(dir_path + 'DataCollection' + '/' + i)\n",
    "    num_tests = int(len(samples)/5);\n",
    "    shuffle(samples, random_state = 0)\n",
    "    for k in range(0, num_tests):\n",
    "#         df = np.genfromtxt(dir_path + 'DataCollection' + '/' + i + '/' + samples[k], delimiter=' ',dtype='float64')\n",
    "        df = pd.read_csv(dir_path + 'DataCollection' + '/' + i + '/' + samples[k], header=None, sep=' ')\n",
    "        df = df.drop(columns=[31])\n",
    "        sp_df = np.array_split(df, 2)\n",
    "        for p in sp_df:\n",
    "            x_test.append(p.to_numpy())\n",
    "            y_test.append(int(i)-1);\n",
    "    \n",
    "    for k in range(num_tests, len(samples)):\n",
    "        df = pd.read_csv(dir_path + 'DataCollection' + '/' + i + '/' + samples[k], header=None, sep=' ')\n",
    "        df = df.drop(columns=[31])\n",
    "        sp_df = np.array_split(df, 2)\n",
    "        for p in sp_df:\n",
    "            x_train.append(p.to_numpy())\n",
    "            y_train.append(int(i)-1);\n",
    "    \n",
    "    print(len(samples), ' ', num_tests)\n",
    "    \n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train);\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "orange-character",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orange-character",
    "outputId": "34a56bfb-28ab-4998-b919-7fea3f10ab3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape:  (2042, 100, 31)\n",
      "y_train.shiape:  (2042,)\n",
      "x_test.shape:  (508, 100, 31)\n",
      "y_test.shape:  (508,)\n",
      "[[[146.102   174.27    146.022   ...  12.747     8.94849 116.923  ]\n",
      "  [138.963   176.016   145.252   ...  12.831     9.10288 116.791  ]\n",
      "  [133.62    177.874   145.35    ...  13.1807    9.07881 117.362  ]\n",
      "  ...\n",
      "  [159.741   177.26    140.089   ...  12.1147    7.51379 113.455  ]\n",
      "  [159.704   176.964   140.108   ...  12.1307    7.5447  113.331  ]\n",
      "  [159.641   176.734   140.127   ...  12.1575    7.58026 113.334  ]]\n",
      "\n",
      " [[159.564   176.371   140.2     ...  12.1792    7.62375 113.332  ]\n",
      "  [159.478   176.007   140.309   ...  12.1938    7.66359 113.347  ]\n",
      "  [159.383   175.67    140.465   ...  12.2283    7.69677 113.433  ]\n",
      "  ...\n",
      "  [154.893   172.515   141.491   ...  12.4594    7.48152 112.73   ]\n",
      "  [154.995   173.132   141.052   ...  12.4302    7.43961 112.92   ]\n",
      "  [155.21    173.614   140.785   ...  12.4084    7.43464 112.993  ]]\n",
      "\n",
      " [[155.597   173.91    140.623   ...  12.3902    7.44884 113.069  ]\n",
      "  [155.597   172.187   141.973   ...  12.5572    7.3875  113.661  ]\n",
      "  [155.573   172.146   141.634   ...  12.532     7.41295 113.582  ]\n",
      "  ...\n",
      "  [159.163   173.995   142.333   ...  12.1941    8.55053 111.649  ]\n",
      "  [158.675   173.528   142.64    ...  12.1146    8.45592 110.712  ]\n",
      "  [158.465   173.104   142.734   ...  12.0304    8.43064 110.14   ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[130.764   145.29    145.713   ...  16.4954   10.1769   12.723  ]\n",
      "  [129.759   143.478   145.469   ...  16.4864   10.2865   11.2903 ]\n",
      "  [129.033   141.921   145.79    ...  16.4876   10.6516   11.0824 ]\n",
      "  ...\n",
      "  [130.675   142.341   147.169   ...  17.3557   10.6279   13.9696 ]\n",
      "  [127.911   143.005   148.024   ...  17.6812   10.8073   14.008  ]\n",
      "  [126.37    142.875   148.408   ...  17.9431   10.8345   14.3812 ]]\n",
      "\n",
      " [[135.587   144.849   147.129   ...  17.3062   10.4009   13.9678 ]\n",
      "  [131.602   145.625   146.174   ...  17.1368   10.101    14.2142 ]\n",
      "  [130.933   145.549   144.683   ...  16.3264    9.93303  13.1141 ]\n",
      "  ...\n",
      "  [120.025   139.995   145.836   ...  16.2357   11.9964   13.614  ]\n",
      "  [123.586   141.211   147.904   ...  16.1259   12.1735   14.5979 ]\n",
      "  [127.652   141.924   149.655   ...  16.081    12.2517   15.2485 ]]\n",
      "\n",
      " [[129.735   143.368   149.817   ...  15.9779   11.9215   15.1983 ]\n",
      "  [129.317   144.885   149.799   ...  15.8825   11.6983   15.1981 ]\n",
      "  [128.511   145.68    149.79    ...  15.8608   11.5415   15.1726 ]\n",
      "  ...\n",
      "  [121.22    137.197   146.603   ...  16.1493   11.0296   13.9955 ]\n",
      "  [119.35    137.073   146.801   ...  16.2988   11.202    13.728  ]\n",
      "  [117.732   137.235   147.129   ...  16.3687   11.288    13.5605 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train.shape: \", x_train.shape)\n",
    "print(\"y_train.shiape: \", y_train.shape)\n",
    "print(\"x_test.shape: \", x_test.shape)\n",
    "print(\"y_test.shape: \", y_test.shape)\n",
    "\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sensitive-cheat",
   "metadata": {
    "id": "sensitive-cheat"
   },
   "outputs": [],
   "source": [
    "def scale_data(data, min_max_scaler):\n",
    "    for i in range(len(data)):\n",
    "        data[i] = min_max_scaler.transform(data[i])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "broke-sailing",
   "metadata": {
    "id": "broke-sailing"
   },
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "num_instances, num_time_steps, num_features = x_train.shape\n",
    "x_train = np.reshape(x_train, newshape=(-1, num_features))\n",
    "x_train = min_max_scaler.fit_transform(x_train)\n",
    "x_train = np.reshape(x_train, newshape=(num_instances, num_time_steps, num_features))\n",
    "\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=0)\n",
    "\n",
    "num_instances, num_time_steps, num_features = x_test.shape\n",
    "x_test = np.reshape(x_test, newshape=(-1, num_features))\n",
    "x_test = min_max_scaler.transform(x_test)\n",
    "x_test = np.reshape(x_test, newshape=(num_instances, num_time_steps, num_features))\n",
    "\n",
    "x_test, y_test = shuffle(x_test, y_test, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "discrete-kansas",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "discrete-kansas",
    "outputId": "ee604653-f6d3-4637-ab26-a474dcc46389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape:  (1021, 200, 31)\n",
      "y_train.shiape:  (1021,)\n",
      "x_test.shape:  (254, 200, 31)\n",
      "y_test.shape:  (254,)\n"
     ]
    }
   ],
   "source": [
    "# x_train = np.delete(x_train, [0,1,2,3,4,5,6,7,8], 0)\n",
    "# y_train = np.delete(y_train, [0,1,2,3,4,5,6,7,8], 0)\n",
    "print(\"x_train.shape: \", x_train.shape)\n",
    "print(\"y_train.shiape: \", y_train.shape)\n",
    "print(\"x_test.shape: \", x_test.shape)\n",
    "print(\"y_test.shape: \", y_test.shape)\n",
    "# print(y_train)\n",
    "# print(\"\\n\")\n",
    "# print(y_test)\n",
    "\n",
    "# y_train = y_train.astype('int')\n",
    "\n",
    "# print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "senior-mystery",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "senior-mystery",
    "outputId": "f773140a-a48f-4d77-9d32-a44b79f455c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 200, 200)          185600    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 200, 200)          800       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200, 200)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200, 200)          320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 200, 200)          800       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200, 200)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200, 200)          320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 200, 200)          800       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200, 200)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 1206      \n",
      "=================================================================\n",
      "Total params: 1,152,406\n",
      "Trainable params: 1,150,806\n",
      "Non-trainable params: 1,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=100, input_shape=x_train.shape[1:], return_sequences=True,dtype='float64'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(100, return_sequences=True ,dtype='float64'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(100, return_sequences=True ,dtype='float64'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(100,dtype='float64'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(30, activation='softmax'))\n",
    "\n",
    "model.add(Dense(6, activation='softmax',dtype='float64'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "after-sperm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "after-sperm",
    "outputId": "e5a9159f-4d97-4f19-aa6d-eca55fdb18b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "12/12 [==============================] - 48s 1s/step - loss: 1.3952 - accuracy: 0.4987 - val_loss: 1.7343 - val_accuracy: 0.3786\n",
      "Epoch 2/800\n",
      "12/12 [==============================] - 11s 953ms/step - loss: 0.2330 - accuracy: 0.9267 - val_loss: 1.6937 - val_accuracy: 0.3786\n",
      "Epoch 3/800\n",
      "12/12 [==============================] - 11s 958ms/step - loss: 0.1376 - accuracy: 0.9457 - val_loss: 1.6599 - val_accuracy: 0.3398\n",
      "Epoch 4/800\n",
      "12/12 [==============================] - 11s 957ms/step - loss: 0.1135 - accuracy: 0.9653 - val_loss: 1.6251 - val_accuracy: 0.3981\n",
      "Epoch 5/800\n",
      "12/12 [==============================] - 11s 956ms/step - loss: 0.0982 - accuracy: 0.9694 - val_loss: 1.5850 - val_accuracy: 0.4272\n",
      "Epoch 6/800\n",
      "12/12 [==============================] - 11s 959ms/step - loss: 0.0784 - accuracy: 0.9734 - val_loss: 1.5426 - val_accuracy: 0.3981\n",
      "Epoch 7/800\n",
      "12/12 [==============================] - 12s 962ms/step - loss: 0.0590 - accuracy: 0.9739 - val_loss: 1.4921 - val_accuracy: 0.4272\n",
      "Epoch 8/800\n",
      "12/12 [==============================] - 11s 959ms/step - loss: 0.0420 - accuracy: 0.9889 - val_loss: 1.4351 - val_accuracy: 0.4369\n",
      "Epoch 9/800\n",
      "12/12 [==============================] - 11s 955ms/step - loss: 0.0412 - accuracy: 0.9914 - val_loss: 1.3686 - val_accuracy: 0.4951\n",
      "Epoch 10/800\n",
      "12/12 [==============================] - 11s 957ms/step - loss: 0.0359 - accuracy: 0.9889 - val_loss: 1.2733 - val_accuracy: 0.5146\n",
      "Epoch 11/800\n",
      "12/12 [==============================] - 11s 954ms/step - loss: 0.0398 - accuracy: 0.9870 - val_loss: 1.1995 - val_accuracy: 0.5534\n",
      "Epoch 12/800\n",
      "12/12 [==============================] - 11s 955ms/step - loss: 0.0311 - accuracy: 0.9921 - val_loss: 1.0885 - val_accuracy: 0.5728\n",
      "Epoch 13/800\n",
      "12/12 [==============================] - 11s 957ms/step - loss: 0.0300 - accuracy: 0.9912 - val_loss: 0.9822 - val_accuracy: 0.7476\n",
      "Epoch 14/800\n",
      "12/12 [==============================] - 11s 958ms/step - loss: 0.0290 - accuracy: 0.9922 - val_loss: 0.8771 - val_accuracy: 0.7864\n",
      "Epoch 15/800\n",
      "12/12 [==============================] - 11s 957ms/step - loss: 0.0294 - accuracy: 0.9906 - val_loss: 0.7664 - val_accuracy: 0.8350\n",
      "Epoch 16/800\n",
      "12/12 [==============================] - 11s 961ms/step - loss: 0.0361 - accuracy: 0.9871 - val_loss: 0.6288 - val_accuracy: 0.8738\n",
      "Epoch 17/800\n",
      "12/12 [==============================] - 12s 962ms/step - loss: 0.0153 - accuracy: 0.9992 - val_loss: 0.5280 - val_accuracy: 0.8738\n",
      "Epoch 18/800\n",
      "12/12 [==============================] - 12s 964ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.4492 - val_accuracy: 0.8738\n",
      "Epoch 19/800\n",
      "12/12 [==============================] - 12s 963ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.3669 - val_accuracy: 0.9223\n",
      "Epoch 20/800\n",
      "12/12 [==============================] - 12s 964ms/step - loss: 0.0096 - accuracy: 0.9969 - val_loss: 0.3203 - val_accuracy: 0.8932\n",
      "Epoch 21/800\n",
      "12/12 [==============================] - 12s 963ms/step - loss: 0.0164 - accuracy: 0.9972 - val_loss: 0.2925 - val_accuracy: 0.8835\n",
      "Epoch 22/800\n",
      "12/12 [==============================] - 12s 962ms/step - loss: 0.0141 - accuracy: 0.9955 - val_loss: 0.2494 - val_accuracy: 0.9126\n",
      "Epoch 23/800\n",
      "12/12 [==============================] - 12s 964ms/step - loss: 0.0135 - accuracy: 0.9965 - val_loss: 0.1760 - val_accuracy: 0.9417\n",
      "Epoch 24/800\n",
      "12/12 [==============================] - 12s 964ms/step - loss: 0.0109 - accuracy: 0.9971 - val_loss: 0.2723 - val_accuracy: 0.8835\n",
      "Epoch 25/800\n",
      "12/12 [==============================] - 12s 966ms/step - loss: 0.0162 - accuracy: 0.9961 - val_loss: 0.1624 - val_accuracy: 0.9417\n",
      "Epoch 26/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 0.0139 - accuracy: 0.9976 - val_loss: 0.1464 - val_accuracy: 0.9320\n",
      "Epoch 27/800\n",
      "12/12 [==============================] - 12s 973ms/step - loss: 0.0118 - accuracy: 0.9979 - val_loss: 0.1612 - val_accuracy: 0.9320\n",
      "Epoch 28/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 0.0092 - accuracy: 0.9969 - val_loss: 0.1272 - val_accuracy: 0.9515\n",
      "Epoch 29/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 0.0096 - accuracy: 0.9986 - val_loss: 0.1143 - val_accuracy: 0.9515\n",
      "Epoch 30/800\n",
      "12/12 [==============================] - 12s 970ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.1455 - val_accuracy: 0.9417\n",
      "Epoch 31/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 0.0068 - accuracy: 0.9984 - val_loss: 0.1049 - val_accuracy: 0.9515\n",
      "Epoch 32/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 0.0089 - accuracy: 0.9988 - val_loss: 0.0966 - val_accuracy: 0.9515\n",
      "Epoch 33/800\n",
      "12/12 [==============================] - 12s 971ms/step - loss: 0.0069 - accuracy: 0.9983 - val_loss: 0.1369 - val_accuracy: 0.9515\n",
      "Epoch 34/800\n",
      "12/12 [==============================] - 12s 970ms/step - loss: 0.0065 - accuracy: 0.9988 - val_loss: 0.1563 - val_accuracy: 0.9515\n",
      "Epoch 35/800\n",
      "12/12 [==============================] - 12s 967ms/step - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.1179 - val_accuracy: 0.9515\n",
      "Epoch 36/800\n",
      "12/12 [==============================] - 12s 967ms/step - loss: 0.0040 - accuracy: 0.9998 - val_loss: 0.0494 - val_accuracy: 0.9806\n",
      "Epoch 37/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 0.0042 - accuracy: 0.9998 - val_loss: 0.0571 - val_accuracy: 0.9806\n",
      "Epoch 38/800\n",
      "12/12 [==============================] - 12s 966ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0432 - val_accuracy: 0.9806\n",
      "Epoch 39/800\n",
      "12/12 [==============================] - 12s 971ms/step - loss: 0.0098 - accuracy: 0.9955 - val_loss: 0.3589 - val_accuracy: 0.9126\n",
      "Epoch 40/800\n",
      "12/12 [==============================] - 12s 970ms/step - loss: 0.0198 - accuracy: 0.9945 - val_loss: 0.1897 - val_accuracy: 0.9417\n",
      "Epoch 41/800\n",
      "12/12 [==============================] - 12s 970ms/step - loss: 0.0203 - accuracy: 0.9923 - val_loss: 0.4050 - val_accuracy: 0.8932\n",
      "Epoch 42/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 0.0107 - accuracy: 0.9966 - val_loss: 0.1246 - val_accuracy: 0.9320\n",
      "Epoch 43/800\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 0.0132 - accuracy: 0.9961 - val_loss: 0.1136 - val_accuracy: 0.9612\n",
      "Epoch 44/800\n",
      "12/12 [==============================] - 12s 970ms/step - loss: 0.0108 - accuracy: 0.9975 - val_loss: 0.0838 - val_accuracy: 0.9709\n",
      "Epoch 45/800\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 0.0080 - accuracy: 0.9984 - val_loss: 0.0688 - val_accuracy: 0.9612\n",
      "Epoch 46/800\n",
      "12/12 [==============================] - 12s 971ms/step - loss: 0.0061 - accuracy: 0.9995 - val_loss: 0.0641 - val_accuracy: 0.9806\n",
      "Epoch 47/800\n",
      "12/12 [==============================] - 12s 971ms/step - loss: 0.0058 - accuracy: 0.9966 - val_loss: 0.0679 - val_accuracy: 0.9709\n",
      "Epoch 48/800\n",
      "12/12 [==============================] - 12s 975ms/step - loss: 0.0051 - accuracy: 0.9987 - val_loss: 0.0767 - val_accuracy: 0.9709\n",
      "Epoch 49/800\n",
      "12/12 [==============================] - 12s 965ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0769 - val_accuracy: 0.9709\n",
      "Epoch 50/800\n",
      "12/12 [==============================] - 12s 966ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0513 - val_accuracy: 0.9806\n",
      "Epoch 51/800\n",
      "12/12 [==============================] - 12s 966ms/step - loss: 0.0044 - accuracy: 0.9982 - val_loss: 0.0644 - val_accuracy: 0.9709\n",
      "Epoch 52/800\n",
      "12/12 [==============================] - 12s 966ms/step - loss: 0.0043 - accuracy: 0.9998 - val_loss: 0.0937 - val_accuracy: 0.9806\n",
      "Epoch 53/800\n",
      "12/12 [==============================] - 12s 965ms/step - loss: 0.0231 - accuracy: 0.9954 - val_loss: 0.1058 - val_accuracy: 0.9612\n",
      "Epoch 54/800\n",
      "12/12 [==============================] - 12s 965ms/step - loss: 0.0114 - accuracy: 0.9962 - val_loss: 0.0877 - val_accuracy: 0.9709\n",
      "Epoch 55/800\n",
      "12/12 [==============================] - 12s 966ms/step - loss: 0.0131 - accuracy: 0.9965 - val_loss: 0.0772 - val_accuracy: 0.9709\n",
      "Epoch 56/800\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 0.0128 - accuracy: 0.9969 - val_loss: 0.1099 - val_accuracy: 0.9612\n",
      "Epoch 57/800\n",
      "12/12 [==============================] - 12s 966ms/step - loss: 0.0156 - accuracy: 0.9963 - val_loss: 0.0654 - val_accuracy: 0.9612\n",
      "Epoch 58/800\n",
      "12/12 [==============================] - 12s 967ms/step - loss: 0.0197 - accuracy: 0.9918 - val_loss: 0.1477 - val_accuracy: 0.9515\n",
      "Epoch 59/800\n",
      "12/12 [==============================] - 12s 965ms/step - loss: 0.0224 - accuracy: 0.9927 - val_loss: 0.1449 - val_accuracy: 0.9515\n",
      "Epoch 60/800\n",
      "12/12 [==============================] - 12s 967ms/step - loss: 0.0271 - accuracy: 0.9952 - val_loss: 0.0860 - val_accuracy: 0.9709\n",
      "Epoch 61/800\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 0.0131 - accuracy: 0.9966 - val_loss: 0.1017 - val_accuracy: 0.9709\n",
      "Epoch 62/800\n",
      "12/12 [==============================] - 12s 971ms/step - loss: 0.0134 - accuracy: 0.9947 - val_loss: 0.1192 - val_accuracy: 0.9709\n",
      "Epoch 63/800\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 0.0124 - accuracy: 0.9970 - val_loss: 0.1084 - val_accuracy: 0.9806\n",
      "Epoch 64/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 0.0135 - accuracy: 0.9961 - val_loss: 0.1091 - val_accuracy: 0.9806\n",
      "Epoch 65/800\n",
      "12/12 [==============================] - 12s 966ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.1091 - val_accuracy: 0.9806\n",
      "Epoch 66/800\n",
      "12/12 [==============================] - 12s 967ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.1215 - val_accuracy: 0.9806\n",
      "Epoch 67/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.1190 - val_accuracy: 0.9806\n",
      "Epoch 68/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.1097 - val_accuracy: 0.9806\n",
      "Epoch 69/800\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1014 - val_accuracy: 0.9806\n",
      "Epoch 70/800\n",
      "12/12 [==============================] - 12s 970ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0940 - val_accuracy: 0.9806\n",
      "Epoch 71/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0882 - val_accuracy: 0.9806\n",
      "Epoch 72/800\n",
      "12/12 [==============================] - 12s 973ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9515\n",
      "Epoch 73/800\n",
      "12/12 [==============================] - 12s 967ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0858 - val_accuracy: 0.9612\n",
      "Epoch 74/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0898 - val_accuracy: 0.9709\n",
      "Epoch 75/800\n",
      "12/12 [==============================] - 12s 970ms/step - loss: 0.0048 - accuracy: 0.9969 - val_loss: 0.0919 - val_accuracy: 0.9709\n",
      "Epoch 76/800\n",
      "12/12 [==============================] - 12s 970ms/step - loss: 6.1543e-04 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9709\n",
      "Epoch 77/800\n",
      "12/12 [==============================] - 12s 967ms/step - loss: 0.0035 - accuracy: 0.9969 - val_loss: 0.1082 - val_accuracy: 0.9709\n",
      "Epoch 78/800\n",
      "12/12 [==============================] - 12s 970ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.1215 - val_accuracy: 0.9515\n",
      "Epoch 79/800\n",
      "12/12 [==============================] - 12s 971ms/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 0.0869 - val_accuracy: 0.9515\n",
      "Epoch 80/800\n",
      "12/12 [==============================] - 12s 971ms/step - loss: 0.0077 - accuracy: 0.9979 - val_loss: 0.0947 - val_accuracy: 0.9709\n",
      "Epoch 81/800\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 0.0156 - accuracy: 0.9984 - val_loss: 0.0636 - val_accuracy: 0.9612\n",
      "Epoch 82/800\n",
      "12/12 [==============================] - 12s 972ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0676 - val_accuracy: 0.9612\n",
      "Epoch 83/800\n",
      "12/12 [==============================] - 12s 971ms/step - loss: 0.0265 - accuracy: 0.9957 - val_loss: 0.1188 - val_accuracy: 0.9515\n",
      "Epoch 84/800\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 0.0059 - accuracy: 0.9993 - val_loss: 0.0985 - val_accuracy: 0.9709\n",
      "Epoch 85/800\n",
      "12/12 [==============================] - 12s 967ms/step - loss: 0.0032 - accuracy: 0.9998 - val_loss: 0.1098 - val_accuracy: 0.9709\n",
      "Epoch 86/800\n",
      "12/12 [==============================] - 12s 974ms/step - loss: 0.0088 - accuracy: 0.9979 - val_loss: 0.0455 - val_accuracy: 0.9806\n",
      "Epoch 87/800\n",
      "12/12 [==============================] - 12s 966ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0356 - val_accuracy: 0.9806\n",
      "Epoch 88/800\n",
      "12/12 [==============================] - 12s 967ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0293 - val_accuracy: 0.9806\n",
      "Epoch 89/800\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.2315 - val_accuracy: 0.9320\n",
      "Epoch 90/800\n",
      "12/12 [==============================] - 12s 967ms/step - loss: 0.0116 - accuracy: 0.9939 - val_loss: 0.0444 - val_accuracy: 0.9903\n",
      "Epoch 91/800\n",
      "12/12 [==============================] - 12s 972ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 0.0608 - val_accuracy: 0.9903\n",
      "Epoch 92/800\n",
      "12/12 [==============================] - 12s 967ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.1023 - val_accuracy: 0.9709\n",
      "Epoch 93/800\n",
      "12/12 [==============================] - 12s 966ms/step - loss: 0.0107 - accuracy: 0.9908 - val_loss: 0.0731 - val_accuracy: 0.9709\n",
      "Epoch 94/800\n",
      "12/12 [==============================] - 12s 973ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0839 - val_accuracy: 0.9806\n",
      "Epoch 95/800\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0871 - val_accuracy: 0.9806\n",
      "Epoch 96/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 0.0048 - accuracy: 0.9984 - val_loss: 0.0853 - val_accuracy: 0.9709\n",
      "Epoch 97/800\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0482 - val_accuracy: 0.9709\n",
      "Epoch 98/800\n",
      "12/12 [==============================] - 12s 971ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.1034 - val_accuracy: 0.9709\n",
      "Epoch 99/800\n",
      "12/12 [==============================] - 12s 966ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0655 - val_accuracy: 0.9612\n",
      "Epoch 100/800\n",
      "12/12 [==============================] - 12s 965ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.0486 - val_accuracy: 0.9806\n",
      "Epoch 101/800\n",
      "12/12 [==============================] - 12s 973ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.1083 - val_accuracy: 0.9709\n",
      "Epoch 102/800\n",
      "12/12 [==============================] - 12s 967ms/step - loss: 8.4734e-04 - accuracy: 1.0000 - val_loss: 0.1328 - val_accuracy: 0.9709\n",
      "Epoch 103/800\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 5.7755e-04 - accuracy: 1.0000 - val_loss: 0.1273 - val_accuracy: 0.9709\n",
      "Epoch 104/800\n",
      "12/12 [==============================] - 12s 972ms/step - loss: 7.2499e-04 - accuracy: 0.9998 - val_loss: 0.1217 - val_accuracy: 0.9709\n",
      "Epoch 105/800\n",
      "12/12 [==============================] - 12s 971ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.1466 - val_accuracy: 0.9612\n",
      "Epoch 106/800\n",
      "12/12 [==============================] - 12s 971ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.1374 - val_accuracy: 0.9515\n",
      "Epoch 107/800\n",
      "12/12 [==============================] - 12s 967ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.1602 - val_accuracy: 0.9417\n",
      "Epoch 108/800\n",
      "12/12 [==============================] - 12s 967ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1222 - val_accuracy: 0.9417\n",
      "Epoch 109/800\n",
      "12/12 [==============================] - 12s 967ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.0701 - val_accuracy: 0.9806\n",
      "Epoch 110/800\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.1562 - val_accuracy: 0.9417\n",
      "Epoch 111/800\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0609 - val_accuracy: 0.9709\n",
      "Epoch 112/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 0.0164 - accuracy: 0.9950 - val_loss: 0.5066 - val_accuracy: 0.8835\n",
      "Epoch 113/800\n",
      "12/12 [==============================] - 12s 971ms/step - loss: 0.0415 - accuracy: 0.9899 - val_loss: 0.2889 - val_accuracy: 0.9029\n",
      "Epoch 114/800\n",
      "12/12 [==============================] - 12s 974ms/step - loss: 0.0643 - accuracy: 0.9810 - val_loss: 0.2240 - val_accuracy: 0.9126\n",
      "Epoch 115/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 0.0190 - accuracy: 0.9946 - val_loss: 0.2798 - val_accuracy: 0.9223\n",
      "Epoch 116/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 0.0197 - accuracy: 0.9945 - val_loss: 0.1324 - val_accuracy: 0.9223\n",
      "Epoch 117/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 0.0107 - accuracy: 0.9972 - val_loss: 0.1056 - val_accuracy: 0.9515\n",
      "Epoch 118/800\n",
      "12/12 [==============================] - 12s 970ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.3133 - val_accuracy: 0.9223\n",
      "Epoch 119/800\n",
      "12/12 [==============================] - 12s 972ms/step - loss: 0.0031 - accuracy: 0.9997 - val_loss: 0.1655 - val_accuracy: 0.9612\n",
      "Epoch 120/800\n",
      "12/12 [==============================] - 12s 973ms/step - loss: 0.0040 - accuracy: 0.9983 - val_loss: 0.0830 - val_accuracy: 0.9709\n",
      "Epoch 121/800\n",
      "12/12 [==============================] - 12s 971ms/step - loss: 0.0116 - accuracy: 0.9973 - val_loss: 0.0336 - val_accuracy: 0.9806\n",
      "Epoch 122/800\n",
      "12/12 [==============================] - 12s 971ms/step - loss: 0.0145 - accuracy: 0.9966 - val_loss: 0.0938 - val_accuracy: 0.9417\n",
      "Epoch 123/800\n",
      "12/12 [==============================] - 12s 971ms/step - loss: 0.0081 - accuracy: 0.9982 - val_loss: 0.0614 - val_accuracy: 0.9709\n",
      "Epoch 124/800\n",
      "12/12 [==============================] - 12s 971ms/step - loss: 0.0140 - accuracy: 0.9957 - val_loss: 0.1017 - val_accuracy: 0.9709\n",
      "Epoch 125/800\n",
      "12/12 [==============================] - 12s 970ms/step - loss: 0.0053 - accuracy: 0.9969 - val_loss: 0.0669 - val_accuracy: 0.9709\n",
      "Epoch 126/800\n",
      "12/12 [==============================] - 12s 970ms/step - loss: 0.0040 - accuracy: 0.9997 - val_loss: 0.0555 - val_accuracy: 0.9806\n",
      "Epoch 127/800\n",
      "12/12 [==============================] - 12s 970ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0421 - val_accuracy: 0.9806\n",
      "Epoch 128/800\n",
      "12/12 [==============================] - 12s 974ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0344 - val_accuracy: 0.9903\n",
      "Epoch 129/800\n",
      "12/12 [==============================] - 12s 972ms/step - loss: 0.0027 - accuracy: 0.9989 - val_loss: 0.0351 - val_accuracy: 0.9903\n",
      "Epoch 130/800\n",
      "12/12 [==============================] - 12s 974ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.1032 - val_accuracy: 0.9709\n",
      "Epoch 131/800\n",
      "12/12 [==============================] - 12s 972ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.3032 - val_accuracy: 0.9417\n",
      "Epoch 132/800\n",
      "12/12 [==============================] - 12s 975ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0389 - val_accuracy: 0.9903\n",
      "Epoch 133/800\n",
      "12/12 [==============================] - 12s 970ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0710 - val_accuracy: 0.9806\n",
      "Epoch 134/800\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 9.6677e-04 - accuracy: 1.0000 - val_loss: 0.0822 - val_accuracy: 0.9806\n",
      "Epoch 135/800\n",
      "12/12 [==============================] - 12s 973ms/step - loss: 8.7863e-04 - accuracy: 1.0000 - val_loss: 0.0775 - val_accuracy: 0.9806\n",
      "Epoch 136/800\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0595 - val_accuracy: 0.9806\n",
      "Epoch 137/800\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 0.0045 - accuracy: 0.9978 - val_loss: 0.0432 - val_accuracy: 0.9903\n",
      "Epoch 138/800\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 8.3764e-04 - accuracy: 1.0000 - val_loss: 0.0343 - val_accuracy: 0.9806\n",
      "Epoch 139/800\n",
      "12/12 [==============================] - 12s 972ms/step - loss: 5.7687e-04 - accuracy: 1.0000 - val_loss: 0.0314 - val_accuracy: 0.9806\n",
      "Epoch 140/800\n",
      "12/12 [==============================] - 12s 970ms/step - loss: 7.4254e-04 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 0.9903\n",
      "Epoch 141/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 5.9853e-04 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 0.9806\n",
      "Epoch 142/800\n",
      "12/12 [==============================] - 12s 974ms/step - loss: 6.0533e-04 - accuracy: 1.0000 - val_loss: 0.0305 - val_accuracy: 0.9806\n",
      "Epoch 143/800\n",
      "12/12 [==============================] - 12s 972ms/step - loss: 9.8525e-04 - accuracy: 1.0000 - val_loss: 0.0209 - val_accuracy: 0.9903\n",
      "Epoch 144/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 5.6819e-04 - accuracy: 1.0000 - val_loss: 0.0191 - val_accuracy: 0.9903\n",
      "Epoch 145/800\n",
      "12/12 [==============================] - 12s 972ms/step - loss: 6.8568e-04 - accuracy: 1.0000 - val_loss: 0.0191 - val_accuracy: 0.9903\n",
      "Epoch 146/800\n",
      "12/12 [==============================] - 12s 972ms/step - loss: 3.4004e-04 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 0.9903\n",
      "Epoch 147/800\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 4.7506e-04 - accuracy: 1.0000 - val_loss: 0.0186 - val_accuracy: 0.9903\n",
      "Epoch 148/800\n",
      "12/12 [==============================] - 12s 967ms/step - loss: 3.6941e-04 - accuracy: 1.0000 - val_loss: 0.0184 - val_accuracy: 0.9903\n",
      "Epoch 149/800\n",
      "12/12 [==============================] - 12s 966ms/step - loss: 4.8374e-04 - accuracy: 1.0000 - val_loss: 0.0186 - val_accuracy: 0.9903\n",
      "Epoch 150/800\n",
      "12/12 [==============================] - 12s 968ms/step - loss: 3.7290e-04 - accuracy: 1.0000 - val_loss: 0.0191 - val_accuracy: 0.9903\n",
      "Epoch 151/800\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 2.1695e-04 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-92aabf036113>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m          )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.0001, decay=1e-5)\n",
    "\n",
    "checkpoint_filepath = 'Checkpoints'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "\n",
    "gestures = model.fit(x = x_train,\n",
    "            y = y_train,\n",
    "            epochs=800,\n",
    "            validation_split=0.1, #split 10% of the trainning set for the validation set,\n",
    "            batch_size=80,\n",
    "            callbacks=[model_checkpoint_callback],\n",
    "            shuffle=True\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KikcQb9ZpuLB",
   "metadata": {
    "id": "KikcQb9ZpuLB"
   },
   "outputs": [],
   "source": [
    "# model.save(dir_path + 'Models/gestures.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "integrated-optimization",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "integrated-optimization",
    "outputId": "ec3d1e3d-cabb-478a-c1d4-4a419c75e202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "4/4 [==============================] - 1s 300ms/step - loss: 0.1531 - accuracy: 0.9567\n",
      "test loss, test acc: [0.15312451124191284, 0.9566929340362549]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./drive/MyDrive/BAKA/Models/gestures_custom/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./drive/MyDrive/BAKA/Models/gestures_custom/assets\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(x_test, y_test, batch_size=80)\n",
    "print(\"test loss, test acc:\", results)\n",
    "model.save(dir_path + 'Models/gestures_custom', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hFsQ_tEXw8WX",
   "metadata": {
    "id": "hFsQ_tEXw8WX"
   },
   "outputs": [],
   "source": [
    "# new_model = load_model(dir_path + 'Models/gestures.h5')\n",
    "# print(\"Evaluate on test data\")\n",
    "# results = new_model.evaluate(x_test, y_test, batch_size=80)\n",
    "# print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faWZ2lOQ09Dj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "faWZ2lOQ09Dj",
    "outputId": "a145642e-9a8e-4526-da62-2d20490ca314"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.30245e+01,  8.99999e+01,  9.00456e+01,  9.00455e+01,\n",
       "        9.00455e+01,  6.54812e+01,  2.57840e+01,  2.77705e+01,\n",
       "        2.79286e+01,  2.04904e+01, -4.24721e+02,  6.97324e+01,\n",
       "       -2.97270e+02, -4.20951e+02,  5.95510e+01, -3.15328e+02,\n",
       "       -4.48325e+02,  6.33254e+01, -3.17749e+02, -4.46258e+02,\n",
       "        8.67906e+01, -3.13954e+02, -4.57886e+02,  8.58241e+01,\n",
       "       -3.02162e+02, -4.42757e+02,  1.03148e+02, -2.82183e+02,\n",
       "        1.86895e-01,  6.48663e-02,  1.07620e-01])"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler.data_min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NOWq0Fev1RI-",
   "metadata": {
    "id": "NOWq0Fev1RI-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "feature_computation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
