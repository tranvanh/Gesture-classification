{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "feature_computation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "caring-species"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
        "# dir_path = './' #local\n",
        "dir_path = './drive/MyDrive/BAKA/' #colab"
      ],
      "id": "caring-species",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "KstfPbJfzIAV",
        "outputId": "848f73d9-3924-4c3b-8750-969e0d7184c8"
      },
      "source": [
        "#uncomment in colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# pd.read_csv(dir_path + 'DataCollection' + '/' + '1' + '/' + '0.txt', sep=' ', index_col=False )\n",
        "# test = np.genfromtxt(dir_path + 'DataCollection' + '/' + '1' + '/' + '0.txt', delimiter=' ',dtype='float64')\n",
        "df = pd.read_csv(dir_path + 'DataCollection' + '/' + '1' + '/' + '0.txt', header=None, sep=' ')\n",
        "df = df.drop(columns=[31])\n",
        "df"
      ],
      "id": "KstfPbJfzIAV",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>152.028</td>\n",
              "      <td>174.270</td>\n",
              "      <td>114.872</td>\n",
              "      <td>135.012</td>\n",
              "      <td>115.821</td>\n",
              "      <td>169.357</td>\n",
              "      <td>108.251</td>\n",
              "      <td>86.5737</td>\n",
              "      <td>86.4014</td>\n",
              "      <td>60.6628</td>\n",
              "      <td>7.49661</td>\n",
              "      <td>368.805</td>\n",
              "      <td>-11.39230</td>\n",
              "      <td>35.4778</td>\n",
              "      <td>372.833</td>\n",
              "      <td>-21.5318</td>\n",
              "      <td>39.6854</td>\n",
              "      <td>348.592</td>\n",
              "      <td>7.46019</td>\n",
              "      <td>38.71440</td>\n",
              "      <td>326.577</td>\n",
              "      <td>19.15310</td>\n",
              "      <td>37.64440</td>\n",
              "      <td>296.277</td>\n",
              "      <td>5.27758</td>\n",
              "      <td>5.69833</td>\n",
              "      <td>303.563</td>\n",
              "      <td>-4.53744</td>\n",
              "      <td>7.62196</td>\n",
              "      <td>12.33870</td>\n",
              "      <td>35.1608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>106.969</td>\n",
              "      <td>174.270</td>\n",
              "      <td>138.093</td>\n",
              "      <td>122.560</td>\n",
              "      <td>112.017</td>\n",
              "      <td>151.941</td>\n",
              "      <td>139.409</td>\n",
              "      <td>67.5126</td>\n",
              "      <td>93.4567</td>\n",
              "      <td>141.7770</td>\n",
              "      <td>25.46860</td>\n",
              "      <td>371.098</td>\n",
              "      <td>-13.88750</td>\n",
              "      <td>43.5763</td>\n",
              "      <td>381.204</td>\n",
              "      <td>-39.2998</td>\n",
              "      <td>39.0602</td>\n",
              "      <td>356.814</td>\n",
              "      <td>-7.43494</td>\n",
              "      <td>49.22350</td>\n",
              "      <td>343.591</td>\n",
              "      <td>0.97681</td>\n",
              "      <td>54.97550</td>\n",
              "      <td>295.547</td>\n",
              "      <td>-61.05800</td>\n",
              "      <td>8.98806</td>\n",
              "      <td>317.394</td>\n",
              "      <td>-4.13179</td>\n",
              "      <td>85.07790</td>\n",
              "      <td>5.64757</td>\n",
              "      <td>43.2827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>115.931</td>\n",
              "      <td>174.270</td>\n",
              "      <td>114.917</td>\n",
              "      <td>134.286</td>\n",
              "      <td>175.119</td>\n",
              "      <td>127.521</td>\n",
              "      <td>177.996</td>\n",
              "      <td>58.0159</td>\n",
              "      <td>69.8662</td>\n",
              "      <td>178.0840</td>\n",
              "      <td>49.53920</td>\n",
              "      <td>383.612</td>\n",
              "      <td>-5.43277</td>\n",
              "      <td>56.8145</td>\n",
              "      <td>405.251</td>\n",
              "      <td>-43.7347</td>\n",
              "      <td>35.5285</td>\n",
              "      <td>371.161</td>\n",
              "      <td>-20.33810</td>\n",
              "      <td>34.98540</td>\n",
              "      <td>355.458</td>\n",
              "      <td>-2.39748</td>\n",
              "      <td>82.31940</td>\n",
              "      <td>336.737</td>\n",
              "      <td>-44.98200</td>\n",
              "      <td>13.94950</td>\n",
              "      <td>337.090</td>\n",
              "      <td>-7.44972</td>\n",
              "      <td>63.64330</td>\n",
              "      <td>21.32290</td>\n",
              "      <td>29.9727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>129.774</td>\n",
              "      <td>90.000</td>\n",
              "      <td>151.379</td>\n",
              "      <td>164.207</td>\n",
              "      <td>147.675</td>\n",
              "      <td>133.193</td>\n",
              "      <td>153.299</td>\n",
              "      <td>53.3455</td>\n",
              "      <td>64.2682</td>\n",
              "      <td>55.6463</td>\n",
              "      <td>68.55800</td>\n",
              "      <td>397.308</td>\n",
              "      <td>-25.93310</td>\n",
              "      <td>57.0065</td>\n",
              "      <td>390.412</td>\n",
              "      <td>-74.1331</td>\n",
              "      <td>42.2847</td>\n",
              "      <td>387.940</td>\n",
              "      <td>-17.66460</td>\n",
              "      <td>43.71540</td>\n",
              "      <td>368.581</td>\n",
              "      <td>6.15326</td>\n",
              "      <td>45.33460</td>\n",
              "      <td>348.796</td>\n",
              "      <td>1.37925</td>\n",
              "      <td>19.70390</td>\n",
              "      <td>370.834</td>\n",
              "      <td>-9.76106</td>\n",
              "      <td>19.74900</td>\n",
              "      <td>17.69820</td>\n",
              "      <td>71.4496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>168.765</td>\n",
              "      <td>174.270</td>\n",
              "      <td>156.177</td>\n",
              "      <td>141.628</td>\n",
              "      <td>146.640</td>\n",
              "      <td>128.245</td>\n",
              "      <td>177.981</td>\n",
              "      <td>80.4555</td>\n",
              "      <td>66.8833</td>\n",
              "      <td>71.7721</td>\n",
              "      <td>72.51100</td>\n",
              "      <td>366.007</td>\n",
              "      <td>-19.21010</td>\n",
              "      <td>35.8622</td>\n",
              "      <td>344.209</td>\n",
              "      <td>-96.2058</td>\n",
              "      <td>65.4289</td>\n",
              "      <td>368.487</td>\n",
              "      <td>-5.98384</td>\n",
              "      <td>51.30090</td>\n",
              "      <td>373.351</td>\n",
              "      <td>7.91760</td>\n",
              "      <td>45.88860</td>\n",
              "      <td>362.047</td>\n",
              "      <td>13.63630</td>\n",
              "      <td>27.27890</td>\n",
              "      <td>388.642</td>\n",
              "      <td>-8.96836</td>\n",
              "      <td>15.10440</td>\n",
              "      <td>19.16930</td>\n",
              "      <td>97.3733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>149.508</td>\n",
              "      <td>178.660</td>\n",
              "      <td>144.652</td>\n",
              "      <td>141.589</td>\n",
              "      <td>141.211</td>\n",
              "      <td>140.317</td>\n",
              "      <td>173.599</td>\n",
              "      <td>76.2446</td>\n",
              "      <td>75.0304</td>\n",
              "      <td>72.7944</td>\n",
              "      <td>3.43306</td>\n",
              "      <td>473.408</td>\n",
              "      <td>-46.58480</td>\n",
              "      <td>25.7845</td>\n",
              "      <td>466.134</td>\n",
              "      <td>-126.5530</td>\n",
              "      <td>14.1716</td>\n",
              "      <td>497.456</td>\n",
              "      <td>-28.98440</td>\n",
              "      <td>8.21895</td>\n",
              "      <td>510.987</td>\n",
              "      <td>-30.11830</td>\n",
              "      <td>2.87239</td>\n",
              "      <td>522.783</td>\n",
              "      <td>-37.79160</td>\n",
              "      <td>36.79610</td>\n",
              "      <td>518.710</td>\n",
              "      <td>-48.87630</td>\n",
              "      <td>12.26400</td>\n",
              "      <td>7.59416</td>\n",
              "      <td>117.8630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>149.347</td>\n",
              "      <td>178.758</td>\n",
              "      <td>145.400</td>\n",
              "      <td>142.273</td>\n",
              "      <td>141.907</td>\n",
              "      <td>139.853</td>\n",
              "      <td>173.076</td>\n",
              "      <td>75.6861</td>\n",
              "      <td>74.6439</td>\n",
              "      <td>72.5005</td>\n",
              "      <td>3.29962</td>\n",
              "      <td>472.204</td>\n",
              "      <td>-45.58130</td>\n",
              "      <td>25.2251</td>\n",
              "      <td>464.618</td>\n",
              "      <td>-126.2930</td>\n",
              "      <td>15.1793</td>\n",
              "      <td>496.000</td>\n",
              "      <td>-28.42360</td>\n",
              "      <td>8.89380</td>\n",
              "      <td>509.273</td>\n",
              "      <td>-29.44780</td>\n",
              "      <td>3.12718</td>\n",
              "      <td>520.981</td>\n",
              "      <td>-36.96540</td>\n",
              "      <td>36.67920</td>\n",
              "      <td>517.278</td>\n",
              "      <td>-48.70080</td>\n",
              "      <td>12.25370</td>\n",
              "      <td>7.67346</td>\n",
              "      <td>118.6160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>148.335</td>\n",
              "      <td>178.866</td>\n",
              "      <td>146.606</td>\n",
              "      <td>143.288</td>\n",
              "      <td>142.679</td>\n",
              "      <td>139.443</td>\n",
              "      <td>172.818</td>\n",
              "      <td>75.3177</td>\n",
              "      <td>74.4949</td>\n",
              "      <td>72.3500</td>\n",
              "      <td>3.41626</td>\n",
              "      <td>470.354</td>\n",
              "      <td>-44.91860</td>\n",
              "      <td>24.3535</td>\n",
              "      <td>463.122</td>\n",
              "      <td>-126.3610</td>\n",
              "      <td>16.1741</td>\n",
              "      <td>493.829</td>\n",
              "      <td>-27.85840</td>\n",
              "      <td>9.43385</td>\n",
              "      <td>506.701</td>\n",
              "      <td>-28.72460</td>\n",
              "      <td>3.23291</td>\n",
              "      <td>518.414</td>\n",
              "      <td>-36.05660</td>\n",
              "      <td>36.29120</td>\n",
              "      <td>515.412</td>\n",
              "      <td>-48.58660</td>\n",
              "      <td>12.25620</td>\n",
              "      <td>7.78125</td>\n",
              "      <td>119.4990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>146.453</td>\n",
              "      <td>178.656</td>\n",
              "      <td>149.390</td>\n",
              "      <td>144.159</td>\n",
              "      <td>143.569</td>\n",
              "      <td>137.445</td>\n",
              "      <td>173.159</td>\n",
              "      <td>74.5098</td>\n",
              "      <td>73.8029</td>\n",
              "      <td>71.7551</td>\n",
              "      <td>3.72750</td>\n",
              "      <td>468.922</td>\n",
              "      <td>-42.89510</td>\n",
              "      <td>22.5524</td>\n",
              "      <td>461.831</td>\n",
              "      <td>-125.2760</td>\n",
              "      <td>18.1151</td>\n",
              "      <td>491.755</td>\n",
              "      <td>-27.40510</td>\n",
              "      <td>10.54130</td>\n",
              "      <td>504.270</td>\n",
              "      <td>-28.28420</td>\n",
              "      <td>3.47881</td>\n",
              "      <td>515.864</td>\n",
              "      <td>-35.43990</td>\n",
              "      <td>35.86260</td>\n",
              "      <td>513.897</td>\n",
              "      <td>-48.75530</td>\n",
              "      <td>12.21440</td>\n",
              "      <td>7.67485</td>\n",
              "      <td>119.8710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>145.719</td>\n",
              "      <td>174.270</td>\n",
              "      <td>150.966</td>\n",
              "      <td>144.930</td>\n",
              "      <td>145.441</td>\n",
              "      <td>135.482</td>\n",
              "      <td>174.064</td>\n",
              "      <td>74.0723</td>\n",
              "      <td>73.4106</td>\n",
              "      <td>71.6242</td>\n",
              "      <td>4.02111</td>\n",
              "      <td>467.512</td>\n",
              "      <td>-41.63760</td>\n",
              "      <td>20.4298</td>\n",
              "      <td>460.548</td>\n",
              "      <td>-122.0890</td>\n",
              "      <td>19.8947</td>\n",
              "      <td>489.747</td>\n",
              "      <td>-27.14730</td>\n",
              "      <td>11.52460</td>\n",
              "      <td>501.977</td>\n",
              "      <td>-27.88550</td>\n",
              "      <td>3.64512</td>\n",
              "      <td>513.367</td>\n",
              "      <td>-34.69660</td>\n",
              "      <td>35.40760</td>\n",
              "      <td>512.290</td>\n",
              "      <td>-49.05100</td>\n",
              "      <td>12.03370</td>\n",
              "      <td>7.47839</td>\n",
              "      <td>118.8100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0        1        2        3   ...        27        28        29        30\n",
              "0    152.028  174.270  114.872  135.012  ...  -4.53744   7.62196  12.33870   35.1608\n",
              "1    106.969  174.270  138.093  122.560  ...  -4.13179  85.07790   5.64757   43.2827\n",
              "2    115.931  174.270  114.917  134.286  ...  -7.44972  63.64330  21.32290   29.9727\n",
              "3    129.774   90.000  151.379  164.207  ...  -9.76106  19.74900  17.69820   71.4496\n",
              "4    168.765  174.270  156.177  141.628  ...  -8.96836  15.10440  19.16930   97.3733\n",
              "..       ...      ...      ...      ...  ...       ...       ...       ...       ...\n",
              "195  149.508  178.660  144.652  141.589  ... -48.87630  12.26400   7.59416  117.8630\n",
              "196  149.347  178.758  145.400  142.273  ... -48.70080  12.25370   7.67346  118.6160\n",
              "197  148.335  178.866  146.606  143.288  ... -48.58660  12.25620   7.78125  119.4990\n",
              "198  146.453  178.656  149.390  144.159  ... -48.75530  12.21440   7.67485  119.8710\n",
              "199  145.719  174.270  150.966  144.930  ... -49.05100  12.03370   7.47839  118.8100\n",
              "\n",
              "[200 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "signal-friendly",
        "outputId": "1a67d662-4d4b-4a31-ea83-913c969c7c85"
      },
      "source": [
        "files = os.listdir(dir_path + 'DataCollection')\n",
        "files"
      ],
      "id": "signal-friendly",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['4', '2', '6', '1', '3', '5']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "annoying-rotation",
        "outputId": "3be4c00e-aa79-4bd7-9613-eab0876051eb"
      },
      "source": [
        "x_train = [];\n",
        "y_train = [];\n",
        "\n",
        "x_test = [];\n",
        "y_test = [];\n",
        "for i in files:\n",
        "    samples = os.listdir(dir_path + 'DataCollection' + '/' + i)\n",
        "    num_tests = int(len(samples)/5);\n",
        "    shuffle(samples, random_state = 0)\n",
        "    for k in range(0, num_tests):\n",
        "#         df = np.genfromtxt(dir_path + 'DataCollection' + '/' + i + '/' + samples[k], delimiter=' ',dtype='float64')\n",
        "        df = pd.read_csv(dir_path + 'DataCollection' + '/' + i + '/' + samples[k], header=None, sep=' ')\n",
        "        df = df.drop(columns=[31])\n",
        "        x_test.append(df.to_numpy())\n",
        "        y_test.append(int(i)-1);\n",
        "    \n",
        "    for k in range(num_tests, len(samples)):\n",
        "        df = pd.read_csv(dir_path + 'DataCollection' + '/' + i + '/' + samples[k], header=None, sep=' ')\n",
        "        df = df.drop(columns=[31])\n",
        "        x_train.append(df.to_numpy())\n",
        "        y_train.append(int(i)-1);\n",
        "    \n",
        "    print(len(samples), ' ', num_tests)\n",
        "    \n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train);\n",
        "\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)"
      ],
      "id": "annoying-rotation",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "205   41\n",
            "256   51\n",
            "151   30\n",
            "301   60\n",
            "151   30\n",
            "211   42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orange-character",
        "outputId": "34a56bfb-28ab-4998-b919-7fea3f10ab3f"
      },
      "source": [
        "print(\"x_train.shape: \", x_train.shape)\n",
        "print(\"y_train.shiape: \", y_train.shape)\n",
        "print(\"x_test.shape: \", x_test.shape)\n",
        "print(\"y_test.shape: \", y_test.shape)\n",
        "\n",
        "print(x_train)"
      ],
      "id": "orange-character",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train.shape:  (1021, 200, 31)\n",
            "y_train.shiape:  (1021,)\n",
            "x_test.shape:  (254, 200, 31)\n",
            "y_test.shape:  (254,)\n",
            "[[[140.263   155.047   153.056   ...  14.6581   10.9596   14.1146 ]\n",
            "  [139.707   154.679   150.799   ...  14.6448   11.2233   14.2539 ]\n",
            "  [140.28    153.484   149.955   ...  14.4216   10.9631   14.1897 ]\n",
            "  ...\n",
            "  [146.804   158.72    155.316   ...  21.6282   10.9693   17.1487 ]\n",
            "  [146.373   159.741   156.251   ...  22.2424   11.1586   17.4078 ]\n",
            "  [145.241   159.593   156.114   ...  23.4668   11.1882   17.5774 ]]\n",
            "\n",
            " [[143.97    157.465   154.8     ...  25.9021   11.1725   17.761  ]\n",
            "  [143.636   157.388   153.438   ...  26.4988   11.1284   17.8446 ]\n",
            "  [143.836   155.892   152.227   ...  26.6327   11.2525   17.7005 ]\n",
            "  ...\n",
            "  [179.259   168.682   167.151   ...  14.1408   11.9837   13.8259 ]\n",
            "  [176.149   168.91    173.358   ...  13.8633   12.3902   13.2978 ]\n",
            "  [172.239   169.42    166.983   ...  14.2964   12.968    13.6028 ]]\n",
            "\n",
            " [[167.026   168.398   163.434   ...  13.772    12.8347   14.8215 ]\n",
            "  [165.254   168.8     154.895   ...  14.2889   12.935    15.7255 ]\n",
            "  [166.054   167.467   151.778   ...  14.7054   13.0573   16.189  ]\n",
            "  ...\n",
            "  [142.266   156.508   166.476   ...  15.1926   11.2145   14.252  ]\n",
            "  [142.74    153.548   158.711   ...  15.3999   11.1679   14.0193 ]\n",
            "  [142.985   154.291   157.163   ...  15.4999   10.9774   14.0364 ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[179.75    170.062   172.222   ...  10.3865    6.07239   3.981  ]\n",
            "  [175.099   169.708   171.691   ...  10.4356    6.53144   4.01029]\n",
            "  [171.384   168.975   170.763   ...  10.2467    7.33261   4.17367]\n",
            "  ...\n",
            "  [172.26    174.889   175.682   ...   9.44867   1.06714   5.07699]\n",
            "  [170.843   174.485   175.759   ...   9.42756   1.20039   5.34008]\n",
            "  [170.142   174.473   175.767   ...   9.01688   1.43542   5.57482]]\n",
            "\n",
            " [[170.592   175.127   174.967   ...   8.8523    1.98709   5.67655]\n",
            "  [170.67    175.586   174.569   ...   8.51609   2.3963    5.46202]\n",
            "  [170.404   175.474   174.376   ...   8.64098   3.22045   5.17215]\n",
            "  ...\n",
            "  [153.715   138.334   165.612   ...   8.13938   7.28259  24.8031 ]\n",
            "  [153.429   139.772   166.306   ...   8.08961   7.17376  24.4588 ]\n",
            "  [153.282   141.834   166.545   ...   8.10696   7.09793  23.9074 ]]\n",
            "\n",
            " [[153.306   143.946   166.993   ...   8.12842   7.05895  23.3505 ]\n",
            "  [153.377   146.593   167.269   ...   8.22326   7.04578  23.1529 ]\n",
            "  [153.373   147.997   167.72    ...   8.21357   7.06461  23.167  ]\n",
            "  ...\n",
            "  [151.872   158.771   167.294   ...   7.5887   11.8086   16.2757 ]\n",
            "  [147.088   160.236   161.896   ...   7.34709  11.9984   16.5872 ]\n",
            "  [144.85    161.92    164.024   ...   7.31403  11.918    16.2273 ]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sensitive-cheat"
      },
      "source": [
        "def scale_data(data, min_max_scaler):\n",
        "    for i in range(len(data)):\n",
        "        data[i] = min_max_scaler.transform(data[i])\n",
        "    return data"
      ],
      "id": "sensitive-cheat",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "broke-sailing"
      },
      "source": [
        "min_max_scaler = MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "num_instances, num_time_steps, num_features = x_train.shape\n",
        "x_train = np.reshape(x_train, newshape=(-1, num_features))\n",
        "x_train = min_max_scaler.fit_transform(x_train)\n",
        "x_train = np.reshape(x_train, newshape=(num_instances, num_time_steps, num_features))\n",
        "\n",
        "x_train, y_train = shuffle(x_train, y_train, random_state=0)\n",
        "\n",
        "num_instances, num_time_steps, num_features = x_test.shape\n",
        "x_test = np.reshape(x_test, newshape=(-1, num_features))\n",
        "x_test = min_max_scaler.transform(x_test)\n",
        "x_test = np.reshape(x_test, newshape=(num_instances, num_time_steps, num_features))\n",
        "\n",
        "x_test, y_test = shuffle(x_test, y_test, random_state=0)\n"
      ],
      "id": "broke-sailing",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "discrete-kansas",
        "outputId": "ee604653-f6d3-4637-ab26-a474dcc46389"
      },
      "source": [
        "# x_train = np.delete(x_train, [0,1,2,3,4,5,6,7,8], 0)\n",
        "# y_train = np.delete(y_train, [0,1,2,3,4,5,6,7,8], 0)\n",
        "print(\"x_train.shape: \", x_train.shape)\n",
        "print(\"y_train.shiape: \", y_train.shape)\n",
        "print(\"x_test.shape: \", x_test.shape)\n",
        "print(\"y_test.shape: \", y_test.shape)\n",
        "# print(y_train)\n",
        "# print(\"\\n\")\n",
        "# print(y_test)\n",
        "\n",
        "# y_train = y_train.astype('int')\n",
        "\n",
        "# print(y_train)\n"
      ],
      "id": "discrete-kansas",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train.shape:  (1021, 200, 31)\n",
            "y_train.shiape:  (1021,)\n",
            "x_test.shape:  (254, 200, 31)\n",
            "y_test.shape:  (254,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "senior-mystery",
        "outputId": "f773140a-a48f-4d77-9d32-a44b79f455c4"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(units=200, input_shape=x_train.shape[1:], return_sequences=True,dtype='float64'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(200, return_sequences=True ,dtype='float64'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(200, return_sequences=True ,dtype='float64'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(200,dtype='float64'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Dense(30, activation='softmax'))\n",
        "\n",
        "model.add(Dense(6, activation='softmax',dtype='float64'))\n",
        "model.summary()"
      ],
      "id": "senior-mystery",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 200, 200)          185600    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 200, 200)          800       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 200, 200)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 200, 200)          320800    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 200, 200)          800       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 200, 200)          0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 200, 200)          320800    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 200, 200)          800       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 200, 200)          0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 200)               320800    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 200)               800       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 1206      \n",
            "=================================================================\n",
            "Total params: 1,152,406\n",
            "Trainable params: 1,150,806\n",
            "Non-trainable params: 1,600\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "after-sperm",
        "outputId": "e5a9159f-4d97-4f19-aa6d-eca55fdb18b7"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(lr=0.0001)\n",
        "\n",
        "checkpoint_filepath = 'Checkpoints/'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "\n",
        "gestures = model.fit(x = x_train,\n",
        "            y = y_train,\n",
        "            epochs=800,\n",
        "            validation_split=0.1, #split 10% of the trainning set for the validation set,\n",
        "            batch_size=80,\n",
        "            callbacks=[model_checkpoint_callback],\n",
        "            shuffle=True\n",
        "         )"
      ],
      "id": "after-sperm",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/800\n",
            "12/12 [==============================] - 48s 1s/step - loss: 1.3952 - accuracy: 0.4987 - val_loss: 1.7343 - val_accuracy: 0.3786\n",
            "Epoch 2/800\n",
            "12/12 [==============================] - 11s 953ms/step - loss: 0.2330 - accuracy: 0.9267 - val_loss: 1.6937 - val_accuracy: 0.3786\n",
            "Epoch 3/800\n",
            "12/12 [==============================] - 11s 958ms/step - loss: 0.1376 - accuracy: 0.9457 - val_loss: 1.6599 - val_accuracy: 0.3398\n",
            "Epoch 4/800\n",
            "12/12 [==============================] - 11s 957ms/step - loss: 0.1135 - accuracy: 0.9653 - val_loss: 1.6251 - val_accuracy: 0.3981\n",
            "Epoch 5/800\n",
            "12/12 [==============================] - 11s 956ms/step - loss: 0.0982 - accuracy: 0.9694 - val_loss: 1.5850 - val_accuracy: 0.4272\n",
            "Epoch 6/800\n",
            "12/12 [==============================] - 11s 959ms/step - loss: 0.0784 - accuracy: 0.9734 - val_loss: 1.5426 - val_accuracy: 0.3981\n",
            "Epoch 7/800\n",
            "12/12 [==============================] - 12s 962ms/step - loss: 0.0590 - accuracy: 0.9739 - val_loss: 1.4921 - val_accuracy: 0.4272\n",
            "Epoch 8/800\n",
            "12/12 [==============================] - 11s 959ms/step - loss: 0.0420 - accuracy: 0.9889 - val_loss: 1.4351 - val_accuracy: 0.4369\n",
            "Epoch 9/800\n",
            "12/12 [==============================] - 11s 955ms/step - loss: 0.0412 - accuracy: 0.9914 - val_loss: 1.3686 - val_accuracy: 0.4951\n",
            "Epoch 10/800\n",
            "12/12 [==============================] - 11s 957ms/step - loss: 0.0359 - accuracy: 0.9889 - val_loss: 1.2733 - val_accuracy: 0.5146\n",
            "Epoch 11/800\n",
            "12/12 [==============================] - 11s 954ms/step - loss: 0.0398 - accuracy: 0.9870 - val_loss: 1.1995 - val_accuracy: 0.5534\n",
            "Epoch 12/800\n",
            "12/12 [==============================] - 11s 955ms/step - loss: 0.0311 - accuracy: 0.9921 - val_loss: 1.0885 - val_accuracy: 0.5728\n",
            "Epoch 13/800\n",
            "12/12 [==============================] - 11s 957ms/step - loss: 0.0300 - accuracy: 0.9912 - val_loss: 0.9822 - val_accuracy: 0.7476\n",
            "Epoch 14/800\n",
            "12/12 [==============================] - 11s 958ms/step - loss: 0.0290 - accuracy: 0.9922 - val_loss: 0.8771 - val_accuracy: 0.7864\n",
            "Epoch 15/800\n",
            "12/12 [==============================] - 11s 957ms/step - loss: 0.0294 - accuracy: 0.9906 - val_loss: 0.7664 - val_accuracy: 0.8350\n",
            "Epoch 16/800\n",
            "12/12 [==============================] - 11s 961ms/step - loss: 0.0361 - accuracy: 0.9871 - val_loss: 0.6288 - val_accuracy: 0.8738\n",
            "Epoch 17/800\n",
            "12/12 [==============================] - 12s 962ms/step - loss: 0.0153 - accuracy: 0.9992 - val_loss: 0.5280 - val_accuracy: 0.8738\n",
            "Epoch 18/800\n",
            "12/12 [==============================] - 12s 964ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.4492 - val_accuracy: 0.8738\n",
            "Epoch 19/800\n",
            "12/12 [==============================] - 12s 963ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.3669 - val_accuracy: 0.9223\n",
            "Epoch 20/800\n",
            "12/12 [==============================] - 12s 964ms/step - loss: 0.0096 - accuracy: 0.9969 - val_loss: 0.3203 - val_accuracy: 0.8932\n",
            "Epoch 21/800\n",
            "12/12 [==============================] - 12s 963ms/step - loss: 0.0164 - accuracy: 0.9972 - val_loss: 0.2925 - val_accuracy: 0.8835\n",
            "Epoch 22/800\n",
            "12/12 [==============================] - 12s 962ms/step - loss: 0.0141 - accuracy: 0.9955 - val_loss: 0.2494 - val_accuracy: 0.9126\n",
            "Epoch 23/800\n",
            "12/12 [==============================] - 12s 964ms/step - loss: 0.0135 - accuracy: 0.9965 - val_loss: 0.1760 - val_accuracy: 0.9417\n",
            "Epoch 24/800\n",
            "12/12 [==============================] - 12s 964ms/step - loss: 0.0109 - accuracy: 0.9971 - val_loss: 0.2723 - val_accuracy: 0.8835\n",
            "Epoch 25/800\n",
            "12/12 [==============================] - 12s 966ms/step - loss: 0.0162 - accuracy: 0.9961 - val_loss: 0.1624 - val_accuracy: 0.9417\n",
            "Epoch 26/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 0.0139 - accuracy: 0.9976 - val_loss: 0.1464 - val_accuracy: 0.9320\n",
            "Epoch 27/800\n",
            "12/12 [==============================] - 12s 973ms/step - loss: 0.0118 - accuracy: 0.9979 - val_loss: 0.1612 - val_accuracy: 0.9320\n",
            "Epoch 28/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 0.0092 - accuracy: 0.9969 - val_loss: 0.1272 - val_accuracy: 0.9515\n",
            "Epoch 29/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 0.0096 - accuracy: 0.9986 - val_loss: 0.1143 - val_accuracy: 0.9515\n",
            "Epoch 30/800\n",
            "12/12 [==============================] - 12s 970ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.1455 - val_accuracy: 0.9417\n",
            "Epoch 31/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 0.0068 - accuracy: 0.9984 - val_loss: 0.1049 - val_accuracy: 0.9515\n",
            "Epoch 32/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 0.0089 - accuracy: 0.9988 - val_loss: 0.0966 - val_accuracy: 0.9515\n",
            "Epoch 33/800\n",
            "12/12 [==============================] - 12s 971ms/step - loss: 0.0069 - accuracy: 0.9983 - val_loss: 0.1369 - val_accuracy: 0.9515\n",
            "Epoch 34/800\n",
            "12/12 [==============================] - 12s 970ms/step - loss: 0.0065 - accuracy: 0.9988 - val_loss: 0.1563 - val_accuracy: 0.9515\n",
            "Epoch 35/800\n",
            "12/12 [==============================] - 12s 967ms/step - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.1179 - val_accuracy: 0.9515\n",
            "Epoch 36/800\n",
            "12/12 [==============================] - 12s 967ms/step - loss: 0.0040 - accuracy: 0.9998 - val_loss: 0.0494 - val_accuracy: 0.9806\n",
            "Epoch 37/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 0.0042 - accuracy: 0.9998 - val_loss: 0.0571 - val_accuracy: 0.9806\n",
            "Epoch 38/800\n",
            "12/12 [==============================] - 12s 966ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0432 - val_accuracy: 0.9806\n",
            "Epoch 39/800\n",
            "12/12 [==============================] - 12s 971ms/step - loss: 0.0098 - accuracy: 0.9955 - val_loss: 0.3589 - val_accuracy: 0.9126\n",
            "Epoch 40/800\n",
            "12/12 [==============================] - 12s 970ms/step - loss: 0.0198 - accuracy: 0.9945 - val_loss: 0.1897 - val_accuracy: 0.9417\n",
            "Epoch 41/800\n",
            "12/12 [==============================] - 12s 970ms/step - loss: 0.0203 - accuracy: 0.9923 - val_loss: 0.4050 - val_accuracy: 0.8932\n",
            "Epoch 42/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 0.0107 - accuracy: 0.9966 - val_loss: 0.1246 - val_accuracy: 0.9320\n",
            "Epoch 43/800\n",
            "12/12 [==============================] - 12s 969ms/step - loss: 0.0132 - accuracy: 0.9961 - val_loss: 0.1136 - val_accuracy: 0.9612\n",
            "Epoch 44/800\n",
            "12/12 [==============================] - 12s 970ms/step - loss: 0.0108 - accuracy: 0.9975 - val_loss: 0.0838 - val_accuracy: 0.9709\n",
            "Epoch 45/800\n",
            "12/12 [==============================] - 12s 969ms/step - loss: 0.0080 - accuracy: 0.9984 - val_loss: 0.0688 - val_accuracy: 0.9612\n",
            "Epoch 46/800\n",
            "12/12 [==============================] - 12s 971ms/step - loss: 0.0061 - accuracy: 0.9995 - val_loss: 0.0641 - val_accuracy: 0.9806\n",
            "Epoch 47/800\n",
            "12/12 [==============================] - 12s 971ms/step - loss: 0.0058 - accuracy: 0.9966 - val_loss: 0.0679 - val_accuracy: 0.9709\n",
            "Epoch 48/800\n",
            "12/12 [==============================] - 12s 975ms/step - loss: 0.0051 - accuracy: 0.9987 - val_loss: 0.0767 - val_accuracy: 0.9709\n",
            "Epoch 49/800\n",
            "12/12 [==============================] - 12s 965ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0769 - val_accuracy: 0.9709\n",
            "Epoch 50/800\n",
            "12/12 [==============================] - 12s 966ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0513 - val_accuracy: 0.9806\n",
            "Epoch 51/800\n",
            "12/12 [==============================] - 12s 966ms/step - loss: 0.0044 - accuracy: 0.9982 - val_loss: 0.0644 - val_accuracy: 0.9709\n",
            "Epoch 52/800\n",
            "12/12 [==============================] - 12s 966ms/step - loss: 0.0043 - accuracy: 0.9998 - val_loss: 0.0937 - val_accuracy: 0.9806\n",
            "Epoch 53/800\n",
            "12/12 [==============================] - 12s 965ms/step - loss: 0.0231 - accuracy: 0.9954 - val_loss: 0.1058 - val_accuracy: 0.9612\n",
            "Epoch 54/800\n",
            "12/12 [==============================] - 12s 965ms/step - loss: 0.0114 - accuracy: 0.9962 - val_loss: 0.0877 - val_accuracy: 0.9709\n",
            "Epoch 55/800\n",
            "12/12 [==============================] - 12s 966ms/step - loss: 0.0131 - accuracy: 0.9965 - val_loss: 0.0772 - val_accuracy: 0.9709\n",
            "Epoch 56/800\n",
            "12/12 [==============================] - 12s 969ms/step - loss: 0.0128 - accuracy: 0.9969 - val_loss: 0.1099 - val_accuracy: 0.9612\n",
            "Epoch 57/800\n",
            "12/12 [==============================] - 12s 966ms/step - loss: 0.0156 - accuracy: 0.9963 - val_loss: 0.0654 - val_accuracy: 0.9612\n",
            "Epoch 58/800\n",
            "12/12 [==============================] - 12s 967ms/step - loss: 0.0197 - accuracy: 0.9918 - val_loss: 0.1477 - val_accuracy: 0.9515\n",
            "Epoch 59/800\n",
            "12/12 [==============================] - 12s 965ms/step - loss: 0.0224 - accuracy: 0.9927 - val_loss: 0.1449 - val_accuracy: 0.9515\n",
            "Epoch 60/800\n",
            "12/12 [==============================] - 12s 967ms/step - loss: 0.0271 - accuracy: 0.9952 - val_loss: 0.0860 - val_accuracy: 0.9709\n",
            "Epoch 61/800\n",
            "12/12 [==============================] - 12s 969ms/step - loss: 0.0131 - accuracy: 0.9966 - val_loss: 0.1017 - val_accuracy: 0.9709\n",
            "Epoch 62/800\n",
            "12/12 [==============================] - 12s 971ms/step - loss: 0.0134 - accuracy: 0.9947 - val_loss: 0.1192 - val_accuracy: 0.9709\n",
            "Epoch 63/800\n",
            "12/12 [==============================] - 12s 969ms/step - loss: 0.0124 - accuracy: 0.9970 - val_loss: 0.1084 - val_accuracy: 0.9806\n",
            "Epoch 64/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 0.0135 - accuracy: 0.9961 - val_loss: 0.1091 - val_accuracy: 0.9806\n",
            "Epoch 65/800\n",
            "12/12 [==============================] - 12s 966ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.1091 - val_accuracy: 0.9806\n",
            "Epoch 66/800\n",
            "12/12 [==============================] - 12s 967ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.1215 - val_accuracy: 0.9806\n",
            "Epoch 67/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.1190 - val_accuracy: 0.9806\n",
            "Epoch 68/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.1097 - val_accuracy: 0.9806\n",
            "Epoch 69/800\n",
            "12/12 [==============================] - 12s 969ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1014 - val_accuracy: 0.9806\n",
            "Epoch 70/800\n",
            "12/12 [==============================] - 12s 970ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0940 - val_accuracy: 0.9806\n",
            "Epoch 71/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0882 - val_accuracy: 0.9806\n",
            "Epoch 72/800\n",
            "12/12 [==============================] - 12s 973ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9515\n",
            "Epoch 73/800\n",
            "12/12 [==============================] - 12s 967ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0858 - val_accuracy: 0.9612\n",
            "Epoch 74/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0898 - val_accuracy: 0.9709\n",
            "Epoch 75/800\n",
            "12/12 [==============================] - 12s 970ms/step - loss: 0.0048 - accuracy: 0.9969 - val_loss: 0.0919 - val_accuracy: 0.9709\n",
            "Epoch 76/800\n",
            "12/12 [==============================] - 12s 970ms/step - loss: 6.1543e-04 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9709\n",
            "Epoch 77/800\n",
            "12/12 [==============================] - 12s 967ms/step - loss: 0.0035 - accuracy: 0.9969 - val_loss: 0.1082 - val_accuracy: 0.9709\n",
            "Epoch 78/800\n",
            "12/12 [==============================] - 12s 970ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.1215 - val_accuracy: 0.9515\n",
            "Epoch 79/800\n",
            "12/12 [==============================] - 12s 971ms/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 0.0869 - val_accuracy: 0.9515\n",
            "Epoch 80/800\n",
            "12/12 [==============================] - 12s 971ms/step - loss: 0.0077 - accuracy: 0.9979 - val_loss: 0.0947 - val_accuracy: 0.9709\n",
            "Epoch 81/800\n",
            "12/12 [==============================] - 12s 969ms/step - loss: 0.0156 - accuracy: 0.9984 - val_loss: 0.0636 - val_accuracy: 0.9612\n",
            "Epoch 82/800\n",
            "12/12 [==============================] - 12s 972ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0676 - val_accuracy: 0.9612\n",
            "Epoch 83/800\n",
            "12/12 [==============================] - 12s 971ms/step - loss: 0.0265 - accuracy: 0.9957 - val_loss: 0.1188 - val_accuracy: 0.9515\n",
            "Epoch 84/800\n",
            "12/12 [==============================] - 12s 969ms/step - loss: 0.0059 - accuracy: 0.9993 - val_loss: 0.0985 - val_accuracy: 0.9709\n",
            "Epoch 85/800\n",
            "12/12 [==============================] - 12s 967ms/step - loss: 0.0032 - accuracy: 0.9998 - val_loss: 0.1098 - val_accuracy: 0.9709\n",
            "Epoch 86/800\n",
            "12/12 [==============================] - 12s 974ms/step - loss: 0.0088 - accuracy: 0.9979 - val_loss: 0.0455 - val_accuracy: 0.9806\n",
            "Epoch 87/800\n",
            "12/12 [==============================] - 12s 966ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0356 - val_accuracy: 0.9806\n",
            "Epoch 88/800\n",
            "12/12 [==============================] - 12s 967ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0293 - val_accuracy: 0.9806\n",
            "Epoch 89/800\n",
            "12/12 [==============================] - 12s 969ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.2315 - val_accuracy: 0.9320\n",
            "Epoch 90/800\n",
            "12/12 [==============================] - 12s 967ms/step - loss: 0.0116 - accuracy: 0.9939 - val_loss: 0.0444 - val_accuracy: 0.9903\n",
            "Epoch 91/800\n",
            "12/12 [==============================] - 12s 972ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 0.0608 - val_accuracy: 0.9903\n",
            "Epoch 92/800\n",
            "12/12 [==============================] - 12s 967ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.1023 - val_accuracy: 0.9709\n",
            "Epoch 93/800\n",
            "12/12 [==============================] - 12s 966ms/step - loss: 0.0107 - accuracy: 0.9908 - val_loss: 0.0731 - val_accuracy: 0.9709\n",
            "Epoch 94/800\n",
            "12/12 [==============================] - 12s 973ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0839 - val_accuracy: 0.9806\n",
            "Epoch 95/800\n",
            "12/12 [==============================] - 12s 969ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0871 - val_accuracy: 0.9806\n",
            "Epoch 96/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 0.0048 - accuracy: 0.9984 - val_loss: 0.0853 - val_accuracy: 0.9709\n",
            "Epoch 97/800\n",
            "12/12 [==============================] - 12s 969ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0482 - val_accuracy: 0.9709\n",
            "Epoch 98/800\n",
            "12/12 [==============================] - 12s 971ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.1034 - val_accuracy: 0.9709\n",
            "Epoch 99/800\n",
            "12/12 [==============================] - 12s 966ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0655 - val_accuracy: 0.9612\n",
            "Epoch 100/800\n",
            "12/12 [==============================] - 12s 965ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.0486 - val_accuracy: 0.9806\n",
            "Epoch 101/800\n",
            "12/12 [==============================] - 12s 973ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.1083 - val_accuracy: 0.9709\n",
            "Epoch 102/800\n",
            "12/12 [==============================] - 12s 967ms/step - loss: 8.4734e-04 - accuracy: 1.0000 - val_loss: 0.1328 - val_accuracy: 0.9709\n",
            "Epoch 103/800\n",
            "12/12 [==============================] - 12s 969ms/step - loss: 5.7755e-04 - accuracy: 1.0000 - val_loss: 0.1273 - val_accuracy: 0.9709\n",
            "Epoch 104/800\n",
            "12/12 [==============================] - 12s 972ms/step - loss: 7.2499e-04 - accuracy: 0.9998 - val_loss: 0.1217 - val_accuracy: 0.9709\n",
            "Epoch 105/800\n",
            "12/12 [==============================] - 12s 971ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.1466 - val_accuracy: 0.9612\n",
            "Epoch 106/800\n",
            "12/12 [==============================] - 12s 971ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.1374 - val_accuracy: 0.9515\n",
            "Epoch 107/800\n",
            "12/12 [==============================] - 12s 967ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.1602 - val_accuracy: 0.9417\n",
            "Epoch 108/800\n",
            "12/12 [==============================] - 12s 967ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1222 - val_accuracy: 0.9417\n",
            "Epoch 109/800\n",
            "12/12 [==============================] - 12s 967ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.0701 - val_accuracy: 0.9806\n",
            "Epoch 110/800\n",
            "12/12 [==============================] - 12s 969ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.1562 - val_accuracy: 0.9417\n",
            "Epoch 111/800\n",
            "12/12 [==============================] - 12s 969ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0609 - val_accuracy: 0.9709\n",
            "Epoch 112/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 0.0164 - accuracy: 0.9950 - val_loss: 0.5066 - val_accuracy: 0.8835\n",
            "Epoch 113/800\n",
            "12/12 [==============================] - 12s 971ms/step - loss: 0.0415 - accuracy: 0.9899 - val_loss: 0.2889 - val_accuracy: 0.9029\n",
            "Epoch 114/800\n",
            "12/12 [==============================] - 12s 974ms/step - loss: 0.0643 - accuracy: 0.9810 - val_loss: 0.2240 - val_accuracy: 0.9126\n",
            "Epoch 115/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 0.0190 - accuracy: 0.9946 - val_loss: 0.2798 - val_accuracy: 0.9223\n",
            "Epoch 116/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 0.0197 - accuracy: 0.9945 - val_loss: 0.1324 - val_accuracy: 0.9223\n",
            "Epoch 117/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 0.0107 - accuracy: 0.9972 - val_loss: 0.1056 - val_accuracy: 0.9515\n",
            "Epoch 118/800\n",
            "12/12 [==============================] - 12s 970ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.3133 - val_accuracy: 0.9223\n",
            "Epoch 119/800\n",
            "12/12 [==============================] - 12s 972ms/step - loss: 0.0031 - accuracy: 0.9997 - val_loss: 0.1655 - val_accuracy: 0.9612\n",
            "Epoch 120/800\n",
            "12/12 [==============================] - 12s 973ms/step - loss: 0.0040 - accuracy: 0.9983 - val_loss: 0.0830 - val_accuracy: 0.9709\n",
            "Epoch 121/800\n",
            "12/12 [==============================] - 12s 971ms/step - loss: 0.0116 - accuracy: 0.9973 - val_loss: 0.0336 - val_accuracy: 0.9806\n",
            "Epoch 122/800\n",
            "12/12 [==============================] - 12s 971ms/step - loss: 0.0145 - accuracy: 0.9966 - val_loss: 0.0938 - val_accuracy: 0.9417\n",
            "Epoch 123/800\n",
            "12/12 [==============================] - 12s 971ms/step - loss: 0.0081 - accuracy: 0.9982 - val_loss: 0.0614 - val_accuracy: 0.9709\n",
            "Epoch 124/800\n",
            "12/12 [==============================] - 12s 971ms/step - loss: 0.0140 - accuracy: 0.9957 - val_loss: 0.1017 - val_accuracy: 0.9709\n",
            "Epoch 125/800\n",
            "12/12 [==============================] - 12s 970ms/step - loss: 0.0053 - accuracy: 0.9969 - val_loss: 0.0669 - val_accuracy: 0.9709\n",
            "Epoch 126/800\n",
            "12/12 [==============================] - 12s 970ms/step - loss: 0.0040 - accuracy: 0.9997 - val_loss: 0.0555 - val_accuracy: 0.9806\n",
            "Epoch 127/800\n",
            "12/12 [==============================] - 12s 970ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0421 - val_accuracy: 0.9806\n",
            "Epoch 128/800\n",
            "12/12 [==============================] - 12s 974ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0344 - val_accuracy: 0.9903\n",
            "Epoch 129/800\n",
            "12/12 [==============================] - 12s 972ms/step - loss: 0.0027 - accuracy: 0.9989 - val_loss: 0.0351 - val_accuracy: 0.9903\n",
            "Epoch 130/800\n",
            "12/12 [==============================] - 12s 974ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.1032 - val_accuracy: 0.9709\n",
            "Epoch 131/800\n",
            "12/12 [==============================] - 12s 972ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.3032 - val_accuracy: 0.9417\n",
            "Epoch 132/800\n",
            "12/12 [==============================] - 12s 975ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0389 - val_accuracy: 0.9903\n",
            "Epoch 133/800\n",
            "12/12 [==============================] - 12s 970ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0710 - val_accuracy: 0.9806\n",
            "Epoch 134/800\n",
            "12/12 [==============================] - 12s 969ms/step - loss: 9.6677e-04 - accuracy: 1.0000 - val_loss: 0.0822 - val_accuracy: 0.9806\n",
            "Epoch 135/800\n",
            "12/12 [==============================] - 12s 973ms/step - loss: 8.7863e-04 - accuracy: 1.0000 - val_loss: 0.0775 - val_accuracy: 0.9806\n",
            "Epoch 136/800\n",
            "12/12 [==============================] - 12s 969ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0595 - val_accuracy: 0.9806\n",
            "Epoch 137/800\n",
            "12/12 [==============================] - 12s 969ms/step - loss: 0.0045 - accuracy: 0.9978 - val_loss: 0.0432 - val_accuracy: 0.9903\n",
            "Epoch 138/800\n",
            "12/12 [==============================] - 12s 969ms/step - loss: 8.3764e-04 - accuracy: 1.0000 - val_loss: 0.0343 - val_accuracy: 0.9806\n",
            "Epoch 139/800\n",
            "12/12 [==============================] - 12s 972ms/step - loss: 5.7687e-04 - accuracy: 1.0000 - val_loss: 0.0314 - val_accuracy: 0.9806\n",
            "Epoch 140/800\n",
            "12/12 [==============================] - 12s 970ms/step - loss: 7.4254e-04 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 0.9903\n",
            "Epoch 141/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 5.9853e-04 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 0.9806\n",
            "Epoch 142/800\n",
            "12/12 [==============================] - 12s 974ms/step - loss: 6.0533e-04 - accuracy: 1.0000 - val_loss: 0.0305 - val_accuracy: 0.9806\n",
            "Epoch 143/800\n",
            "12/12 [==============================] - 12s 972ms/step - loss: 9.8525e-04 - accuracy: 1.0000 - val_loss: 0.0209 - val_accuracy: 0.9903\n",
            "Epoch 144/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 5.6819e-04 - accuracy: 1.0000 - val_loss: 0.0191 - val_accuracy: 0.9903\n",
            "Epoch 145/800\n",
            "12/12 [==============================] - 12s 972ms/step - loss: 6.8568e-04 - accuracy: 1.0000 - val_loss: 0.0191 - val_accuracy: 0.9903\n",
            "Epoch 146/800\n",
            "12/12 [==============================] - 12s 972ms/step - loss: 3.4004e-04 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 0.9903\n",
            "Epoch 147/800\n",
            "12/12 [==============================] - 12s 969ms/step - loss: 4.7506e-04 - accuracy: 1.0000 - val_loss: 0.0186 - val_accuracy: 0.9903\n",
            "Epoch 148/800\n",
            "12/12 [==============================] - 12s 967ms/step - loss: 3.6941e-04 - accuracy: 1.0000 - val_loss: 0.0184 - val_accuracy: 0.9903\n",
            "Epoch 149/800\n",
            "12/12 [==============================] - 12s 966ms/step - loss: 4.8374e-04 - accuracy: 1.0000 - val_loss: 0.0186 - val_accuracy: 0.9903\n",
            "Epoch 150/800\n",
            "12/12 [==============================] - 12s 968ms/step - loss: 3.7290e-04 - accuracy: 1.0000 - val_loss: 0.0191 - val_accuracy: 0.9903\n",
            "Epoch 151/800\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 2.1695e-04 - accuracy: 1.0000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-92aabf036113>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m          )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KikcQb9ZpuLB"
      },
      "source": [
        "# model.save(dir_path + 'Models/gestures.h5')"
      ],
      "id": "KikcQb9ZpuLB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "integrated-optimization",
        "outputId": "ec3d1e3d-cabb-478a-c1d4-4a419c75e202"
      },
      "source": [
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(x_test, y_test, batch_size=80)\n",
        "print(\"test loss, test acc:\", results)\n",
        "model.save(dir_path + 'Models/gestures_custom', save_format='tf')"
      ],
      "id": "integrated-optimization",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate on test data\n",
            "4/4 [==============================] - 1s 300ms/step - loss: 0.1531 - accuracy: 0.9567\n",
            "test loss, test acc: [0.15312451124191284, 0.9566929340362549]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./drive/MyDrive/BAKA/Models/gestures_custom/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./drive/MyDrive/BAKA/Models/gestures_custom/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFsQ_tEXw8WX"
      },
      "source": [
        "# new_model = load_model(dir_path + 'Models/gestures.h5')\n",
        "# print(\"Evaluate on test data\")\n",
        "# results = new_model.evaluate(x_test, y_test, batch_size=80)\n",
        "# print(\"test loss, test acc:\", results)"
      ],
      "id": "hFsQ_tEXw8WX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faWZ2lOQ09Dj",
        "outputId": "a145642e-9a8e-4526-da62-2d20490ca314",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "min_max_scaler.data_min_"
      ],
      "id": "faWZ2lOQ09Dj",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9.30245e+01,  8.99999e+01,  9.00456e+01,  9.00455e+01,\n",
              "        9.00455e+01,  6.54812e+01,  2.57840e+01,  2.77705e+01,\n",
              "        2.79286e+01,  2.04904e+01, -4.24721e+02,  6.97324e+01,\n",
              "       -2.97270e+02, -4.20951e+02,  5.95510e+01, -3.15328e+02,\n",
              "       -4.48325e+02,  6.33254e+01, -3.17749e+02, -4.46258e+02,\n",
              "        8.67906e+01, -3.13954e+02, -4.57886e+02,  8.58241e+01,\n",
              "       -3.02162e+02, -4.42757e+02,  1.03148e+02, -2.82183e+02,\n",
              "        1.86895e-01,  6.48663e-02,  1.07620e-01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOWq0Fev1RI-"
      },
      "source": [
        ""
      ],
      "id": "NOWq0Fev1RI-",
      "execution_count": null,
      "outputs": []
    }
  ]
}