{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "caring-species",
   "metadata": {
    "id": "caring-species"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "dir_path = './' #local\n",
    "# dir_path = './drive/MyDrive/BAKA/' #colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "KstfPbJfzIAV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "KstfPbJfzIAV",
    "outputId": "eb6621b0-b46d-4449-d3a0-81a34fe927e1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>164.280661</td>\n",
       "      <td>176.325903</td>\n",
       "      <td>178.121795</td>\n",
       "      <td>173.909634</td>\n",
       "      <td>179.942562</td>\n",
       "      <td>165.068138</td>\n",
       "      <td>179.880849</td>\n",
       "      <td>169.886642</td>\n",
       "      <td>179.693632</td>\n",
       "      <td>178.665290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.589049</td>\n",
       "      <td>0.564965</td>\n",
       "      <td>-0.296812</td>\n",
       "      <td>0.587671</td>\n",
       "      <td>0.523166</td>\n",
       "      <td>-0.401421</td>\n",
       "      <td>0.608364</td>\n",
       "      <td>13.702051</td>\n",
       "      <td>7.694128</td>\n",
       "      <td>16.043218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>165.122590</td>\n",
       "      <td>175.624965</td>\n",
       "      <td>179.600487</td>\n",
       "      <td>179.520481</td>\n",
       "      <td>177.638911</td>\n",
       "      <td>166.203109</td>\n",
       "      <td>168.936561</td>\n",
       "      <td>175.217537</td>\n",
       "      <td>179.756639</td>\n",
       "      <td>179.472871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.589785</td>\n",
       "      <td>0.563702</td>\n",
       "      <td>-0.294720</td>\n",
       "      <td>0.586825</td>\n",
       "      <td>0.522370</td>\n",
       "      <td>-0.401031</td>\n",
       "      <td>0.608108</td>\n",
       "      <td>14.720415</td>\n",
       "      <td>4.666922</td>\n",
       "      <td>14.926564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>165.078781</td>\n",
       "      <td>176.016382</td>\n",
       "      <td>179.983630</td>\n",
       "      <td>179.527066</td>\n",
       "      <td>178.258843</td>\n",
       "      <td>166.794220</td>\n",
       "      <td>169.728408</td>\n",
       "      <td>179.934052</td>\n",
       "      <td>179.742772</td>\n",
       "      <td>179.466085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588971</td>\n",
       "      <td>0.563148</td>\n",
       "      <td>-0.294356</td>\n",
       "      <td>0.586612</td>\n",
       "      <td>0.522276</td>\n",
       "      <td>-0.401130</td>\n",
       "      <td>0.608519</td>\n",
       "      <td>14.914579</td>\n",
       "      <td>3.442307</td>\n",
       "      <td>12.785423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>160.916565</td>\n",
       "      <td>172.198336</td>\n",
       "      <td>179.873084</td>\n",
       "      <td>178.794688</td>\n",
       "      <td>171.190540</td>\n",
       "      <td>169.025568</td>\n",
       "      <td>169.618909</td>\n",
       "      <td>178.304716</td>\n",
       "      <td>179.278919</td>\n",
       "      <td>176.811213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588143</td>\n",
       "      <td>0.564566</td>\n",
       "      <td>-0.294643</td>\n",
       "      <td>0.588136</td>\n",
       "      <td>0.521309</td>\n",
       "      <td>-0.400198</td>\n",
       "      <td>0.607621</td>\n",
       "      <td>20.174759</td>\n",
       "      <td>4.338054</td>\n",
       "      <td>7.668797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>159.852724</td>\n",
       "      <td>179.822872</td>\n",
       "      <td>179.118311</td>\n",
       "      <td>179.866069</td>\n",
       "      <td>178.201757</td>\n",
       "      <td>168.584125</td>\n",
       "      <td>175.381159</td>\n",
       "      <td>179.493500</td>\n",
       "      <td>178.231357</td>\n",
       "      <td>178.974019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.585181</td>\n",
       "      <td>0.557927</td>\n",
       "      <td>-0.292103</td>\n",
       "      <td>0.584925</td>\n",
       "      <td>0.520796</td>\n",
       "      <td>-0.399540</td>\n",
       "      <td>0.607557</td>\n",
       "      <td>12.097098</td>\n",
       "      <td>5.726537</td>\n",
       "      <td>6.220917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0           1           2           3           4           5   \\\n",
       "0   164.280661  176.325903  178.121795  173.909634  179.942562  165.068138   \n",
       "1   165.122590  175.624965  179.600487  179.520481  177.638911  166.203109   \n",
       "2   165.078781  176.016382  179.983630  179.527066  178.258843  166.794220   \n",
       "3   160.916565  172.198336  179.873084  178.794688  171.190540  169.025568   \n",
       "4   159.852724  179.822872  179.118311  179.866069  178.201757  168.584125   \n",
       "..         ...         ...         ...         ...         ...         ...   \n",
       "95    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "96    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "97    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "98    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "99    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "            6           7           8           9   ...        21        22  \\\n",
       "0   179.880849  169.886642  179.693632  178.665290  ...  0.589049  0.564965   \n",
       "1   168.936561  175.217537  179.756639  179.472871  ...  0.589785  0.563702   \n",
       "2   169.728408  179.934052  179.742772  179.466085  ...  0.588971  0.563148   \n",
       "3   169.618909  178.304716  179.278919  176.811213  ...  0.588143  0.564566   \n",
       "4   175.381159  179.493500  178.231357  178.974019  ...  0.585181  0.557927   \n",
       "..         ...         ...         ...         ...  ...       ...       ...   \n",
       "95    0.000000    0.000000    0.000000    0.000000  ...  0.000000  0.000000   \n",
       "96    0.000000    0.000000    0.000000    0.000000  ...  0.000000  0.000000   \n",
       "97    0.000000    0.000000    0.000000    0.000000  ...  0.000000  0.000000   \n",
       "98    0.000000    0.000000    0.000000    0.000000  ...  0.000000  0.000000   \n",
       "99    0.000000    0.000000    0.000000    0.000000  ...  0.000000  0.000000   \n",
       "\n",
       "          23        24        25        26        27         28        29  \\\n",
       "0  -0.296812  0.587671  0.523166 -0.401421  0.608364  13.702051  7.694128   \n",
       "1  -0.294720  0.586825  0.522370 -0.401031  0.608108  14.720415  4.666922   \n",
       "2  -0.294356  0.586612  0.522276 -0.401130  0.608519  14.914579  3.442307   \n",
       "3  -0.294643  0.588136  0.521309 -0.400198  0.607621  20.174759  4.338054   \n",
       "4  -0.292103  0.584925  0.520796 -0.399540  0.607557  12.097098  5.726537   \n",
       "..       ...       ...       ...       ...       ...        ...       ...   \n",
       "95  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "96  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "97  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "98  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "99  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "\n",
       "           30  \n",
       "0   16.043218  \n",
       "1   14.926564  \n",
       "2   12.785423  \n",
       "3    7.668797  \n",
       "4    6.220917  \n",
       "..        ...  \n",
       "95   0.000000  \n",
       "96   0.000000  \n",
       "97   0.000000  \n",
       "98   0.000000  \n",
       "99   0.000000  \n",
       "\n",
       "[100 rows x 31 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uncomment in colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# pd.read_csv(dir_path + 'DataCollection' + '/' + '1' + '/' + '0.txt', sep=' ', index_col=False )\n",
    "# test = np.genfromtxt(dir_path + 'DataCollection' + '/' + '1' + '/' + '0.txt', delimiter=' ',dtype='float64')\n",
    "df = pd.read_csv(dir_path + 'DataCollection' + '/' + '10' + '/' + '0.txt', header=None, sep=' ')\n",
    "df.drop(df.index[100:])\n",
    "filler = np.zeros((100-len(df), 31))\n",
    "df = df.append(pd.DataFrame(filler), ignore_index=True )\n",
    "df\n",
    "# df = df.drop(columns=[31])\n",
    "# sp_df = np.array_split(df, 2)\n",
    "# sp_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "signal-friendly",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "signal-friendly",
    "outputId": "2055e2ba-8a80-447f-c2d5-c2acf2e7b7de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '10', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(dir_path + 'DataCollection')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "annoying-rotation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "annoying-rotation",
    "outputId": "4b21af4b-5e45-4164-c12a-bbeadc9732a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302   60\n",
      "602   120\n",
      "200   40\n",
      "512   102\n",
      "302   60\n",
      "410   82\n",
      "422   84\n",
      "200   40\n",
      "200   40\n",
      "200   40\n",
      "200   40\n"
     ]
    }
   ],
   "source": [
    "x_train = [];\n",
    "y_train = [];\n",
    "\n",
    "x_test = [];\n",
    "y_test = [];\n",
    "for i in files:\n",
    "    samples = os.listdir(dir_path + 'DataCollection' + '/' + i)\n",
    "    num_tests = int(len(samples)/5);\n",
    "    shuffle(samples, random_state = 0)\n",
    "    for k in range(0, num_tests):\n",
    "#         df = np.genfromtxt(dir_path + 'DataCollection' + '/' + i + '/' + samples[k], delimiter=' ',dtype='float64')\n",
    "        df = pd.read_csv(dir_path + 'DataCollection' + '/' + i + '/' + samples[k], header=None, sep=' ')\n",
    "        df = df.drop(df.index[100:])\n",
    "        if len(df) < 100:\n",
    "            filler = np.zeros((100-len(df), 31))\n",
    "            df = df.append(pd.DataFrame(filler), ignore_index=True )\n",
    "        x_test.append(df.to_numpy())\n",
    "        y_test.append(int(i));\n",
    "    \n",
    "    for k in range(num_tests, len(samples)):\n",
    "        df = pd.read_csv(dir_path + 'DataCollection' + '/' + i + '/' + samples[k], header=None, sep=' ')\n",
    "        df = df.drop(df.index[100:])\n",
    "        if len(df) < 100:\n",
    "            filler = np.zeros((100-len(df), 31))\n",
    "            df = df.append(pd.DataFrame(filler), ignore_index=True )\n",
    "        x_train.append(df.to_numpy())\n",
    "        y_train.append(int(i));\n",
    "    \n",
    "    print(len(samples), ' ', num_tests)\n",
    "    \n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train);\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "orange-character",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orange-character",
    "outputId": "523b68e9-cc97-4b5c-ff51-1d337463e529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape:  (2842, 100, 31)\n",
      "y_train.shiape:  (2842,)\n",
      "x_test.shape:  (708, 100, 31)\n",
      "y_test.shape:  (708,)\n",
      "[[[150.587      144.419      135.792      ...  13.0169       7.71234\n",
      "    11.0556    ]\n",
      "  [150.859      144.337      135.95       ...  12.9418       7.68933\n",
      "    11.0159    ]\n",
      "  [150.946      144.576      136.628      ...  12.8984       7.67985\n",
      "    10.8432    ]\n",
      "  ...\n",
      "  [153.648      144.694      138.125      ...  13.418        6.05023\n",
      "    11.3359    ]\n",
      "  [153.672      144.746      138.565      ...  13.3282       6.02335\n",
      "    11.4155    ]\n",
      "  [153.61       144.807      138.928      ...  13.2624       6.07887\n",
      "    11.4544    ]]\n",
      "\n",
      " [[153.587      144.8        139.027      ...  13.2341       6.15317\n",
      "    11.4522    ]\n",
      "  [153.514      144.783      139.073      ...  13.2043       6.23986\n",
      "    11.4108    ]\n",
      "  [153.465      144.756      138.957      ...  13.1946       6.35819\n",
      "    11.3776    ]\n",
      "  ...\n",
      "  [153.072      139.277      133.717      ...  13.7425       8.01148\n",
      "    15.0987    ]\n",
      "  [152.886      139.304      134.046      ...  13.7856       8.1632\n",
      "    15.1925    ]\n",
      "  [152.656      139.245      134.448      ...  13.8042       8.36099\n",
      "    15.332     ]]\n",
      "\n",
      " [[152.363      139.427      134.935      ...  13.8147       8.50642\n",
      "    15.5408    ]\n",
      "  [152.074      139.698      135.441      ...  13.8056       8.67336\n",
      "    15.8031    ]\n",
      "  [151.976      139.661      135.576      ...  13.7628       8.7362\n",
      "    16.1       ]\n",
      "  ...\n",
      "  [157.531      141.462      135.234      ...  10.572        7.5791\n",
      "    13.1759    ]\n",
      "  [157.47       141.256      135.215      ...  10.5627       7.58764\n",
      "    13.2905    ]\n",
      "  [157.446      141.186      135.17       ...  10.5486       7.5798\n",
      "    13.3199    ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[155.25170311 179.11539785 111.77561358 ...  30.27942694  52.32663425\n",
      "    61.8105259 ]\n",
      "  [156.13909939 178.54645881 136.42083699 ...  44.00366348  64.80371254\n",
      "    64.72118429]\n",
      "  [149.67511881 178.73642394 158.1457932  ... 127.24165705  26.05783436\n",
      "   148.4339265 ]\n",
      "  ...\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]]\n",
      "\n",
      " [[162.5163262  176.81283934 174.89204637 ...  24.20530599   4.37923447\n",
      "    10.70108505]\n",
      "  [152.49218932 179.18995121 162.92984205 ...   7.38901953   4.37692984\n",
      "    24.33196998]\n",
      "  [163.75287913 177.42596868  95.33509893 ...  22.81009973  53.15164015\n",
      "    93.37811813]\n",
      "  ...\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]]\n",
      "\n",
      " [[153.9638502  172.89358834 103.65563271 ...  70.94412459  23.15656749\n",
      "   121.91610974]\n",
      "  [158.20227275 178.8420499  173.33018821 ...  98.81020758  52.98660392\n",
      "   113.92107171]\n",
      "  [152.77012804 179.54197068 169.34480092 ...  75.36481336  33.47634294\n",
      "   129.28476098]\n",
      "  ...\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...   0.           0.\n",
      "     0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train.shape: \", x_train.shape)\n",
    "print(\"y_train.shiape: \", y_train.shape)\n",
    "print(\"x_test.shape: \", x_test.shape)\n",
    "print(\"y_test.shape: \", y_test.shape)\n",
    "\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sensitive-cheat",
   "metadata": {
    "id": "sensitive-cheat"
   },
   "outputs": [],
   "source": [
    "def scale_data(data, min_max_scaler):\n",
    "    for i in range(len(data)):\n",
    "        data[i] = min_max_scaler.transform(data[i])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "broke-sailing",
   "metadata": {
    "id": "broke-sailing"
   },
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "num_instances, num_time_steps, num_features = x_train.shape\n",
    "x_train = np.reshape(x_train, newshape=(-1, num_features))\n",
    "x_train = min_max_scaler.fit_transform(x_train)\n",
    "x_train = np.reshape(x_train, newshape=(num_instances, num_time_steps, num_features))\n",
    "\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=0)\n",
    "\n",
    "num_instances, num_time_steps, num_features = x_test.shape\n",
    "x_test = np.reshape(x_test, newshape=(-1, num_features))\n",
    "x_test = min_max_scaler.transform(x_test)\n",
    "x_test = np.reshape(x_test, newshape=(num_instances, num_time_steps, num_features))\n",
    "\n",
    "x_test, y_test = shuffle(x_test, y_test, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "discrete-kansas",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "discrete-kansas",
    "outputId": "dfb135b5-f9fc-4db5-cd29-ce1d12115968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape:  (2042, 100, 31)\n",
      "y_train.shiape:  (2042,)\n",
      "x_test.shape:  (508, 100, 31)\n",
      "y_test.shape:  (508,)\n"
     ]
    }
   ],
   "source": [
    "# x_train = np.delete(x_train, [0,1,2,3,4,5,6,7,8], 0)\n",
    "# y_train = np.delete(y_train, [0,1,2,3,4,5,6,7,8], 0)\n",
    "print(\"x_train.shape: \", x_train.shape)\n",
    "print(\"y_train.shiape: \", y_train.shape)\n",
    "print(\"x_test.shape: \", x_test.shape)\n",
    "print(\"y_test.shape: \", y_test.shape)\n",
    "# print(y_train)\n",
    "# print(\"\\n\")\n",
    "# print(y_test)\n",
    "\n",
    "# y_train = y_train.astype('int')\n",
    "\n",
    "# print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "senior-mystery",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "senior-mystery",
    "outputId": "81f2dd61-f990-4bdd-b4e5-afb32cdc755b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100, 100)          52800     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 100, 100)          400       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100, 100)          80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100, 100)          400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100, 100)          80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100, 100)          400       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 296,206\n",
      "Trainable params: 295,406\n",
      "Non-trainable params: 800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=100, input_shape=x_train.shape[1:], return_sequences=True,dtype='float64'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(100, return_sequences=True ,dtype='float64'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(100, return_sequences=True ,dtype='float64'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(100,dtype='float64'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(30, activation='softmax'))\n",
    "\n",
    "model.add(Dense(6, activation='softmax',dtype='float64'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "after-sperm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "after-sperm",
    "outputId": "8ea05251-0734-47a5-c810-b269df57e057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "23/23 [==============================] - 42s 257ms/step - loss: 1.6793 - accuracy: 0.4241 - val_loss: 1.7454 - val_accuracy: 0.2146\n",
      "Epoch 2/200\n",
      "23/23 [==============================] - 5s 203ms/step - loss: 0.3826 - accuracy: 0.8845 - val_loss: 1.6815 - val_accuracy: 0.3171\n",
      "Epoch 3/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.2610 - accuracy: 0.9316 - val_loss: 1.5989 - val_accuracy: 0.4244\n",
      "Epoch 4/200\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 0.2113 - accuracy: 0.9483 - val_loss: 1.4961 - val_accuracy: 0.4780\n",
      "Epoch 5/200\n",
      "23/23 [==============================] - 5s 204ms/step - loss: 0.1627 - accuracy: 0.9574 - val_loss: 1.3390 - val_accuracy: 0.6098\n",
      "Epoch 6/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.1468 - accuracy: 0.9604 - val_loss: 1.1434 - val_accuracy: 0.7317\n",
      "Epoch 7/200\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 0.1377 - accuracy: 0.9669 - val_loss: 0.9546 - val_accuracy: 0.8000\n",
      "Epoch 8/200\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 0.1088 - accuracy: 0.9690 - val_loss: 0.7201 - val_accuracy: 0.9171\n",
      "Epoch 9/200\n",
      "23/23 [==============================] - 5s 204ms/step - loss: 0.0879 - accuracy: 0.9773 - val_loss: 0.5230 - val_accuracy: 0.9366\n",
      "Epoch 10/200\n",
      "23/23 [==============================] - 5s 203ms/step - loss: 0.0803 - accuracy: 0.9781 - val_loss: 0.3724 - val_accuracy: 0.9220\n",
      "Epoch 11/200\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 0.0757 - accuracy: 0.9812 - val_loss: 0.2532 - val_accuracy: 0.9659\n",
      "Epoch 12/200\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 0.0771 - accuracy: 0.9785 - val_loss: 0.2008 - val_accuracy: 0.9610\n",
      "Epoch 13/200\n",
      "23/23 [==============================] - 5s 204ms/step - loss: 0.0614 - accuracy: 0.9832 - val_loss: 0.1501 - val_accuracy: 0.9610\n",
      "Epoch 14/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0632 - accuracy: 0.9853 - val_loss: 0.1424 - val_accuracy: 0.9659\n",
      "Epoch 15/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0457 - accuracy: 0.9891 - val_loss: 0.1080 - val_accuracy: 0.9707\n",
      "Epoch 16/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0506 - accuracy: 0.9870 - val_loss: 0.3325 - val_accuracy: 0.8829\n",
      "Epoch 17/200\n",
      "23/23 [==============================] - 5s 209ms/step - loss: 0.0462 - accuracy: 0.9873 - val_loss: 0.0956 - val_accuracy: 0.9659\n",
      "Epoch 18/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0517 - accuracy: 0.9871 - val_loss: 0.1948 - val_accuracy: 0.9317\n",
      "Epoch 19/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0387 - accuracy: 0.9881 - val_loss: 0.0976 - val_accuracy: 0.9756\n",
      "Epoch 20/200\n",
      "23/23 [==============================] - 5s 209ms/step - loss: 0.0346 - accuracy: 0.9913 - val_loss: 0.0635 - val_accuracy: 0.9854\n",
      "Epoch 21/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0342 - accuracy: 0.9888 - val_loss: 0.0643 - val_accuracy: 0.9756\n",
      "Epoch 22/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0303 - accuracy: 0.9953 - val_loss: 0.1133 - val_accuracy: 0.9610\n",
      "Epoch 23/200\n",
      "23/23 [==============================] - 5s 209ms/step - loss: 0.0398 - accuracy: 0.9890 - val_loss: 0.0692 - val_accuracy: 0.9805\n",
      "Epoch 24/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0206 - accuracy: 0.9960 - val_loss: 0.1618 - val_accuracy: 0.9561\n",
      "Epoch 25/200\n",
      "23/23 [==============================] - 5s 209ms/step - loss: 0.0312 - accuracy: 0.9926 - val_loss: 0.1387 - val_accuracy: 0.9659\n",
      "Epoch 26/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0189 - accuracy: 0.9963 - val_loss: 0.0644 - val_accuracy: 0.9902\n",
      "Epoch 27/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0324 - accuracy: 0.9914 - val_loss: 0.0861 - val_accuracy: 0.9805\n",
      "Epoch 28/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0317 - accuracy: 0.9896 - val_loss: 0.0772 - val_accuracy: 0.9854\n",
      "Epoch 29/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0176 - accuracy: 0.9981 - val_loss: 0.0444 - val_accuracy: 0.9951\n",
      "Epoch 30/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0184 - accuracy: 0.9957 - val_loss: 0.0660 - val_accuracy: 0.9854\n",
      "Epoch 31/200\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 0.0135 - accuracy: 0.9975 - val_loss: 0.0598 - val_accuracy: 0.9902\n",
      "Epoch 32/200\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 0.0205 - accuracy: 0.9944 - val_loss: 0.0832 - val_accuracy: 0.9707\n",
      "Epoch 33/200\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 0.0274 - accuracy: 0.9937 - val_loss: 0.1259 - val_accuracy: 0.9707\n",
      "Epoch 34/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0149 - accuracy: 0.9972 - val_loss: 0.0606 - val_accuracy: 0.9902\n",
      "Epoch 35/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0131 - accuracy: 0.9971 - val_loss: 0.0341 - val_accuracy: 0.9951\n",
      "Epoch 36/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0087 - accuracy: 0.9977 - val_loss: 0.0628 - val_accuracy: 0.9854\n",
      "Epoch 37/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0108 - accuracy: 0.9986 - val_loss: 0.0614 - val_accuracy: 0.9902\n",
      "Epoch 38/200\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 0.0176 - accuracy: 0.9956 - val_loss: 0.0448 - val_accuracy: 0.9951\n",
      "Epoch 39/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0088 - accuracy: 0.9979 - val_loss: 0.0297 - val_accuracy: 0.9951\n",
      "Epoch 40/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0100 - accuracy: 0.9973 - val_loss: 0.0097 - val_accuracy: 0.9951\n",
      "Epoch 41/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0133 - accuracy: 0.9949 - val_loss: 0.0593 - val_accuracy: 0.9854\n",
      "Epoch 42/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0098 - accuracy: 0.9981 - val_loss: 0.0518 - val_accuracy: 0.9902\n",
      "Epoch 43/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0147 - accuracy: 0.9939 - val_loss: 0.0874 - val_accuracy: 0.9805\n",
      "Epoch 44/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0085 - accuracy: 0.9987 - val_loss: 0.0378 - val_accuracy: 0.9902\n",
      "Epoch 45/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0123 - accuracy: 0.9965 - val_loss: 0.0735 - val_accuracy: 0.9902\n",
      "Epoch 46/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0077 - accuracy: 0.9987 - val_loss: 0.0556 - val_accuracy: 0.9951\n",
      "Epoch 47/200\n",
      "23/23 [==============================] - 5s 209ms/step - loss: 0.0088 - accuracy: 0.9975 - val_loss: 0.1381 - val_accuracy: 0.9707\n",
      "Epoch 48/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0091 - accuracy: 0.9977 - val_loss: 0.2059 - val_accuracy: 0.9756\n",
      "Epoch 49/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0081 - accuracy: 0.9992 - val_loss: 0.0686 - val_accuracy: 0.9902\n",
      "Epoch 50/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0137 - accuracy: 0.9964 - val_loss: 0.0542 - val_accuracy: 0.9902\n",
      "Epoch 51/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0162 - accuracy: 0.9948 - val_loss: 0.1694 - val_accuracy: 0.9707\n",
      "Epoch 52/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0136 - accuracy: 0.9966 - val_loss: 0.4268 - val_accuracy: 0.9171\n",
      "Epoch 53/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0365 - accuracy: 0.9903 - val_loss: 0.0434 - val_accuracy: 0.9951\n",
      "Epoch 54/200\n",
      "23/23 [==============================] - 5s 204ms/step - loss: 0.0274 - accuracy: 0.9915 - val_loss: 0.0301 - val_accuracy: 0.9951\n",
      "Epoch 55/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0203 - accuracy: 0.9925 - val_loss: 0.0390 - val_accuracy: 0.9951\n",
      "Epoch 56/200\n",
      "23/23 [==============================] - 5s 204ms/step - loss: 0.0184 - accuracy: 0.9935 - val_loss: 0.0749 - val_accuracy: 0.9805\n",
      "Epoch 57/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0111 - accuracy: 0.9970 - val_loss: 0.0600 - val_accuracy: 0.9951\n",
      "Epoch 58/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0157 - accuracy: 0.9973 - val_loss: 0.0844 - val_accuracy: 0.9854\n",
      "Epoch 59/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0106 - accuracy: 0.9975 - val_loss: 0.0544 - val_accuracy: 0.9951\n",
      "Epoch 60/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.0589 - val_accuracy: 0.9854\n",
      "Epoch 61/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0082 - accuracy: 0.9978 - val_loss: 0.0462 - val_accuracy: 0.9951\n",
      "Epoch 62/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0049 - accuracy: 0.9991 - val_loss: 0.0534 - val_accuracy: 0.9902\n",
      "Epoch 63/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0084 - accuracy: 0.9988 - val_loss: 0.0901 - val_accuracy: 0.9805\n",
      "Epoch 64/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0165 - accuracy: 0.9950 - val_loss: 0.1055 - val_accuracy: 0.9707\n",
      "Epoch 65/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0141 - accuracy: 0.9963 - val_loss: 0.0684 - val_accuracy: 0.9854\n",
      "Epoch 66/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0068 - accuracy: 0.9977 - val_loss: 0.0551 - val_accuracy: 0.9951\n",
      "Epoch 67/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0042 - accuracy: 0.9998 - val_loss: 0.0635 - val_accuracy: 0.9902\n",
      "Epoch 68/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0066 - accuracy: 0.9997 - val_loss: 0.0498 - val_accuracy: 0.9951\n",
      "Epoch 69/200\n",
      "23/23 [==============================] - 5s 209ms/step - loss: 0.0040 - accuracy: 0.9996 - val_loss: 0.0915 - val_accuracy: 0.9756\n",
      "Epoch 70/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.0477 - val_accuracy: 0.9951\n",
      "Epoch 71/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0111 - accuracy: 0.9974 - val_loss: 0.0507 - val_accuracy: 0.9902\n",
      "Epoch 72/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0085 - accuracy: 0.9988 - val_loss: 0.0437 - val_accuracy: 0.9951\n",
      "Epoch 73/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0053 - accuracy: 0.9992 - val_loss: 0.1767 - val_accuracy: 0.9610\n",
      "Epoch 74/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0117 - accuracy: 0.9973 - val_loss: 0.0440 - val_accuracy: 0.9951\n",
      "Epoch 75/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0138 - accuracy: 0.9959 - val_loss: 0.0563 - val_accuracy: 0.9902\n",
      "Epoch 76/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0103 - accuracy: 0.9970 - val_loss: 0.1150 - val_accuracy: 0.9659\n",
      "Epoch 77/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.0673 - val_accuracy: 0.9902\n",
      "Epoch 78/200\n",
      "23/23 [==============================] - 5s 209ms/step - loss: 0.0109 - accuracy: 0.9976 - val_loss: 0.1057 - val_accuracy: 0.9805\n",
      "Epoch 79/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0104 - accuracy: 0.9966 - val_loss: 0.0594 - val_accuracy: 0.9951\n",
      "Epoch 80/200\n",
      "23/23 [==============================] - 5s 209ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.0697 - val_accuracy: 0.9805\n",
      "Epoch 81/200\n",
      "23/23 [==============================] - 5s 209ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.3804 - val_accuracy: 0.9268\n",
      "Epoch 82/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0101 - accuracy: 0.9980 - val_loss: 0.0535 - val_accuracy: 0.9951\n",
      "Epoch 83/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0525 - val_accuracy: 0.9951\n",
      "Epoch 84/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.0232 - val_accuracy: 0.9951\n",
      "Epoch 85/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.0301 - val_accuracy: 0.9951\n",
      "Epoch 86/200\n",
      "23/23 [==============================] - 5s 209ms/step - loss: 0.0022 - accuracy: 0.9999 - val_loss: 0.0410 - val_accuracy: 0.9951\n",
      "Epoch 87/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.0848 - val_accuracy: 0.9854\n",
      "Epoch 88/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0047 - accuracy: 0.9996 - val_loss: 0.0531 - val_accuracy: 0.9951\n",
      "Epoch 89/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0042 - accuracy: 0.9981 - val_loss: 0.0519 - val_accuracy: 0.9951\n",
      "Epoch 90/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.0452 - val_accuracy: 0.9951\n",
      "Epoch 91/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 0.9951\n",
      "Epoch 92/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 0.9951\n",
      "Epoch 93/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0414 - val_accuracy: 0.9951\n",
      "Epoch 94/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.0369 - val_accuracy: 0.9951\n",
      "Epoch 95/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0030 - accuracy: 0.9997 - val_loss: 0.0432 - val_accuracy: 0.9951\n",
      "Epoch 96/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0454 - val_accuracy: 0.9951\n",
      "Epoch 97/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0057 - accuracy: 0.9979 - val_loss: 0.1914 - val_accuracy: 0.9707\n",
      "Epoch 98/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0214 - accuracy: 0.9941 - val_loss: 0.1396 - val_accuracy: 0.9610\n",
      "Epoch 99/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0264 - accuracy: 0.9926 - val_loss: 0.1200 - val_accuracy: 0.9659\n",
      "Epoch 100/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0123 - accuracy: 0.9962 - val_loss: 0.0705 - val_accuracy: 0.9902\n",
      "Epoch 101/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0066 - accuracy: 0.9968 - val_loss: 0.0799 - val_accuracy: 0.9854\n",
      "Epoch 102/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0080 - accuracy: 0.9969 - val_loss: 0.0669 - val_accuracy: 0.9951\n",
      "Epoch 103/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.0752 - val_accuracy: 0.9902\n",
      "Epoch 104/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0680 - val_accuracy: 0.9902\n",
      "Epoch 105/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0026 - accuracy: 0.9998 - val_loss: 0.0887 - val_accuracy: 0.9902\n",
      "Epoch 106/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0654 - val_accuracy: 0.9902\n",
      "Epoch 107/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0551 - val_accuracy: 0.9951\n",
      "Epoch 108/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.0524 - val_accuracy: 0.9951\n",
      "Epoch 109/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0558 - val_accuracy: 0.9951\n",
      "Epoch 110/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0555 - val_accuracy: 0.9951\n",
      "Epoch 111/200\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.0559 - val_accuracy: 0.9951\n",
      "Epoch 112/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0557 - val_accuracy: 0.9951\n",
      "Epoch 113/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0085 - accuracy: 0.9962 - val_loss: 0.0535 - val_accuracy: 0.9902\n",
      "Epoch 114/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0055 - accuracy: 0.9981 - val_loss: 0.0441 - val_accuracy: 0.9951\n",
      "Epoch 115/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0037 - accuracy: 0.9985 - val_loss: 0.0403 - val_accuracy: 0.9902\n",
      "Epoch 116/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0461 - val_accuracy: 0.9951\n",
      "Epoch 117/200\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0476 - val_accuracy: 0.9951\n",
      "Epoch 118/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.0445 - val_accuracy: 0.9902\n",
      "Epoch 119/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0456 - val_accuracy: 0.9951\n",
      "Epoch 120/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.0647 - val_accuracy: 0.9902\n",
      "Epoch 121/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0546 - val_accuracy: 0.9902\n",
      "Epoch 122/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0665 - val_accuracy: 0.9902\n",
      "Epoch 123/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0333 - val_accuracy: 0.9951\n",
      "Epoch 124/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0294 - val_accuracy: 0.9951\n",
      "Epoch 125/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.0153 - val_accuracy: 0.9951\n",
      "Epoch 126/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0077 - val_accuracy: 0.9951\n",
      "Epoch 127/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1023 - val_accuracy: 0.9805\n",
      "Epoch 128/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.1601 - val_accuracy: 0.9512\n",
      "Epoch 129/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.0275 - val_accuracy: 0.9951\n",
      "Epoch 130/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0024 - accuracy: 0.9986 - val_loss: 0.0443 - val_accuracy: 0.9951\n",
      "Epoch 131/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0185 - accuracy: 0.9942 - val_loss: 0.0882 - val_accuracy: 0.9854\n",
      "Epoch 132/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.1035 - val_accuracy: 0.9805\n",
      "Epoch 133/200\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 0.0070 - accuracy: 0.9981 - val_loss: 0.1422 - val_accuracy: 0.9610\n",
      "Epoch 134/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0486 - val_accuracy: 0.9902\n",
      "Epoch 135/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0561 - val_accuracy: 0.9902\n",
      "Epoch 136/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 9.8624e-04 - accuracy: 1.0000 - val_loss: 0.0618 - val_accuracy: 0.9902\n",
      "Epoch 137/200\n",
      "23/23 [==============================] - 5s 209ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0604 - val_accuracy: 0.9902\n",
      "Epoch 138/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.0758 - val_accuracy: 0.9854\n",
      "Epoch 139/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0559 - val_accuracy: 0.9902\n",
      "Epoch 140/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 8.8356e-04 - accuracy: 1.0000 - val_loss: 0.0456 - val_accuracy: 0.9951\n",
      "Epoch 141/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.0436 - val_accuracy: 0.9951\n",
      "Epoch 142/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0619 - val_accuracy: 0.9902\n",
      "Epoch 143/200\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.0701 - val_accuracy: 0.9854\n",
      "Epoch 144/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0040 - accuracy: 0.9995 - val_loss: 0.0748 - val_accuracy: 0.9854\n",
      "Epoch 145/200\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 0.0062 - accuracy: 0.9990 - val_loss: 0.0504 - val_accuracy: 0.9951\n",
      "Epoch 146/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0033 - accuracy: 0.9985 - val_loss: 0.0423 - val_accuracy: 0.9902\n",
      "Epoch 147/200\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0972 - val_accuracy: 0.9902\n",
      "Epoch 148/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0863 - val_accuracy: 0.9902\n",
      "Epoch 149/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 6.7851e-04 - accuracy: 1.0000 - val_loss: 0.0721 - val_accuracy: 0.9902\n",
      "Epoch 150/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 8.6867e-04 - accuracy: 1.0000 - val_loss: 0.0716 - val_accuracy: 0.9902\n",
      "Epoch 151/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0660 - val_accuracy: 0.9902\n",
      "Epoch 152/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0562 - val_accuracy: 0.9951\n",
      "Epoch 153/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0016 - accuracy: 0.9991 - val_loss: 0.0500 - val_accuracy: 0.9951\n",
      "Epoch 154/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0147 - accuracy: 0.9966 - val_loss: 0.0813 - val_accuracy: 0.9854\n",
      "Epoch 155/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0085 - accuracy: 0.9989 - val_loss: 0.0611 - val_accuracy: 0.9951\n",
      "Epoch 156/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 0.1049 - val_accuracy: 0.9854\n",
      "Epoch 157/200\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 0.0133 - accuracy: 0.9976 - val_loss: 0.0529 - val_accuracy: 0.9951\n",
      "Epoch 158/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0550 - val_accuracy: 0.9951\n",
      "Epoch 159/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.0601 - val_accuracy: 0.9951\n",
      "Epoch 160/200\n",
      "23/23 [==============================] - 5s 209ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0608 - val_accuracy: 0.9951\n",
      "Epoch 161/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0091 - accuracy: 0.9963 - val_loss: 0.0351 - val_accuracy: 0.9951\n",
      "Epoch 162/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0027 - accuracy: 0.9988 - val_loss: 0.0344 - val_accuracy: 0.9902\n",
      "Epoch 163/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0365 - val_accuracy: 0.9902\n",
      "Epoch 164/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0014 - accuracy: 0.9993 - val_loss: 0.0341 - val_accuracy: 0.9902\n",
      "Epoch 165/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 9.9523e-04 - accuracy: 0.9997 - val_loss: 0.0392 - val_accuracy: 0.9902\n",
      "Epoch 166/200\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 9.5949e-04 - accuracy: 1.0000 - val_loss: 0.0441 - val_accuracy: 0.9902\n",
      "Epoch 167/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 8.5275e-04 - accuracy: 1.0000 - val_loss: 0.0523 - val_accuracy: 0.9902\n",
      "Epoch 168/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0025 - accuracy: 0.9980 - val_loss: 0.0465 - val_accuracy: 0.9902\n",
      "Epoch 169/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 6.7336e-04 - accuracy: 1.0000 - val_loss: 0.0458 - val_accuracy: 0.9902\n",
      "Epoch 170/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.3856 - val_accuracy: 0.9268\n",
      "Epoch 171/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0134 - accuracy: 0.9982 - val_loss: 0.0947 - val_accuracy: 0.9854\n",
      "Epoch 172/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 0.0035 - accuracy: 0.9980 - val_loss: 0.0561 - val_accuracy: 0.9951\n",
      "Epoch 173/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0553 - val_accuracy: 0.9902\n",
      "Epoch 174/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0016 - accuracy: 0.9992 - val_loss: 0.0374 - val_accuracy: 0.9951\n",
      "Epoch 175/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0609 - val_accuracy: 0.9902\n",
      "Epoch 176/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0067 - accuracy: 0.9981 - val_loss: 0.0944 - val_accuracy: 0.9756\n",
      "Epoch 177/200\n",
      "23/23 [==============================] - 5s 209ms/step - loss: 0.0075 - accuracy: 0.9974 - val_loss: 0.0403 - val_accuracy: 0.9951\n",
      "Epoch 178/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0014 - accuracy: 0.9992 - val_loss: 0.0372 - val_accuracy: 0.9951\n",
      "Epoch 179/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0116 - accuracy: 0.9978 - val_loss: 0.0664 - val_accuracy: 0.9805\n",
      "Epoch 180/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0078 - accuracy: 0.9989 - val_loss: 0.0688 - val_accuracy: 0.9902\n",
      "Epoch 181/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.0757 - val_accuracy: 0.9902\n",
      "Epoch 182/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 6.6393e-04 - accuracy: 1.0000 - val_loss: 0.0384 - val_accuracy: 0.9951\n",
      "Epoch 183/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0494 - val_accuracy: 0.9951\n",
      "Epoch 184/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 8.4614e-04 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 0.9951\n",
      "Epoch 185/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 8.0122e-04 - accuracy: 0.9998 - val_loss: 0.0505 - val_accuracy: 0.9951\n",
      "Epoch 186/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0017 - accuracy: 0.9990 - val_loss: 0.0377 - val_accuracy: 0.9951\n",
      "Epoch 187/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 6.5017e-04 - accuracy: 1.0000 - val_loss: 0.0358 - val_accuracy: 0.9951\n",
      "Epoch 188/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 6.1118e-04 - accuracy: 1.0000 - val_loss: 0.0362 - val_accuracy: 0.9951\n",
      "Epoch 189/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 5.1553e-04 - accuracy: 1.0000 - val_loss: 0.0353 - val_accuracy: 0.9951\n",
      "Epoch 190/200\n",
      "23/23 [==============================] - 5s 208ms/step - loss: 8.5702e-04 - accuracy: 0.9996 - val_loss: 0.0392 - val_accuracy: 0.9951\n",
      "Epoch 191/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0622 - val_accuracy: 0.9902\n",
      "Epoch 192/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0075 - val_accuracy: 0.9951\n",
      "Epoch 193/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 0.0024 - accuracy: 0.9991 - val_loss: 0.0502 - val_accuracy: 0.9951\n",
      "Epoch 194/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0015 - accuracy: 0.9992 - val_loss: 0.0463 - val_accuracy: 0.9951\n",
      "Epoch 195/200\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 3.7747e-04 - accuracy: 1.0000 - val_loss: 0.0453 - val_accuracy: 0.9951\n",
      "Epoch 196/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 9.1246e-04 - accuracy: 1.0000 - val_loss: 0.0373 - val_accuracy: 0.9951\n",
      "Epoch 197/200\n",
      "23/23 [==============================] - 5s 205ms/step - loss: 5.2745e-04 - accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 0.9951\n",
      "Epoch 198/200\n",
      "23/23 [==============================] - 5s 206ms/step - loss: 4.3513e-04 - accuracy: 1.0000 - val_loss: 0.0390 - val_accuracy: 0.9951\n",
      "Epoch 199/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 3.8253e-04 - accuracy: 1.0000 - val_loss: 0.0411 - val_accuracy: 0.9951\n",
      "Epoch 200/200\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 3.1628e-04 - accuracy: 1.0000 - val_loss: 0.0414 - val_accuracy: 0.9951\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.0001, decay=1e-5)\n",
    "\n",
    "checkpoint_filepath = dir_path + 'Checkpoints'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "\n",
    "gestures = model.fit(x = x_train,\n",
    "            y = y_train,\n",
    "            epochs=200,\n",
    "            validation_split=0.1, #split 10% of the trainning set for the validation set,\n",
    "            batch_size=80,\n",
    "            callbacks=[model_checkpoint_callback],\n",
    "            shuffle=True\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "KikcQb9ZpuLB",
   "metadata": {
    "id": "KikcQb9ZpuLB"
   },
   "outputs": [],
   "source": [
    "# model.save(dir_path + 'Models/gestures.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "integrated-optimization",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "integrated-optimization",
    "outputId": "21150691-98ed-4c9a-a18a-fbc3c8bf811c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "7/7 [==============================] - 1s 74ms/step - loss: 0.1182 - accuracy: 0.9724\n",
      "test loss, test acc: [0.11823232471942902, 0.9724409580230713]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./drive/MyDrive/BAKA/Models/gestures_100/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./drive/MyDrive/BAKA/Models/gestures_100/assets\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(x_test, y_test, batch_size=80)\n",
    "print(\"test loss, test acc:\", results)\n",
    "model.save(dir_path + 'Models/gestures_100', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hFsQ_tEXw8WX",
   "metadata": {
    "id": "hFsQ_tEXw8WX"
   },
   "outputs": [],
   "source": [
    "# new_model = load_model(dir_path + 'Models/gestures.h5')\n",
    "# print(\"Evaluate on test data\")\n",
    "# results = new_model.evaluate(x_test, y_test, batch_size=80)\n",
    "# print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faWZ2lOQ09Dj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "faWZ2lOQ09Dj",
    "outputId": "0b6161bf-00cf-41b6-df95-3272d9133f53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.30245e+01,  8.99999e+01,  9.00456e+01,  9.00455e+01,\n",
       "        9.00455e+01,  6.54812e+01,  2.57840e+01,  2.77705e+01,\n",
       "        2.79286e+01,  2.04904e+01, -4.24721e+02,  6.97324e+01,\n",
       "       -2.97270e+02, -4.20951e+02,  5.95510e+01, -3.15328e+02,\n",
       "       -4.48325e+02,  6.33254e+01, -3.17749e+02, -4.46258e+02,\n",
       "        8.67906e+01, -3.13954e+02, -4.57886e+02,  8.58241e+01,\n",
       "       -3.02162e+02, -4.42757e+02,  1.03148e+02, -2.82183e+02,\n",
       "        1.86895e-01,  6.48663e-02,  1.07620e-01])"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler.data_min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "NOWq0Fev1RI-",
   "metadata": {
    "id": "NOWq0Fev1RI-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "feature_computation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
