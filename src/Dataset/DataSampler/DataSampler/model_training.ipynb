{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "feature_computation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "caring-species"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
        "# dir_path = './' #local\n",
        "dir_path = './drive/MyDrive/BAKA/' #colab"
      ],
      "id": "caring-species",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "KstfPbJfzIAV",
        "outputId": "eb6621b0-b46d-4449-d3a0-81a34fe927e1"
      },
      "source": [
        "#uncomment in colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# pd.read_csv(dir_path + 'DataCollection' + '/' + '1' + '/' + '0.txt', sep=' ', index_col=False )\n",
        "# test = np.genfromtxt(dir_path + 'DataCollection' + '/' + '1' + '/' + '0.txt', delimiter=' ',dtype='float64')\n",
        "df = pd.read_csv(dir_path + 'DataCollection' + '/' + '1' + '/' + '0.txt', header=None, sep=' ')\n",
        "df = df.drop(columns=[31])\n",
        "sp_df = np.array_split(df, 2)\n",
        "sp_df[0]"
      ],
      "id": "KstfPbJfzIAV",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>152.028</td>\n",
              "      <td>174.270</td>\n",
              "      <td>114.872</td>\n",
              "      <td>135.012</td>\n",
              "      <td>115.821</td>\n",
              "      <td>169.357</td>\n",
              "      <td>108.251</td>\n",
              "      <td>86.5737</td>\n",
              "      <td>86.4014</td>\n",
              "      <td>60.6628</td>\n",
              "      <td>7.49661</td>\n",
              "      <td>368.805</td>\n",
              "      <td>-11.39230</td>\n",
              "      <td>35.47780</td>\n",
              "      <td>372.833</td>\n",
              "      <td>-21.5318</td>\n",
              "      <td>39.6854</td>\n",
              "      <td>348.592</td>\n",
              "      <td>7.46019</td>\n",
              "      <td>38.7144</td>\n",
              "      <td>326.577</td>\n",
              "      <td>19.15310</td>\n",
              "      <td>37.6444</td>\n",
              "      <td>296.277</td>\n",
              "      <td>5.277580</td>\n",
              "      <td>5.69833</td>\n",
              "      <td>303.563</td>\n",
              "      <td>-4.53744</td>\n",
              "      <td>7.62196</td>\n",
              "      <td>12.33870</td>\n",
              "      <td>35.1608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>106.969</td>\n",
              "      <td>174.270</td>\n",
              "      <td>138.093</td>\n",
              "      <td>122.560</td>\n",
              "      <td>112.017</td>\n",
              "      <td>151.941</td>\n",
              "      <td>139.409</td>\n",
              "      <td>67.5126</td>\n",
              "      <td>93.4567</td>\n",
              "      <td>141.7770</td>\n",
              "      <td>25.46860</td>\n",
              "      <td>371.098</td>\n",
              "      <td>-13.88750</td>\n",
              "      <td>43.57630</td>\n",
              "      <td>381.204</td>\n",
              "      <td>-39.2998</td>\n",
              "      <td>39.0602</td>\n",
              "      <td>356.814</td>\n",
              "      <td>-7.43494</td>\n",
              "      <td>49.2235</td>\n",
              "      <td>343.591</td>\n",
              "      <td>0.97681</td>\n",
              "      <td>54.9755</td>\n",
              "      <td>295.547</td>\n",
              "      <td>-61.058000</td>\n",
              "      <td>8.98806</td>\n",
              "      <td>317.394</td>\n",
              "      <td>-4.13179</td>\n",
              "      <td>85.07790</td>\n",
              "      <td>5.64757</td>\n",
              "      <td>43.2827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>115.931</td>\n",
              "      <td>174.270</td>\n",
              "      <td>114.917</td>\n",
              "      <td>134.286</td>\n",
              "      <td>175.119</td>\n",
              "      <td>127.521</td>\n",
              "      <td>177.996</td>\n",
              "      <td>58.0159</td>\n",
              "      <td>69.8662</td>\n",
              "      <td>178.0840</td>\n",
              "      <td>49.53920</td>\n",
              "      <td>383.612</td>\n",
              "      <td>-5.43277</td>\n",
              "      <td>56.81450</td>\n",
              "      <td>405.251</td>\n",
              "      <td>-43.7347</td>\n",
              "      <td>35.5285</td>\n",
              "      <td>371.161</td>\n",
              "      <td>-20.33810</td>\n",
              "      <td>34.9854</td>\n",
              "      <td>355.458</td>\n",
              "      <td>-2.39748</td>\n",
              "      <td>82.3194</td>\n",
              "      <td>336.737</td>\n",
              "      <td>-44.982000</td>\n",
              "      <td>13.94950</td>\n",
              "      <td>337.090</td>\n",
              "      <td>-7.44972</td>\n",
              "      <td>63.64330</td>\n",
              "      <td>21.32290</td>\n",
              "      <td>29.9727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>129.774</td>\n",
              "      <td>90.000</td>\n",
              "      <td>151.379</td>\n",
              "      <td>164.207</td>\n",
              "      <td>147.675</td>\n",
              "      <td>133.193</td>\n",
              "      <td>153.299</td>\n",
              "      <td>53.3455</td>\n",
              "      <td>64.2682</td>\n",
              "      <td>55.6463</td>\n",
              "      <td>68.55800</td>\n",
              "      <td>397.308</td>\n",
              "      <td>-25.93310</td>\n",
              "      <td>57.00650</td>\n",
              "      <td>390.412</td>\n",
              "      <td>-74.1331</td>\n",
              "      <td>42.2847</td>\n",
              "      <td>387.940</td>\n",
              "      <td>-17.66460</td>\n",
              "      <td>43.7154</td>\n",
              "      <td>368.581</td>\n",
              "      <td>6.15326</td>\n",
              "      <td>45.3346</td>\n",
              "      <td>348.796</td>\n",
              "      <td>1.379250</td>\n",
              "      <td>19.70390</td>\n",
              "      <td>370.834</td>\n",
              "      <td>-9.76106</td>\n",
              "      <td>19.74900</td>\n",
              "      <td>17.69820</td>\n",
              "      <td>71.4496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>168.765</td>\n",
              "      <td>174.270</td>\n",
              "      <td>156.177</td>\n",
              "      <td>141.628</td>\n",
              "      <td>146.640</td>\n",
              "      <td>128.245</td>\n",
              "      <td>177.981</td>\n",
              "      <td>80.4555</td>\n",
              "      <td>66.8833</td>\n",
              "      <td>71.7721</td>\n",
              "      <td>72.51100</td>\n",
              "      <td>366.007</td>\n",
              "      <td>-19.21010</td>\n",
              "      <td>35.86220</td>\n",
              "      <td>344.209</td>\n",
              "      <td>-96.2058</td>\n",
              "      <td>65.4289</td>\n",
              "      <td>368.487</td>\n",
              "      <td>-5.98384</td>\n",
              "      <td>51.3009</td>\n",
              "      <td>373.351</td>\n",
              "      <td>7.91760</td>\n",
              "      <td>45.8886</td>\n",
              "      <td>362.047</td>\n",
              "      <td>13.636300</td>\n",
              "      <td>27.27890</td>\n",
              "      <td>388.642</td>\n",
              "      <td>-8.96836</td>\n",
              "      <td>15.10440</td>\n",
              "      <td>19.16930</td>\n",
              "      <td>97.3733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>155.025</td>\n",
              "      <td>173.721</td>\n",
              "      <td>137.026</td>\n",
              "      <td>141.175</td>\n",
              "      <td>142.413</td>\n",
              "      <td>144.205</td>\n",
              "      <td>171.451</td>\n",
              "      <td>71.6357</td>\n",
              "      <td>76.7435</td>\n",
              "      <td>77.4298</td>\n",
              "      <td>32.57980</td>\n",
              "      <td>385.636</td>\n",
              "      <td>-55.35270</td>\n",
              "      <td>3.71505</td>\n",
              "      <td>339.714</td>\n",
              "      <td>-94.7966</td>\n",
              "      <td>50.2944</td>\n",
              "      <td>397.548</td>\n",
              "      <td>-18.37210</td>\n",
              "      <td>54.0502</td>\n",
              "      <td>394.459</td>\n",
              "      <td>-5.88642</td>\n",
              "      <td>50.2178</td>\n",
              "      <td>384.405</td>\n",
              "      <td>1.723980</td>\n",
              "      <td>18.24680</td>\n",
              "      <td>398.022</td>\n",
              "      <td>-10.44550</td>\n",
              "      <td>15.77150</td>\n",
              "      <td>9.59649</td>\n",
              "      <td>110.1400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>156.901</td>\n",
              "      <td>173.395</td>\n",
              "      <td>136.497</td>\n",
              "      <td>140.909</td>\n",
              "      <td>141.700</td>\n",
              "      <td>144.869</td>\n",
              "      <td>170.816</td>\n",
              "      <td>71.1895</td>\n",
              "      <td>76.4634</td>\n",
              "      <td>77.2505</td>\n",
              "      <td>31.79950</td>\n",
              "      <td>386.771</td>\n",
              "      <td>-56.38500</td>\n",
              "      <td>4.50401</td>\n",
              "      <td>341.073</td>\n",
              "      <td>-95.1066</td>\n",
              "      <td>49.8712</td>\n",
              "      <td>398.885</td>\n",
              "      <td>-19.14140</td>\n",
              "      <td>54.1542</td>\n",
              "      <td>395.603</td>\n",
              "      <td>-6.63851</td>\n",
              "      <td>50.7962</td>\n",
              "      <td>385.537</td>\n",
              "      <td>0.938872</td>\n",
              "      <td>18.37180</td>\n",
              "      <td>398.392</td>\n",
              "      <td>-10.57410</td>\n",
              "      <td>15.82180</td>\n",
              "      <td>9.31662</td>\n",
              "      <td>109.5280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>158.522</td>\n",
              "      <td>173.625</td>\n",
              "      <td>136.929</td>\n",
              "      <td>141.031</td>\n",
              "      <td>141.552</td>\n",
              "      <td>145.678</td>\n",
              "      <td>170.873</td>\n",
              "      <td>71.2914</td>\n",
              "      <td>76.3473</td>\n",
              "      <td>76.4080</td>\n",
              "      <td>31.83980</td>\n",
              "      <td>387.572</td>\n",
              "      <td>-57.14600</td>\n",
              "      <td>4.90973</td>\n",
              "      <td>341.923</td>\n",
              "      <td>-95.7195</td>\n",
              "      <td>49.8744</td>\n",
              "      <td>399.897</td>\n",
              "      <td>-19.53570</td>\n",
              "      <td>54.2922</td>\n",
              "      <td>396.469</td>\n",
              "      <td>-7.09323</td>\n",
              "      <td>50.9274</td>\n",
              "      <td>386.211</td>\n",
              "      <td>0.622400</td>\n",
              "      <td>18.48320</td>\n",
              "      <td>398.925</td>\n",
              "      <td>-10.66650</td>\n",
              "      <td>15.72430</td>\n",
              "      <td>9.22443</td>\n",
              "      <td>109.0020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>158.859</td>\n",
              "      <td>174.598</td>\n",
              "      <td>137.177</td>\n",
              "      <td>140.760</td>\n",
              "      <td>141.265</td>\n",
              "      <td>147.142</td>\n",
              "      <td>171.260</td>\n",
              "      <td>71.2260</td>\n",
              "      <td>76.3613</td>\n",
              "      <td>76.1615</td>\n",
              "      <td>32.39350</td>\n",
              "      <td>388.048</td>\n",
              "      <td>-57.93710</td>\n",
              "      <td>4.57446</td>\n",
              "      <td>342.496</td>\n",
              "      <td>-95.8547</td>\n",
              "      <td>49.8242</td>\n",
              "      <td>400.669</td>\n",
              "      <td>-19.98300</td>\n",
              "      <td>54.4927</td>\n",
              "      <td>397.135</td>\n",
              "      <td>-7.47433</td>\n",
              "      <td>51.2412</td>\n",
              "      <td>386.845</td>\n",
              "      <td>0.361023</td>\n",
              "      <td>18.55280</td>\n",
              "      <td>399.310</td>\n",
              "      <td>-10.78070</td>\n",
              "      <td>15.59450</td>\n",
              "      <td>8.98942</td>\n",
              "      <td>109.1230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>159.968</td>\n",
              "      <td>175.177</td>\n",
              "      <td>138.385</td>\n",
              "      <td>139.917</td>\n",
              "      <td>140.353</td>\n",
              "      <td>148.704</td>\n",
              "      <td>171.572</td>\n",
              "      <td>71.8511</td>\n",
              "      <td>76.6114</td>\n",
              "      <td>75.7387</td>\n",
              "      <td>32.37700</td>\n",
              "      <td>389.289</td>\n",
              "      <td>-59.26810</td>\n",
              "      <td>4.84945</td>\n",
              "      <td>343.116</td>\n",
              "      <td>-95.9216</td>\n",
              "      <td>50.0713</td>\n",
              "      <td>402.129</td>\n",
              "      <td>-20.09640</td>\n",
              "      <td>54.8142</td>\n",
              "      <td>398.339</td>\n",
              "      <td>-7.88371</td>\n",
              "      <td>51.7601</td>\n",
              "      <td>387.757</td>\n",
              "      <td>0.181574</td>\n",
              "      <td>18.65410</td>\n",
              "      <td>399.774</td>\n",
              "      <td>-10.76800</td>\n",
              "      <td>15.30470</td>\n",
              "      <td>8.98171</td>\n",
              "      <td>109.3330</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows Ã— 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0        1        2        3   ...        27        28        29        30\n",
              "0   152.028  174.270  114.872  135.012  ...  -4.53744   7.62196  12.33870   35.1608\n",
              "1   106.969  174.270  138.093  122.560  ...  -4.13179  85.07790   5.64757   43.2827\n",
              "2   115.931  174.270  114.917  134.286  ...  -7.44972  63.64330  21.32290   29.9727\n",
              "3   129.774   90.000  151.379  164.207  ...  -9.76106  19.74900  17.69820   71.4496\n",
              "4   168.765  174.270  156.177  141.628  ...  -8.96836  15.10440  19.16930   97.3733\n",
              "..      ...      ...      ...      ...  ...       ...       ...       ...       ...\n",
              "95  155.025  173.721  137.026  141.175  ... -10.44550  15.77150   9.59649  110.1400\n",
              "96  156.901  173.395  136.497  140.909  ... -10.57410  15.82180   9.31662  109.5280\n",
              "97  158.522  173.625  136.929  141.031  ... -10.66650  15.72430   9.22443  109.0020\n",
              "98  158.859  174.598  137.177  140.760  ... -10.78070  15.59450   8.98942  109.1230\n",
              "99  159.968  175.177  138.385  139.917  ... -10.76800  15.30470   8.98171  109.3330\n",
              "\n",
              "[100 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "signal-friendly",
        "outputId": "2055e2ba-8a80-447f-c2d5-c2acf2e7b7de"
      },
      "source": [
        "files = os.listdir(dir_path + 'DataCollection')\n",
        "files"
      ],
      "id": "signal-friendly",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['4', '2', '6', '1', '3', '5']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "annoying-rotation",
        "outputId": "4b21af4b-5e45-4164-c12a-bbeadc9732a9"
      },
      "source": [
        "x_train = [];\n",
        "y_train = [];\n",
        "\n",
        "x_test = [];\n",
        "y_test = [];\n",
        "for i in files:\n",
        "    samples = os.listdir(dir_path + 'DataCollection' + '/' + i)\n",
        "    num_tests = int(len(samples)/5);\n",
        "    shuffle(samples, random_state = 0)\n",
        "    for k in range(0, num_tests):\n",
        "#         df = np.genfromtxt(dir_path + 'DataCollection' + '/' + i + '/' + samples[k], delimiter=' ',dtype='float64')\n",
        "        df = pd.read_csv(dir_path + 'DataCollection' + '/' + i + '/' + samples[k], header=None, sep=' ')\n",
        "        df = df.drop(columns=[31])\n",
        "        sp_df = np.array_split(df, 2)\n",
        "        for p in sp_df:\n",
        "            x_test.append(p.to_numpy())\n",
        "            y_test.append(int(i)-1);\n",
        "    \n",
        "    for k in range(num_tests, len(samples)):\n",
        "        df = pd.read_csv(dir_path + 'DataCollection' + '/' + i + '/' + samples[k], header=None, sep=' ')\n",
        "        df = df.drop(columns=[31])\n",
        "        sp_df = np.array_split(df, 2)\n",
        "        for p in sp_df:\n",
        "            x_train.append(p.to_numpy())\n",
        "            y_train.append(int(i)-1);\n",
        "    \n",
        "    print(len(samples), ' ', num_tests)\n",
        "    \n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train);\n",
        "\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)"
      ],
      "id": "annoying-rotation",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "205   41\n",
            "256   51\n",
            "151   30\n",
            "301   60\n",
            "151   30\n",
            "211   42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orange-character",
        "outputId": "523b68e9-cc97-4b5c-ff51-1d337463e529"
      },
      "source": [
        "print(\"x_train.shape: \", x_train.shape)\n",
        "print(\"y_train.shiape: \", y_train.shape)\n",
        "print(\"x_test.shape: \", x_test.shape)\n",
        "print(\"y_test.shape: \", y_test.shape)\n",
        "\n",
        "print(x_train)"
      ],
      "id": "orange-character",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train.shape:  (2042, 100, 31)\n",
            "y_train.shiape:  (2042,)\n",
            "x_test.shape:  (508, 100, 31)\n",
            "y_test.shape:  (508,)\n",
            "[[[140.263   155.047   153.056   ...  14.6581   10.9596   14.1146 ]\n",
            "  [139.707   154.679   150.799   ...  14.6448   11.2233   14.2539 ]\n",
            "  [140.28    153.484   149.955   ...  14.4216   10.9631   14.1897 ]\n",
            "  ...\n",
            "  [147.243   146.769   148.691   ...  24.7754   12.0004   19.3087 ]\n",
            "  [146.985   146.403   148.536   ...  25.2561   11.4941   19.4561 ]\n",
            "  [147.045   147.45    149.014   ...  23.664    11.4556   20.0084 ]]\n",
            "\n",
            " [[146.053   146.94    146.925   ...  22.0255   10.9032   21.6817 ]\n",
            "  [145.846   148.58    148.411   ...  20.4109   10.9894   22.0994 ]\n",
            "  [145.929   149.622   150.173   ...  20.5501   10.91     21.5744 ]\n",
            "  ...\n",
            "  [146.804   158.72    155.316   ...  21.6282   10.9693   17.1487 ]\n",
            "  [146.373   159.741   156.251   ...  22.2424   11.1586   17.4078 ]\n",
            "  [145.241   159.593   156.114   ...  23.4668   11.1882   17.5774 ]]\n",
            "\n",
            " [[143.97    157.465   154.8     ...  25.9021   11.1725   17.761  ]\n",
            "  [143.636   157.388   153.438   ...  26.4988   11.1284   17.8446 ]\n",
            "  [143.836   155.892   152.227   ...  26.6327   11.2525   17.7005 ]\n",
            "  ...\n",
            "  [160.44    164.946   161.556   ...  15.7838   13.7062   12.669  ]\n",
            "  [159.191   165.183   161.474   ...  15.3893   13.4588   13.1054 ]\n",
            "  [158.557   165.216   161.207   ...  15.3785   13.6682   13.1788 ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[147.849   112.117   172.031   ...   5.32548   8.62222  19.8686 ]\n",
            "  [147.61    110.534   170.941   ...   6.01363   9.01933  22.4083 ]\n",
            "  [147.808   112.471   166.666   ...   6.4227    9.641    23.4851 ]\n",
            "  ...\n",
            "  [153.715   138.334   165.612   ...   8.13938   7.28259  24.8031 ]\n",
            "  [153.429   139.772   166.306   ...   8.08961   7.17376  24.4588 ]\n",
            "  [153.282   141.834   166.545   ...   8.10696   7.09793  23.9074 ]]\n",
            "\n",
            " [[153.306   143.946   166.993   ...   8.12842   7.05895  23.3505 ]\n",
            "  [153.377   146.593   167.269   ...   8.22326   7.04578  23.1529 ]\n",
            "  [153.373   147.997   167.72    ...   8.21357   7.06461  23.167  ]\n",
            "  ...\n",
            "  [154.062   157.38    169.846   ...   7.52496   6.1382   18.4357 ]\n",
            "  [153.867   158.485   169.887   ...   7.57417   6.1537   18.5584 ]\n",
            "  [153.695   159.841   169.846   ...   7.51856   6.16442  18.7812 ]]\n",
            "\n",
            " [[153.519   161.061   170.064   ...   7.54072   6.25536  19.0307 ]\n",
            "  [153.338   162.239   170.146   ...   7.57623   6.26319  19.1333 ]\n",
            "  [153.207   163.569   169.955   ...   7.58657   6.28136  19.1912 ]\n",
            "  ...\n",
            "  [151.872   158.771   167.294   ...   7.5887   11.8086   16.2757 ]\n",
            "  [147.088   160.236   161.896   ...   7.34709  11.9984   16.5872 ]\n",
            "  [144.85    161.92    164.024   ...   7.31403  11.918    16.2273 ]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sensitive-cheat"
      },
      "source": [
        "def scale_data(data, min_max_scaler):\n",
        "    for i in range(len(data)):\n",
        "        data[i] = min_max_scaler.transform(data[i])\n",
        "    return data"
      ],
      "id": "sensitive-cheat",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "broke-sailing"
      },
      "source": [
        "min_max_scaler = MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "num_instances, num_time_steps, num_features = x_train.shape\n",
        "x_train = np.reshape(x_train, newshape=(-1, num_features))\n",
        "x_train = min_max_scaler.fit_transform(x_train)\n",
        "x_train = np.reshape(x_train, newshape=(num_instances, num_time_steps, num_features))\n",
        "\n",
        "x_train, y_train = shuffle(x_train, y_train, random_state=0)\n",
        "\n",
        "num_instances, num_time_steps, num_features = x_test.shape\n",
        "x_test = np.reshape(x_test, newshape=(-1, num_features))\n",
        "x_test = min_max_scaler.transform(x_test)\n",
        "x_test = np.reshape(x_test, newshape=(num_instances, num_time_steps, num_features))\n",
        "\n",
        "x_test, y_test = shuffle(x_test, y_test, random_state=0)\n"
      ],
      "id": "broke-sailing",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "discrete-kansas",
        "outputId": "dfb135b5-f9fc-4db5-cd29-ce1d12115968"
      },
      "source": [
        "# x_train = np.delete(x_train, [0,1,2,3,4,5,6,7,8], 0)\n",
        "# y_train = np.delete(y_train, [0,1,2,3,4,5,6,7,8], 0)\n",
        "print(\"x_train.shape: \", x_train.shape)\n",
        "print(\"y_train.shiape: \", y_train.shape)\n",
        "print(\"x_test.shape: \", x_test.shape)\n",
        "print(\"y_test.shape: \", y_test.shape)\n",
        "# print(y_train)\n",
        "# print(\"\\n\")\n",
        "# print(y_test)\n",
        "\n",
        "# y_train = y_train.astype('int')\n",
        "\n",
        "# print(y_train)\n"
      ],
      "id": "discrete-kansas",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train.shape:  (2042, 100, 31)\n",
            "y_train.shiape:  (2042,)\n",
            "x_test.shape:  (508, 100, 31)\n",
            "y_test.shape:  (508,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "senior-mystery",
        "outputId": "81f2dd61-f990-4bdd-b4e5-afb32cdc755b"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(units=100, input_shape=x_train.shape[1:], return_sequences=True,dtype='float64'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(100, return_sequences=True ,dtype='float64'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(100, return_sequences=True ,dtype='float64'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(100,dtype='float64'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Dense(30, activation='softmax'))\n",
        "\n",
        "model.add(Dense(6, activation='softmax',dtype='float64'))\n",
        "model.summary()"
      ],
      "id": "senior-mystery",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 100, 100)          52800     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 100, 100)          400       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 100, 100)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100, 100)          80400     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 100, 100)          400       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100, 100)          0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100, 100)          80400     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 100, 100)          400       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100, 100)          0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 606       \n",
            "=================================================================\n",
            "Total params: 296,206\n",
            "Trainable params: 295,406\n",
            "Non-trainable params: 800\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "after-sperm",
        "outputId": "8ea05251-0734-47a5-c810-b269df57e057"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(lr=0.0001, decay=1e-5)\n",
        "\n",
        "checkpoint_filepath = 'Checkpoints'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "\n",
        "gestures = model.fit(x = x_train,\n",
        "            y = y_train,\n",
        "            epochs=200,\n",
        "            validation_split=0.1, #split 10% of the trainning set for the validation set,\n",
        "            batch_size=80,\n",
        "            callbacks=[model_checkpoint_callback],\n",
        "            shuffle=True\n",
        "         )"
      ],
      "id": "after-sperm",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "23/23 [==============================] - 42s 257ms/step - loss: 1.6793 - accuracy: 0.4241 - val_loss: 1.7454 - val_accuracy: 0.2146\n",
            "Epoch 2/200\n",
            "23/23 [==============================] - 5s 203ms/step - loss: 0.3826 - accuracy: 0.8845 - val_loss: 1.6815 - val_accuracy: 0.3171\n",
            "Epoch 3/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.2610 - accuracy: 0.9316 - val_loss: 1.5989 - val_accuracy: 0.4244\n",
            "Epoch 4/200\n",
            "23/23 [==============================] - 5s 205ms/step - loss: 0.2113 - accuracy: 0.9483 - val_loss: 1.4961 - val_accuracy: 0.4780\n",
            "Epoch 5/200\n",
            "23/23 [==============================] - 5s 204ms/step - loss: 0.1627 - accuracy: 0.9574 - val_loss: 1.3390 - val_accuracy: 0.6098\n",
            "Epoch 6/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.1468 - accuracy: 0.9604 - val_loss: 1.1434 - val_accuracy: 0.7317\n",
            "Epoch 7/200\n",
            "23/23 [==============================] - 5s 205ms/step - loss: 0.1377 - accuracy: 0.9669 - val_loss: 0.9546 - val_accuracy: 0.8000\n",
            "Epoch 8/200\n",
            "23/23 [==============================] - 5s 205ms/step - loss: 0.1088 - accuracy: 0.9690 - val_loss: 0.7201 - val_accuracy: 0.9171\n",
            "Epoch 9/200\n",
            "23/23 [==============================] - 5s 204ms/step - loss: 0.0879 - accuracy: 0.9773 - val_loss: 0.5230 - val_accuracy: 0.9366\n",
            "Epoch 10/200\n",
            "23/23 [==============================] - 5s 203ms/step - loss: 0.0803 - accuracy: 0.9781 - val_loss: 0.3724 - val_accuracy: 0.9220\n",
            "Epoch 11/200\n",
            "23/23 [==============================] - 5s 205ms/step - loss: 0.0757 - accuracy: 0.9812 - val_loss: 0.2532 - val_accuracy: 0.9659\n",
            "Epoch 12/200\n",
            "23/23 [==============================] - 5s 205ms/step - loss: 0.0771 - accuracy: 0.9785 - val_loss: 0.2008 - val_accuracy: 0.9610\n",
            "Epoch 13/200\n",
            "23/23 [==============================] - 5s 204ms/step - loss: 0.0614 - accuracy: 0.9832 - val_loss: 0.1501 - val_accuracy: 0.9610\n",
            "Epoch 14/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0632 - accuracy: 0.9853 - val_loss: 0.1424 - val_accuracy: 0.9659\n",
            "Epoch 15/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0457 - accuracy: 0.9891 - val_loss: 0.1080 - val_accuracy: 0.9707\n",
            "Epoch 16/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0506 - accuracy: 0.9870 - val_loss: 0.3325 - val_accuracy: 0.8829\n",
            "Epoch 17/200\n",
            "23/23 [==============================] - 5s 209ms/step - loss: 0.0462 - accuracy: 0.9873 - val_loss: 0.0956 - val_accuracy: 0.9659\n",
            "Epoch 18/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0517 - accuracy: 0.9871 - val_loss: 0.1948 - val_accuracy: 0.9317\n",
            "Epoch 19/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0387 - accuracy: 0.9881 - val_loss: 0.0976 - val_accuracy: 0.9756\n",
            "Epoch 20/200\n",
            "23/23 [==============================] - 5s 209ms/step - loss: 0.0346 - accuracy: 0.9913 - val_loss: 0.0635 - val_accuracy: 0.9854\n",
            "Epoch 21/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0342 - accuracy: 0.9888 - val_loss: 0.0643 - val_accuracy: 0.9756\n",
            "Epoch 22/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0303 - accuracy: 0.9953 - val_loss: 0.1133 - val_accuracy: 0.9610\n",
            "Epoch 23/200\n",
            "23/23 [==============================] - 5s 209ms/step - loss: 0.0398 - accuracy: 0.9890 - val_loss: 0.0692 - val_accuracy: 0.9805\n",
            "Epoch 24/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0206 - accuracy: 0.9960 - val_loss: 0.1618 - val_accuracy: 0.9561\n",
            "Epoch 25/200\n",
            "23/23 [==============================] - 5s 209ms/step - loss: 0.0312 - accuracy: 0.9926 - val_loss: 0.1387 - val_accuracy: 0.9659\n",
            "Epoch 26/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0189 - accuracy: 0.9963 - val_loss: 0.0644 - val_accuracy: 0.9902\n",
            "Epoch 27/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0324 - accuracy: 0.9914 - val_loss: 0.0861 - val_accuracy: 0.9805\n",
            "Epoch 28/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0317 - accuracy: 0.9896 - val_loss: 0.0772 - val_accuracy: 0.9854\n",
            "Epoch 29/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0176 - accuracy: 0.9981 - val_loss: 0.0444 - val_accuracy: 0.9951\n",
            "Epoch 30/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0184 - accuracy: 0.9957 - val_loss: 0.0660 - val_accuracy: 0.9854\n",
            "Epoch 31/200\n",
            "23/23 [==============================] - 5s 205ms/step - loss: 0.0135 - accuracy: 0.9975 - val_loss: 0.0598 - val_accuracy: 0.9902\n",
            "Epoch 32/200\n",
            "23/23 [==============================] - 5s 205ms/step - loss: 0.0205 - accuracy: 0.9944 - val_loss: 0.0832 - val_accuracy: 0.9707\n",
            "Epoch 33/200\n",
            "23/23 [==============================] - 5s 205ms/step - loss: 0.0274 - accuracy: 0.9937 - val_loss: 0.1259 - val_accuracy: 0.9707\n",
            "Epoch 34/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0149 - accuracy: 0.9972 - val_loss: 0.0606 - val_accuracy: 0.9902\n",
            "Epoch 35/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0131 - accuracy: 0.9971 - val_loss: 0.0341 - val_accuracy: 0.9951\n",
            "Epoch 36/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0087 - accuracy: 0.9977 - val_loss: 0.0628 - val_accuracy: 0.9854\n",
            "Epoch 37/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0108 - accuracy: 0.9986 - val_loss: 0.0614 - val_accuracy: 0.9902\n",
            "Epoch 38/200\n",
            "23/23 [==============================] - 5s 205ms/step - loss: 0.0176 - accuracy: 0.9956 - val_loss: 0.0448 - val_accuracy: 0.9951\n",
            "Epoch 39/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0088 - accuracy: 0.9979 - val_loss: 0.0297 - val_accuracy: 0.9951\n",
            "Epoch 40/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0100 - accuracy: 0.9973 - val_loss: 0.0097 - val_accuracy: 0.9951\n",
            "Epoch 41/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0133 - accuracy: 0.9949 - val_loss: 0.0593 - val_accuracy: 0.9854\n",
            "Epoch 42/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0098 - accuracy: 0.9981 - val_loss: 0.0518 - val_accuracy: 0.9902\n",
            "Epoch 43/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0147 - accuracy: 0.9939 - val_loss: 0.0874 - val_accuracy: 0.9805\n",
            "Epoch 44/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0085 - accuracy: 0.9987 - val_loss: 0.0378 - val_accuracy: 0.9902\n",
            "Epoch 45/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0123 - accuracy: 0.9965 - val_loss: 0.0735 - val_accuracy: 0.9902\n",
            "Epoch 46/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0077 - accuracy: 0.9987 - val_loss: 0.0556 - val_accuracy: 0.9951\n",
            "Epoch 47/200\n",
            "23/23 [==============================] - 5s 209ms/step - loss: 0.0088 - accuracy: 0.9975 - val_loss: 0.1381 - val_accuracy: 0.9707\n",
            "Epoch 48/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0091 - accuracy: 0.9977 - val_loss: 0.2059 - val_accuracy: 0.9756\n",
            "Epoch 49/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0081 - accuracy: 0.9992 - val_loss: 0.0686 - val_accuracy: 0.9902\n",
            "Epoch 50/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0137 - accuracy: 0.9964 - val_loss: 0.0542 - val_accuracy: 0.9902\n",
            "Epoch 51/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0162 - accuracy: 0.9948 - val_loss: 0.1694 - val_accuracy: 0.9707\n",
            "Epoch 52/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0136 - accuracy: 0.9966 - val_loss: 0.4268 - val_accuracy: 0.9171\n",
            "Epoch 53/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0365 - accuracy: 0.9903 - val_loss: 0.0434 - val_accuracy: 0.9951\n",
            "Epoch 54/200\n",
            "23/23 [==============================] - 5s 204ms/step - loss: 0.0274 - accuracy: 0.9915 - val_loss: 0.0301 - val_accuracy: 0.9951\n",
            "Epoch 55/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0203 - accuracy: 0.9925 - val_loss: 0.0390 - val_accuracy: 0.9951\n",
            "Epoch 56/200\n",
            "23/23 [==============================] - 5s 204ms/step - loss: 0.0184 - accuracy: 0.9935 - val_loss: 0.0749 - val_accuracy: 0.9805\n",
            "Epoch 57/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0111 - accuracy: 0.9970 - val_loss: 0.0600 - val_accuracy: 0.9951\n",
            "Epoch 58/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0157 - accuracy: 0.9973 - val_loss: 0.0844 - val_accuracy: 0.9854\n",
            "Epoch 59/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0106 - accuracy: 0.9975 - val_loss: 0.0544 - val_accuracy: 0.9951\n",
            "Epoch 60/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.0589 - val_accuracy: 0.9854\n",
            "Epoch 61/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0082 - accuracy: 0.9978 - val_loss: 0.0462 - val_accuracy: 0.9951\n",
            "Epoch 62/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0049 - accuracy: 0.9991 - val_loss: 0.0534 - val_accuracy: 0.9902\n",
            "Epoch 63/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0084 - accuracy: 0.9988 - val_loss: 0.0901 - val_accuracy: 0.9805\n",
            "Epoch 64/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0165 - accuracy: 0.9950 - val_loss: 0.1055 - val_accuracy: 0.9707\n",
            "Epoch 65/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0141 - accuracy: 0.9963 - val_loss: 0.0684 - val_accuracy: 0.9854\n",
            "Epoch 66/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0068 - accuracy: 0.9977 - val_loss: 0.0551 - val_accuracy: 0.9951\n",
            "Epoch 67/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0042 - accuracy: 0.9998 - val_loss: 0.0635 - val_accuracy: 0.9902\n",
            "Epoch 68/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0066 - accuracy: 0.9997 - val_loss: 0.0498 - val_accuracy: 0.9951\n",
            "Epoch 69/200\n",
            "23/23 [==============================] - 5s 209ms/step - loss: 0.0040 - accuracy: 0.9996 - val_loss: 0.0915 - val_accuracy: 0.9756\n",
            "Epoch 70/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.0477 - val_accuracy: 0.9951\n",
            "Epoch 71/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0111 - accuracy: 0.9974 - val_loss: 0.0507 - val_accuracy: 0.9902\n",
            "Epoch 72/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0085 - accuracy: 0.9988 - val_loss: 0.0437 - val_accuracy: 0.9951\n",
            "Epoch 73/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0053 - accuracy: 0.9992 - val_loss: 0.1767 - val_accuracy: 0.9610\n",
            "Epoch 74/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0117 - accuracy: 0.9973 - val_loss: 0.0440 - val_accuracy: 0.9951\n",
            "Epoch 75/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0138 - accuracy: 0.9959 - val_loss: 0.0563 - val_accuracy: 0.9902\n",
            "Epoch 76/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0103 - accuracy: 0.9970 - val_loss: 0.1150 - val_accuracy: 0.9659\n",
            "Epoch 77/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.0673 - val_accuracy: 0.9902\n",
            "Epoch 78/200\n",
            "23/23 [==============================] - 5s 209ms/step - loss: 0.0109 - accuracy: 0.9976 - val_loss: 0.1057 - val_accuracy: 0.9805\n",
            "Epoch 79/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0104 - accuracy: 0.9966 - val_loss: 0.0594 - val_accuracy: 0.9951\n",
            "Epoch 80/200\n",
            "23/23 [==============================] - 5s 209ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.0697 - val_accuracy: 0.9805\n",
            "Epoch 81/200\n",
            "23/23 [==============================] - 5s 209ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.3804 - val_accuracy: 0.9268\n",
            "Epoch 82/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0101 - accuracy: 0.9980 - val_loss: 0.0535 - val_accuracy: 0.9951\n",
            "Epoch 83/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0525 - val_accuracy: 0.9951\n",
            "Epoch 84/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.0232 - val_accuracy: 0.9951\n",
            "Epoch 85/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.0301 - val_accuracy: 0.9951\n",
            "Epoch 86/200\n",
            "23/23 [==============================] - 5s 209ms/step - loss: 0.0022 - accuracy: 0.9999 - val_loss: 0.0410 - val_accuracy: 0.9951\n",
            "Epoch 87/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.0848 - val_accuracy: 0.9854\n",
            "Epoch 88/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0047 - accuracy: 0.9996 - val_loss: 0.0531 - val_accuracy: 0.9951\n",
            "Epoch 89/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0042 - accuracy: 0.9981 - val_loss: 0.0519 - val_accuracy: 0.9951\n",
            "Epoch 90/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.0452 - val_accuracy: 0.9951\n",
            "Epoch 91/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 0.9951\n",
            "Epoch 92/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 0.9951\n",
            "Epoch 93/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0414 - val_accuracy: 0.9951\n",
            "Epoch 94/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.0369 - val_accuracy: 0.9951\n",
            "Epoch 95/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0030 - accuracy: 0.9997 - val_loss: 0.0432 - val_accuracy: 0.9951\n",
            "Epoch 96/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0454 - val_accuracy: 0.9951\n",
            "Epoch 97/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0057 - accuracy: 0.9979 - val_loss: 0.1914 - val_accuracy: 0.9707\n",
            "Epoch 98/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0214 - accuracy: 0.9941 - val_loss: 0.1396 - val_accuracy: 0.9610\n",
            "Epoch 99/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0264 - accuracy: 0.9926 - val_loss: 0.1200 - val_accuracy: 0.9659\n",
            "Epoch 100/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0123 - accuracy: 0.9962 - val_loss: 0.0705 - val_accuracy: 0.9902\n",
            "Epoch 101/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0066 - accuracy: 0.9968 - val_loss: 0.0799 - val_accuracy: 0.9854\n",
            "Epoch 102/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0080 - accuracy: 0.9969 - val_loss: 0.0669 - val_accuracy: 0.9951\n",
            "Epoch 103/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.0752 - val_accuracy: 0.9902\n",
            "Epoch 104/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0680 - val_accuracy: 0.9902\n",
            "Epoch 105/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0026 - accuracy: 0.9998 - val_loss: 0.0887 - val_accuracy: 0.9902\n",
            "Epoch 106/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0654 - val_accuracy: 0.9902\n",
            "Epoch 107/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0551 - val_accuracy: 0.9951\n",
            "Epoch 108/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.0524 - val_accuracy: 0.9951\n",
            "Epoch 109/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0558 - val_accuracy: 0.9951\n",
            "Epoch 110/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0555 - val_accuracy: 0.9951\n",
            "Epoch 111/200\n",
            "23/23 [==============================] - 5s 205ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.0559 - val_accuracy: 0.9951\n",
            "Epoch 112/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0557 - val_accuracy: 0.9951\n",
            "Epoch 113/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0085 - accuracy: 0.9962 - val_loss: 0.0535 - val_accuracy: 0.9902\n",
            "Epoch 114/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0055 - accuracy: 0.9981 - val_loss: 0.0441 - val_accuracy: 0.9951\n",
            "Epoch 115/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0037 - accuracy: 0.9985 - val_loss: 0.0403 - val_accuracy: 0.9902\n",
            "Epoch 116/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0461 - val_accuracy: 0.9951\n",
            "Epoch 117/200\n",
            "23/23 [==============================] - 5s 205ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0476 - val_accuracy: 0.9951\n",
            "Epoch 118/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.0445 - val_accuracy: 0.9902\n",
            "Epoch 119/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0456 - val_accuracy: 0.9951\n",
            "Epoch 120/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.0647 - val_accuracy: 0.9902\n",
            "Epoch 121/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0546 - val_accuracy: 0.9902\n",
            "Epoch 122/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0665 - val_accuracy: 0.9902\n",
            "Epoch 123/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0333 - val_accuracy: 0.9951\n",
            "Epoch 124/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0294 - val_accuracy: 0.9951\n",
            "Epoch 125/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.0153 - val_accuracy: 0.9951\n",
            "Epoch 126/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0077 - val_accuracy: 0.9951\n",
            "Epoch 127/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1023 - val_accuracy: 0.9805\n",
            "Epoch 128/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.1601 - val_accuracy: 0.9512\n",
            "Epoch 129/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.0275 - val_accuracy: 0.9951\n",
            "Epoch 130/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0024 - accuracy: 0.9986 - val_loss: 0.0443 - val_accuracy: 0.9951\n",
            "Epoch 131/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0185 - accuracy: 0.9942 - val_loss: 0.0882 - val_accuracy: 0.9854\n",
            "Epoch 132/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.1035 - val_accuracy: 0.9805\n",
            "Epoch 133/200\n",
            "23/23 [==============================] - 5s 205ms/step - loss: 0.0070 - accuracy: 0.9981 - val_loss: 0.1422 - val_accuracy: 0.9610\n",
            "Epoch 134/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0486 - val_accuracy: 0.9902\n",
            "Epoch 135/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0561 - val_accuracy: 0.9902\n",
            "Epoch 136/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 9.8624e-04 - accuracy: 1.0000 - val_loss: 0.0618 - val_accuracy: 0.9902\n",
            "Epoch 137/200\n",
            "23/23 [==============================] - 5s 209ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0604 - val_accuracy: 0.9902\n",
            "Epoch 138/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.0758 - val_accuracy: 0.9854\n",
            "Epoch 139/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0559 - val_accuracy: 0.9902\n",
            "Epoch 140/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 8.8356e-04 - accuracy: 1.0000 - val_loss: 0.0456 - val_accuracy: 0.9951\n",
            "Epoch 141/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.0436 - val_accuracy: 0.9951\n",
            "Epoch 142/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0619 - val_accuracy: 0.9902\n",
            "Epoch 143/200\n",
            "23/23 [==============================] - 5s 205ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.0701 - val_accuracy: 0.9854\n",
            "Epoch 144/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0040 - accuracy: 0.9995 - val_loss: 0.0748 - val_accuracy: 0.9854\n",
            "Epoch 145/200\n",
            "23/23 [==============================] - 5s 205ms/step - loss: 0.0062 - accuracy: 0.9990 - val_loss: 0.0504 - val_accuracy: 0.9951\n",
            "Epoch 146/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0033 - accuracy: 0.9985 - val_loss: 0.0423 - val_accuracy: 0.9902\n",
            "Epoch 147/200\n",
            "23/23 [==============================] - 5s 205ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0972 - val_accuracy: 0.9902\n",
            "Epoch 148/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0863 - val_accuracy: 0.9902\n",
            "Epoch 149/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 6.7851e-04 - accuracy: 1.0000 - val_loss: 0.0721 - val_accuracy: 0.9902\n",
            "Epoch 150/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 8.6867e-04 - accuracy: 1.0000 - val_loss: 0.0716 - val_accuracy: 0.9902\n",
            "Epoch 151/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0660 - val_accuracy: 0.9902\n",
            "Epoch 152/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0562 - val_accuracy: 0.9951\n",
            "Epoch 153/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0016 - accuracy: 0.9991 - val_loss: 0.0500 - val_accuracy: 0.9951\n",
            "Epoch 154/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0147 - accuracy: 0.9966 - val_loss: 0.0813 - val_accuracy: 0.9854\n",
            "Epoch 155/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0085 - accuracy: 0.9989 - val_loss: 0.0611 - val_accuracy: 0.9951\n",
            "Epoch 156/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 0.1049 - val_accuracy: 0.9854\n",
            "Epoch 157/200\n",
            "23/23 [==============================] - 5s 205ms/step - loss: 0.0133 - accuracy: 0.9976 - val_loss: 0.0529 - val_accuracy: 0.9951\n",
            "Epoch 158/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0550 - val_accuracy: 0.9951\n",
            "Epoch 159/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.0601 - val_accuracy: 0.9951\n",
            "Epoch 160/200\n",
            "23/23 [==============================] - 5s 209ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0608 - val_accuracy: 0.9951\n",
            "Epoch 161/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0091 - accuracy: 0.9963 - val_loss: 0.0351 - val_accuracy: 0.9951\n",
            "Epoch 162/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0027 - accuracy: 0.9988 - val_loss: 0.0344 - val_accuracy: 0.9902\n",
            "Epoch 163/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0365 - val_accuracy: 0.9902\n",
            "Epoch 164/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0014 - accuracy: 0.9993 - val_loss: 0.0341 - val_accuracy: 0.9902\n",
            "Epoch 165/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 9.9523e-04 - accuracy: 0.9997 - val_loss: 0.0392 - val_accuracy: 0.9902\n",
            "Epoch 166/200\n",
            "23/23 [==============================] - 5s 205ms/step - loss: 9.5949e-04 - accuracy: 1.0000 - val_loss: 0.0441 - val_accuracy: 0.9902\n",
            "Epoch 167/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 8.5275e-04 - accuracy: 1.0000 - val_loss: 0.0523 - val_accuracy: 0.9902\n",
            "Epoch 168/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0025 - accuracy: 0.9980 - val_loss: 0.0465 - val_accuracy: 0.9902\n",
            "Epoch 169/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 6.7336e-04 - accuracy: 1.0000 - val_loss: 0.0458 - val_accuracy: 0.9902\n",
            "Epoch 170/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.3856 - val_accuracy: 0.9268\n",
            "Epoch 171/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0134 - accuracy: 0.9982 - val_loss: 0.0947 - val_accuracy: 0.9854\n",
            "Epoch 172/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 0.0035 - accuracy: 0.9980 - val_loss: 0.0561 - val_accuracy: 0.9951\n",
            "Epoch 173/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0553 - val_accuracy: 0.9902\n",
            "Epoch 174/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0016 - accuracy: 0.9992 - val_loss: 0.0374 - val_accuracy: 0.9951\n",
            "Epoch 175/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0609 - val_accuracy: 0.9902\n",
            "Epoch 176/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0067 - accuracy: 0.9981 - val_loss: 0.0944 - val_accuracy: 0.9756\n",
            "Epoch 177/200\n",
            "23/23 [==============================] - 5s 209ms/step - loss: 0.0075 - accuracy: 0.9974 - val_loss: 0.0403 - val_accuracy: 0.9951\n",
            "Epoch 178/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0014 - accuracy: 0.9992 - val_loss: 0.0372 - val_accuracy: 0.9951\n",
            "Epoch 179/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0116 - accuracy: 0.9978 - val_loss: 0.0664 - val_accuracy: 0.9805\n",
            "Epoch 180/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0078 - accuracy: 0.9989 - val_loss: 0.0688 - val_accuracy: 0.9902\n",
            "Epoch 181/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.0757 - val_accuracy: 0.9902\n",
            "Epoch 182/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 6.6393e-04 - accuracy: 1.0000 - val_loss: 0.0384 - val_accuracy: 0.9951\n",
            "Epoch 183/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0494 - val_accuracy: 0.9951\n",
            "Epoch 184/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 8.4614e-04 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 0.9951\n",
            "Epoch 185/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 8.0122e-04 - accuracy: 0.9998 - val_loss: 0.0505 - val_accuracy: 0.9951\n",
            "Epoch 186/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0017 - accuracy: 0.9990 - val_loss: 0.0377 - val_accuracy: 0.9951\n",
            "Epoch 187/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 6.5017e-04 - accuracy: 1.0000 - val_loss: 0.0358 - val_accuracy: 0.9951\n",
            "Epoch 188/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 6.1118e-04 - accuracy: 1.0000 - val_loss: 0.0362 - val_accuracy: 0.9951\n",
            "Epoch 189/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 5.1553e-04 - accuracy: 1.0000 - val_loss: 0.0353 - val_accuracy: 0.9951\n",
            "Epoch 190/200\n",
            "23/23 [==============================] - 5s 208ms/step - loss: 8.5702e-04 - accuracy: 0.9996 - val_loss: 0.0392 - val_accuracy: 0.9951\n",
            "Epoch 191/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0622 - val_accuracy: 0.9902\n",
            "Epoch 192/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0075 - val_accuracy: 0.9951\n",
            "Epoch 193/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 0.0024 - accuracy: 0.9991 - val_loss: 0.0502 - val_accuracy: 0.9951\n",
            "Epoch 194/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 0.0015 - accuracy: 0.9992 - val_loss: 0.0463 - val_accuracy: 0.9951\n",
            "Epoch 195/200\n",
            "23/23 [==============================] - 5s 205ms/step - loss: 3.7747e-04 - accuracy: 1.0000 - val_loss: 0.0453 - val_accuracy: 0.9951\n",
            "Epoch 196/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 9.1246e-04 - accuracy: 1.0000 - val_loss: 0.0373 - val_accuracy: 0.9951\n",
            "Epoch 197/200\n",
            "23/23 [==============================] - 5s 205ms/step - loss: 5.2745e-04 - accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 0.9951\n",
            "Epoch 198/200\n",
            "23/23 [==============================] - 5s 206ms/step - loss: 4.3513e-04 - accuracy: 1.0000 - val_loss: 0.0390 - val_accuracy: 0.9951\n",
            "Epoch 199/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 3.8253e-04 - accuracy: 1.0000 - val_loss: 0.0411 - val_accuracy: 0.9951\n",
            "Epoch 200/200\n",
            "23/23 [==============================] - 5s 207ms/step - loss: 3.1628e-04 - accuracy: 1.0000 - val_loss: 0.0414 - val_accuracy: 0.9951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KikcQb9ZpuLB"
      },
      "source": [
        "# model.save(dir_path + 'Models/gestures.h5')"
      ],
      "id": "KikcQb9ZpuLB",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "integrated-optimization",
        "outputId": "21150691-98ed-4c9a-a18a-fbc3c8bf811c"
      },
      "source": [
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(x_test, y_test, batch_size=80)\n",
        "print(\"test loss, test acc:\", results)\n",
        "model.save(dir_path + 'Models/gestures_100', save_format='tf')"
      ],
      "id": "integrated-optimization",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate on test data\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.1182 - accuracy: 0.9724\n",
            "test loss, test acc: [0.11823232471942902, 0.9724409580230713]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./drive/MyDrive/BAKA/Models/gestures_100/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./drive/MyDrive/BAKA/Models/gestures_100/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFsQ_tEXw8WX"
      },
      "source": [
        "# new_model = load_model(dir_path + 'Models/gestures.h5')\n",
        "# print(\"Evaluate on test data\")\n",
        "# results = new_model.evaluate(x_test, y_test, batch_size=80)\n",
        "# print(\"test loss, test acc:\", results)"
      ],
      "id": "hFsQ_tEXw8WX",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faWZ2lOQ09Dj",
        "outputId": "0b6161bf-00cf-41b6-df95-3272d9133f53"
      },
      "source": [
        "min_max_scaler.data_min_"
      ],
      "id": "faWZ2lOQ09Dj",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9.30245e+01,  8.99999e+01,  9.00456e+01,  9.00455e+01,\n",
              "        9.00455e+01,  6.54812e+01,  2.57840e+01,  2.77705e+01,\n",
              "        2.79286e+01,  2.04904e+01, -4.24721e+02,  6.97324e+01,\n",
              "       -2.97270e+02, -4.20951e+02,  5.95510e+01, -3.15328e+02,\n",
              "       -4.48325e+02,  6.33254e+01, -3.17749e+02, -4.46258e+02,\n",
              "        8.67906e+01, -3.13954e+02, -4.57886e+02,  8.58241e+01,\n",
              "       -3.02162e+02, -4.42757e+02,  1.03148e+02, -2.82183e+02,\n",
              "        1.86895e-01,  6.48663e-02,  1.07620e-01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOWq0Fev1RI-"
      },
      "source": [
        ""
      ],
      "id": "NOWq0Fev1RI-",
      "execution_count": 16,
      "outputs": []
    }
  ]
}