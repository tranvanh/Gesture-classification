{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "feature_computation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "caring-species"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
        "# dir_path = './' #local\n",
        "dir_path = './drive/MyDrive/BAKA/' #colab"
      ],
      "id": "caring-species",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "KstfPbJfzIAV",
        "outputId": "1393f4f5-a0a0-4f61-cd9e-435737db6735"
      },
      "source": [
        "#uncomment in colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# pd.read_csv(dir_path + 'DataCollection' + '/' + '1' + '/' + '0.txt', sep=' ', index_col=False )\n",
        "# test = np.genfromtxt(dir_path + 'DataCollection' + '/' + '1' + '/' + '0.txt', delimiter=' ',dtype='float64')\n",
        "df = pd.read_csv(dir_path + 'DataCollection' + '/' + '10' + '/' + '0.txt', header=None, sep=' ')\n",
        "df.drop(df.index[100:])\n",
        "filler = np.zeros((100-len(df), 31))\n",
        "df = df.append(pd.DataFrame(filler), ignore_index=True )\n",
        "df\n",
        "# df = df.drop(columns=[31])\n",
        "# sp_df = np.array_split(df, 2)\n",
        "# sp_df[0]"
      ],
      "id": "KstfPbJfzIAV",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>164.280661</td>\n",
              "      <td>176.325903</td>\n",
              "      <td>178.121795</td>\n",
              "      <td>173.909634</td>\n",
              "      <td>179.942562</td>\n",
              "      <td>165.068138</td>\n",
              "      <td>179.880849</td>\n",
              "      <td>169.886642</td>\n",
              "      <td>179.693632</td>\n",
              "      <td>178.665290</td>\n",
              "      <td>0.428724</td>\n",
              "      <td>-0.367061</td>\n",
              "      <td>0.589080</td>\n",
              "      <td>0.470112</td>\n",
              "      <td>-0.276933</td>\n",
              "      <td>0.597770</td>\n",
              "      <td>0.503002</td>\n",
              "      <td>-0.261926</td>\n",
              "      <td>0.597986</td>\n",
              "      <td>0.521324</td>\n",
              "      <td>-0.265650</td>\n",
              "      <td>0.589049</td>\n",
              "      <td>0.564965</td>\n",
              "      <td>-0.296812</td>\n",
              "      <td>0.587671</td>\n",
              "      <td>0.523166</td>\n",
              "      <td>-0.401421</td>\n",
              "      <td>0.608364</td>\n",
              "      <td>13.702051</td>\n",
              "      <td>7.694128</td>\n",
              "      <td>16.043218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>165.122590</td>\n",
              "      <td>175.624965</td>\n",
              "      <td>179.600487</td>\n",
              "      <td>179.520481</td>\n",
              "      <td>177.638911</td>\n",
              "      <td>166.203109</td>\n",
              "      <td>168.936561</td>\n",
              "      <td>175.217537</td>\n",
              "      <td>179.756639</td>\n",
              "      <td>179.472871</td>\n",
              "      <td>0.429849</td>\n",
              "      <td>-0.363784</td>\n",
              "      <td>0.589629</td>\n",
              "      <td>0.474212</td>\n",
              "      <td>-0.278814</td>\n",
              "      <td>0.602315</td>\n",
              "      <td>0.502064</td>\n",
              "      <td>-0.260462</td>\n",
              "      <td>0.597467</td>\n",
              "      <td>0.521769</td>\n",
              "      <td>-0.265654</td>\n",
              "      <td>0.589785</td>\n",
              "      <td>0.563702</td>\n",
              "      <td>-0.294720</td>\n",
              "      <td>0.586825</td>\n",
              "      <td>0.522370</td>\n",
              "      <td>-0.401031</td>\n",
              "      <td>0.608108</td>\n",
              "      <td>14.720415</td>\n",
              "      <td>4.666922</td>\n",
              "      <td>14.926564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>165.078781</td>\n",
              "      <td>176.016382</td>\n",
              "      <td>179.983630</td>\n",
              "      <td>179.527066</td>\n",
              "      <td>178.258843</td>\n",
              "      <td>166.794220</td>\n",
              "      <td>169.728408</td>\n",
              "      <td>179.934052</td>\n",
              "      <td>179.742772</td>\n",
              "      <td>179.466085</td>\n",
              "      <td>0.430263</td>\n",
              "      <td>-0.362915</td>\n",
              "      <td>0.589836</td>\n",
              "      <td>0.473503</td>\n",
              "      <td>-0.277637</td>\n",
              "      <td>0.601022</td>\n",
              "      <td>0.501912</td>\n",
              "      <td>-0.259693</td>\n",
              "      <td>0.597525</td>\n",
              "      <td>0.520842</td>\n",
              "      <td>-0.264959</td>\n",
              "      <td>0.588971</td>\n",
              "      <td>0.563148</td>\n",
              "      <td>-0.294356</td>\n",
              "      <td>0.586612</td>\n",
              "      <td>0.522276</td>\n",
              "      <td>-0.401130</td>\n",
              "      <td>0.608519</td>\n",
              "      <td>14.914579</td>\n",
              "      <td>3.442307</td>\n",
              "      <td>12.785423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>160.916565</td>\n",
              "      <td>172.198336</td>\n",
              "      <td>179.873084</td>\n",
              "      <td>178.794688</td>\n",
              "      <td>171.190540</td>\n",
              "      <td>169.025568</td>\n",
              "      <td>169.618909</td>\n",
              "      <td>178.304716</td>\n",
              "      <td>179.278919</td>\n",
              "      <td>176.811213</td>\n",
              "      <td>0.436674</td>\n",
              "      <td>-0.352632</td>\n",
              "      <td>0.587307</td>\n",
              "      <td>0.476525</td>\n",
              "      <td>-0.275765</td>\n",
              "      <td>0.603303</td>\n",
              "      <td>0.501905</td>\n",
              "      <td>-0.259208</td>\n",
              "      <td>0.597259</td>\n",
              "      <td>0.520186</td>\n",
              "      <td>-0.263787</td>\n",
              "      <td>0.588143</td>\n",
              "      <td>0.564566</td>\n",
              "      <td>-0.294643</td>\n",
              "      <td>0.588136</td>\n",
              "      <td>0.521309</td>\n",
              "      <td>-0.400198</td>\n",
              "      <td>0.607621</td>\n",
              "      <td>20.174759</td>\n",
              "      <td>4.338054</td>\n",
              "      <td>7.668797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>159.852724</td>\n",
              "      <td>179.822872</td>\n",
              "      <td>179.118311</td>\n",
              "      <td>179.866069</td>\n",
              "      <td>178.201757</td>\n",
              "      <td>168.584125</td>\n",
              "      <td>175.381159</td>\n",
              "      <td>179.493500</td>\n",
              "      <td>178.231357</td>\n",
              "      <td>178.974019</td>\n",
              "      <td>0.437839</td>\n",
              "      <td>-0.352005</td>\n",
              "      <td>0.587716</td>\n",
              "      <td>0.479530</td>\n",
              "      <td>-0.276220</td>\n",
              "      <td>0.605988</td>\n",
              "      <td>0.502092</td>\n",
              "      <td>-0.258766</td>\n",
              "      <td>0.597192</td>\n",
              "      <td>0.518232</td>\n",
              "      <td>-0.261848</td>\n",
              "      <td>0.585181</td>\n",
              "      <td>0.557927</td>\n",
              "      <td>-0.292103</td>\n",
              "      <td>0.584925</td>\n",
              "      <td>0.520796</td>\n",
              "      <td>-0.399540</td>\n",
              "      <td>0.607557</td>\n",
              "      <td>12.097098</td>\n",
              "      <td>5.726537</td>\n",
              "      <td>6.220917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows Ã— 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0           1           2   ...         28        29         30\n",
              "0   164.280661  176.325903  178.121795  ...  13.702051  7.694128  16.043218\n",
              "1   165.122590  175.624965  179.600487  ...  14.720415  4.666922  14.926564\n",
              "2   165.078781  176.016382  179.983630  ...  14.914579  3.442307  12.785423\n",
              "3   160.916565  172.198336  179.873084  ...  20.174759  4.338054   7.668797\n",
              "4   159.852724  179.822872  179.118311  ...  12.097098  5.726537   6.220917\n",
              "..         ...         ...         ...  ...        ...       ...        ...\n",
              "95    0.000000    0.000000    0.000000  ...   0.000000  0.000000   0.000000\n",
              "96    0.000000    0.000000    0.000000  ...   0.000000  0.000000   0.000000\n",
              "97    0.000000    0.000000    0.000000  ...   0.000000  0.000000   0.000000\n",
              "98    0.000000    0.000000    0.000000  ...   0.000000  0.000000   0.000000\n",
              "99    0.000000    0.000000    0.000000  ...   0.000000  0.000000   0.000000\n",
              "\n",
              "[100 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "signal-friendly",
        "outputId": "abd84fdd-5dfa-4c34-cd4e-bc334932654f"
      },
      "source": [
        "files = os.listdir(dir_path + 'DataCollection')\n",
        "files"
      ],
      "id": "signal-friendly",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['6', '9', '7', '10', '8', '1', '0', '5', '4', '3', '2']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "annoying-rotation",
        "outputId": "4cefeab1-5bad-47b6-edb2-b8229be47341"
      },
      "source": [
        "x_train = [];\n",
        "y_train = [];\n",
        "\n",
        "x_test = [];\n",
        "y_test = [];\n",
        "for i in files:\n",
        "    samples = os.listdir(dir_path + 'DataCollection' + '/' + i)\n",
        "    num_tests = int(len(samples)/5);\n",
        "    shuffle(samples, random_state = 0)\n",
        "    for k in range(0, num_tests):\n",
        "#         df = np.genfromtxt(dir_path + 'DataCollection' + '/' + i + '/' + samples[k], delimiter=' ',dtype='float64')\n",
        "        df = pd.read_csv(dir_path + 'DataCollection' + '/' + i + '/' + samples[k], header=None, sep=' ')\n",
        "        df = df.drop(df.index[100:])\n",
        "        if len(df) < 100:\n",
        "            filler = np.zeros((100-len(df), 31))\n",
        "            df = df.append(pd.DataFrame(filler), ignore_index=True )\n",
        "        x_test.append(df.to_numpy())\n",
        "        y_test.append(int(i));\n",
        "    \n",
        "    for k in range(num_tests, len(samples)):\n",
        "        df = pd.read_csv(dir_path + 'DataCollection' + '/' + i + '/' + samples[k], header=None, sep=' ')\n",
        "        df = df.drop(df.index[100:])\n",
        "        if len(df) < 100:\n",
        "            filler = np.zeros((100-len(df), 31))\n",
        "            df = df.append(pd.DataFrame(filler), ignore_index=True )\n",
        "        x_train.append(df.to_numpy())\n",
        "        y_train.append(int(i));\n",
        "    \n",
        "    print(len(samples), ' ', num_tests)\n",
        "    \n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train);\n",
        "\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)"
      ],
      "id": "annoying-rotation",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200   40\n",
            "200   40\n",
            "200   40\n",
            "200   40\n",
            "200   40\n",
            "602   120\n",
            "302   60\n",
            "422   84\n",
            "410   82\n",
            "302   60\n",
            "512   102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orange-character",
        "outputId": "028ccc97-a8b2-4141-b024-b2a1724ebd0a"
      },
      "source": [
        "print(\"x_train.shape: \", x_train.shape)\n",
        "print(\"y_train.shiape: \", y_train.shape)\n",
        "print(\"x_test.shape: \", x_test.shape)\n",
        "print(\"y_test.shape: \", y_test.shape)\n",
        "\n",
        "print(x_train)"
      ],
      "id": "orange-character",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train.shape:  (2842, 100, 31)\n",
            "y_train.shiape:  (2842,)\n",
            "x_test.shape:  (708, 100, 31)\n",
            "y_test.shape:  (708,)\n",
            "[[[171.05097271 179.077056   108.14802011 ...  19.93247993   8.68486513\n",
            "   143.25822471]\n",
            "  [171.03335405 178.99149018 107.83135339 ...  19.72261924   8.60300522\n",
            "   143.08174588]\n",
            "  [170.65335795 159.33471712 102.94041811 ...  19.80701946   7.45194274\n",
            "   146.13562988]\n",
            "  ...\n",
            "  [  0.           0.           0.         ...   0.           0.\n",
            "     0.        ]\n",
            "  [  0.           0.           0.         ...   0.           0.\n",
            "     0.        ]\n",
            "  [  0.           0.           0.         ...   0.           0.\n",
            "     0.        ]]\n",
            "\n",
            " [[169.0399894  178.81230926 143.30123449 ...   8.36777393  16.31071094\n",
            "   141.50138724]\n",
            "  [168.67810689 178.69997299 141.9033215  ...   7.42614716  18.02363086\n",
            "   140.09869202]\n",
            "  [168.44696217 178.73440153 140.70784505 ...   6.36973187  18.08955845\n",
            "   139.45877392]\n",
            "  ...\n",
            "  [  0.           0.           0.         ...   0.           0.\n",
            "     0.        ]\n",
            "  [  0.           0.           0.         ...   0.           0.\n",
            "     0.        ]\n",
            "  [  0.           0.           0.         ...   0.           0.\n",
            "     0.        ]]\n",
            "\n",
            " [[170.89832548 128.90090053 145.24731529 ...  28.68139558  26.72918532\n",
            "    24.06874816]\n",
            "  [167.92021297  95.01890608 151.73678217 ...   5.56027002   3.5142948\n",
            "    21.09472253]\n",
            "  [167.62141199 121.58564827 148.43114319 ...   7.15400608   2.41860527\n",
            "   132.08587817]\n",
            "  ...\n",
            "  [  0.           0.           0.         ...   0.           0.\n",
            "     0.        ]\n",
            "  [  0.           0.           0.         ...   0.           0.\n",
            "     0.        ]\n",
            "  [  0.           0.           0.         ...   0.           0.\n",
            "     0.        ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[145.722      169.604      172.903      ...  14.0268     105.085\n",
            "    20.3285    ]\n",
            "  [147.655      169.749      171.251      ...  14.3162     105.855\n",
            "    20.5465    ]\n",
            "  [148.685      170.832      174.314      ...  14.7879     105.742\n",
            "    20.9858    ]\n",
            "  ...\n",
            "  [152.837      175.458      173.663      ...  13.2498     105.51\n",
            "    24.9729    ]\n",
            "  [153.331      175.512      173.51       ...  13.4037     105.503\n",
            "    25.1409    ]\n",
            "  [153.488      175.485      174.92       ...  13.5804     106.099\n",
            "    24.9067    ]]\n",
            "\n",
            " [[153.125      175.494      177.571      ...  13.6826     106.949\n",
            "    24.4607    ]\n",
            "  [153.062      175.477      178.826      ...  13.8016     107.616\n",
            "    24.4428    ]\n",
            "  [153.006      175.347      179.817      ...  13.8575     108.592\n",
            "    23.9224    ]\n",
            "  ...\n",
            "  [149.748      173.743      176.306      ...  13.3844     106.209\n",
            "    22.9041    ]\n",
            "  [149.259      174.058      175.908      ...  13.3562     105.958\n",
            "    22.925     ]\n",
            "  [148.735      174.229      175.43       ...  13.3755     105.544\n",
            "    23.2019    ]]\n",
            "\n",
            " [[113.38       177.798      174.141      ...   8.92757    116.565\n",
            "     4.29074   ]\n",
            "  [118.875      177.896      174.097      ...   9.09763    117.355\n",
            "     4.35757   ]\n",
            "  [124.054      178.047      173.991      ...   9.25773    117.895\n",
            "     4.27354   ]\n",
            "  ...\n",
            "  [150.84       176.027      180.         ...  10.9915     113.94\n",
            "    22.7717    ]\n",
            "  [151.485      175.748      180.         ...  10.9307     113.82\n",
            "    22.5311    ]\n",
            "  [152.123      175.105      180.         ...  10.7797     114.362\n",
            "    21.993     ]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sensitive-cheat"
      },
      "source": [
        "def scale_data(data, min_max_scaler):\n",
        "    for i in range(len(data)):\n",
        "        data[i] = min_max_scaler.transform(data[i])\n",
        "    return data"
      ],
      "id": "sensitive-cheat",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "broke-sailing"
      },
      "source": [
        "min_max_scaler = MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "num_instances, num_time_steps, num_features = x_train.shape\n",
        "x_train = np.reshape(x_train, newshape=(-1, num_features))\n",
        "x_train = min_max_scaler.fit_transform(x_train)\n",
        "x_train = np.reshape(x_train, newshape=(num_instances, num_time_steps, num_features))\n",
        "\n",
        "x_train, y_train = shuffle(x_train, y_train, random_state=0)\n",
        "\n",
        "num_instances, num_time_steps, num_features = x_test.shape\n",
        "x_test = np.reshape(x_test, newshape=(-1, num_features))\n",
        "x_test = min_max_scaler.transform(x_test)\n",
        "x_test = np.reshape(x_test, newshape=(num_instances, num_time_steps, num_features))\n",
        "\n",
        "x_test, y_test = shuffle(x_test, y_test, random_state=0)\n"
      ],
      "id": "broke-sailing",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "discrete-kansas",
        "outputId": "39fd7dbe-34bf-445f-8a9d-8c6720079079"
      },
      "source": [
        "# x_train = np.delete(x_train, [0,1,2,3,4,5,6,7,8], 0)\n",
        "# y_train = np.delete(y_train, [0,1,2,3,4,5,6,7,8], 0)\n",
        "print(\"x_train.shape: \", x_train.shape)\n",
        "print(\"y_train.shiape: \", y_train.shape)\n",
        "print(\"x_test.shape: \", x_test.shape)\n",
        "print(\"y_test.shape: \", y_test.shape)\n",
        "# print(y_train)\n",
        "# print(\"\\n\")\n",
        "# print(y_test)\n",
        "\n",
        "# y_train = y_train.astype('int')\n",
        "\n",
        "# print(y_train)\n"
      ],
      "id": "discrete-kansas",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train.shape:  (2842, 100, 31)\n",
            "y_train.shiape:  (2842,)\n",
            "x_test.shape:  (708, 100, 31)\n",
            "y_test.shape:  (708,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "senior-mystery",
        "outputId": "0be79650-beb1-4a94-c950-d94fbb5d03d7"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(units=100, input_shape=x_train.shape[1:], return_sequences=True,dtype='float64'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(100, return_sequences=True ,dtype='float64'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(100, return_sequences=True ,dtype='float64'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(100,dtype='float64'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Dense(30, activation='softmax'))\n",
        "\n",
        "model.add(Dense(11, activation='softmax',dtype='float64'))\n",
        "model.summary()"
      ],
      "id": "senior-mystery",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_8 (LSTM)                (None, 100, 100)          52800     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 100, 100)          400       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 100, 100)          0         \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 100, 100)          80400     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 100, 100)          400       \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 100, 100)          0         \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 100, 100)          80400     \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 100, 100)          400       \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 100, 100)          0         \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 11)                1111      \n",
            "=================================================================\n",
            "Total params: 296,711\n",
            "Trainable params: 295,911\n",
            "Non-trainable params: 800\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "after-sperm",
        "outputId": "600e074a-cdd0-4637-b0be-d4e7cb777962"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(lr=0.0001, decay=1e-5)\n",
        "\n",
        "checkpoint_filepath = dir_path + 'Checkpoints/'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "\n",
        "gestures = model.fit(x = x_train,\n",
        "            y = y_train,\n",
        "            epochs=200,\n",
        "            validation_split=0.1, #split 10% of the trainning set for the validation set,\n",
        "            batch_size=80,\n",
        "            callbacks=[model_checkpoint_callback],\n",
        "            shuffle=True\n",
        "         )"
      ],
      "id": "after-sperm",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "32/32 [==============================] - 13s 240ms/step - loss: 2.4255 - accuracy: 0.2728 - val_loss: 2.2895 - val_accuracy: 0.4421\n",
            "Epoch 2/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.9333 - accuracy: 0.6618 - val_loss: 2.1823 - val_accuracy: 0.4877\n",
            "Epoch 3/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.7489 - accuracy: 0.7171 - val_loss: 2.0377 - val_accuracy: 0.5509\n",
            "Epoch 4/200\n",
            "32/32 [==============================] - 7s 204ms/step - loss: 0.6943 - accuracy: 0.7297 - val_loss: 1.8293 - val_accuracy: 0.6386\n",
            "Epoch 5/200\n",
            "32/32 [==============================] - 7s 204ms/step - loss: 0.6548 - accuracy: 0.7439 - val_loss: 1.5479 - val_accuracy: 0.6526\n",
            "Epoch 6/200\n",
            "32/32 [==============================] - 7s 204ms/step - loss: 0.6119 - accuracy: 0.7524 - val_loss: 1.2813 - val_accuracy: 0.6281\n",
            "Epoch 7/200\n",
            "32/32 [==============================] - 7s 204ms/step - loss: 0.5839 - accuracy: 0.7627 - val_loss: 0.9907 - val_accuracy: 0.6807\n",
            "Epoch 8/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.5906 - accuracy: 0.7519 - val_loss: 0.7803 - val_accuracy: 0.7193\n",
            "Epoch 9/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.5487 - accuracy: 0.7677 - val_loss: 0.6603 - val_accuracy: 0.7439\n",
            "Epoch 10/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.5731 - accuracy: 0.7598 - val_loss: 0.5752 - val_accuracy: 0.7579\n",
            "Epoch 11/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.5425 - accuracy: 0.7645 - val_loss: 0.5947 - val_accuracy: 0.7228\n",
            "Epoch 12/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.5272 - accuracy: 0.7780 - val_loss: 0.5196 - val_accuracy: 0.7754\n",
            "Epoch 13/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.5385 - accuracy: 0.7656 - val_loss: 0.6003 - val_accuracy: 0.7474\n",
            "Epoch 14/200\n",
            "32/32 [==============================] - 7s 204ms/step - loss: 0.5341 - accuracy: 0.7674 - val_loss: 0.5252 - val_accuracy: 0.7614\n",
            "Epoch 15/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.5516 - accuracy: 0.7572 - val_loss: 0.4586 - val_accuracy: 0.7965\n",
            "Epoch 16/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.5176 - accuracy: 0.7761 - val_loss: 0.4639 - val_accuracy: 0.7930\n",
            "Epoch 17/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.5271 - accuracy: 0.7707 - val_loss: 0.4379 - val_accuracy: 0.8105\n",
            "Epoch 18/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.5238 - accuracy: 0.7656 - val_loss: 0.4963 - val_accuracy: 0.7930\n",
            "Epoch 19/200\n",
            "32/32 [==============================] - 7s 208ms/step - loss: 0.5074 - accuracy: 0.7835 - val_loss: 0.4889 - val_accuracy: 0.7965\n",
            "Epoch 20/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.5128 - accuracy: 0.7781 - val_loss: 0.4753 - val_accuracy: 0.7895\n",
            "Epoch 21/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.4996 - accuracy: 0.7862 - val_loss: 0.4248 - val_accuracy: 0.8035\n",
            "Epoch 22/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.5033 - accuracy: 0.7796 - val_loss: 0.5166 - val_accuracy: 0.7860\n",
            "Epoch 23/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.4790 - accuracy: 0.7810 - val_loss: 0.4777 - val_accuracy: 0.7930\n",
            "Epoch 24/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.4794 - accuracy: 0.7931 - val_loss: 0.4476 - val_accuracy: 0.7825\n",
            "Epoch 25/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.4883 - accuracy: 0.7889 - val_loss: 0.3983 - val_accuracy: 0.8246\n",
            "Epoch 26/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.4633 - accuracy: 0.7994 - val_loss: 0.4151 - val_accuracy: 0.8246\n",
            "Epoch 27/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.4983 - accuracy: 0.7823 - val_loss: 0.4291 - val_accuracy: 0.8175\n",
            "Epoch 28/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.4792 - accuracy: 0.7871 - val_loss: 0.4263 - val_accuracy: 0.8105\n",
            "Epoch 29/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.5088 - accuracy: 0.7824 - val_loss: 0.3934 - val_accuracy: 0.8140\n",
            "Epoch 30/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.4564 - accuracy: 0.7996 - val_loss: 0.4102 - val_accuracy: 0.8070\n",
            "Epoch 31/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.4408 - accuracy: 0.8062 - val_loss: 0.4130 - val_accuracy: 0.7965\n",
            "Epoch 32/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.4840 - accuracy: 0.7830 - val_loss: 0.3981 - val_accuracy: 0.8070\n",
            "Epoch 33/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.4332 - accuracy: 0.8095 - val_loss: 0.5259 - val_accuracy: 0.7614\n",
            "Epoch 34/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.4179 - accuracy: 0.8247 - val_loss: 0.5408 - val_accuracy: 0.7860\n",
            "Epoch 35/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.4745 - accuracy: 0.7893 - val_loss: 0.5109 - val_accuracy: 0.8035\n",
            "Epoch 36/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.4437 - accuracy: 0.8037 - val_loss: 0.4282 - val_accuracy: 0.8175\n",
            "Epoch 37/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.4479 - accuracy: 0.8030 - val_loss: 0.4066 - val_accuracy: 0.8211\n",
            "Epoch 38/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.4137 - accuracy: 0.8180 - val_loss: 0.3844 - val_accuracy: 0.8421\n",
            "Epoch 39/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.4301 - accuracy: 0.8095 - val_loss: 0.3837 - val_accuracy: 0.8421\n",
            "Epoch 40/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.4200 - accuracy: 0.8215 - val_loss: 0.4193 - val_accuracy: 0.8281\n",
            "Epoch 41/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.4260 - accuracy: 0.8095 - val_loss: 0.4096 - val_accuracy: 0.8421\n",
            "Epoch 42/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.4057 - accuracy: 0.8230 - val_loss: 0.3965 - val_accuracy: 0.8211\n",
            "Epoch 43/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.4251 - accuracy: 0.8217 - val_loss: 0.3909 - val_accuracy: 0.8316\n",
            "Epoch 44/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.3966 - accuracy: 0.8351 - val_loss: 0.4426 - val_accuracy: 0.8246\n",
            "Epoch 45/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.4015 - accuracy: 0.8316 - val_loss: 0.4203 - val_accuracy: 0.8246\n",
            "Epoch 46/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3867 - accuracy: 0.8329 - val_loss: 0.4177 - val_accuracy: 0.8281\n",
            "Epoch 47/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.4131 - accuracy: 0.8215 - val_loss: 0.4471 - val_accuracy: 0.8351\n",
            "Epoch 48/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.3871 - accuracy: 0.8375 - val_loss: 0.3876 - val_accuracy: 0.8246\n",
            "Epoch 49/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.4059 - accuracy: 0.8261 - val_loss: 0.3958 - val_accuracy: 0.8351\n",
            "Epoch 50/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3935 - accuracy: 0.8395 - val_loss: 0.4879 - val_accuracy: 0.8211\n",
            "Epoch 51/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3980 - accuracy: 0.8301 - val_loss: 0.4154 - val_accuracy: 0.8246\n",
            "Epoch 52/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.3836 - accuracy: 0.8373 - val_loss: 0.4867 - val_accuracy: 0.8140\n",
            "Epoch 53/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.4031 - accuracy: 0.8314 - val_loss: 0.3944 - val_accuracy: 0.8526\n",
            "Epoch 54/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.3747 - accuracy: 0.8363 - val_loss: 0.4344 - val_accuracy: 0.8316\n",
            "Epoch 55/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3956 - accuracy: 0.8395 - val_loss: 0.3965 - val_accuracy: 0.8526\n",
            "Epoch 56/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3815 - accuracy: 0.8351 - val_loss: 0.3873 - val_accuracy: 0.8491\n",
            "Epoch 57/200\n",
            "32/32 [==============================] - 7s 208ms/step - loss: 0.3815 - accuracy: 0.8367 - val_loss: 0.4310 - val_accuracy: 0.8316\n",
            "Epoch 58/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3776 - accuracy: 0.8343 - val_loss: 0.4212 - val_accuracy: 0.8421\n",
            "Epoch 59/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3908 - accuracy: 0.8389 - val_loss: 0.4540 - val_accuracy: 0.8281\n",
            "Epoch 60/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.3788 - accuracy: 0.8434 - val_loss: 0.4966 - val_accuracy: 0.8211\n",
            "Epoch 61/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.4041 - accuracy: 0.8276 - val_loss: 0.4476 - val_accuracy: 0.8316\n",
            "Epoch 62/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.3641 - accuracy: 0.8517 - val_loss: 0.4203 - val_accuracy: 0.8316\n",
            "Epoch 63/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.3734 - accuracy: 0.8351 - val_loss: 0.4057 - val_accuracy: 0.8491\n",
            "Epoch 64/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.3585 - accuracy: 0.8486 - val_loss: 0.3869 - val_accuracy: 0.8526\n",
            "Epoch 65/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.3677 - accuracy: 0.8472 - val_loss: 0.4230 - val_accuracy: 0.8421\n",
            "Epoch 66/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3571 - accuracy: 0.8534 - val_loss: 0.4014 - val_accuracy: 0.8561\n",
            "Epoch 67/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3539 - accuracy: 0.8651 - val_loss: 0.4222 - val_accuracy: 0.8491\n",
            "Epoch 68/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3761 - accuracy: 0.8499 - val_loss: 0.4177 - val_accuracy: 0.8561\n",
            "Epoch 69/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3660 - accuracy: 0.8492 - val_loss: 0.3986 - val_accuracy: 0.8386\n",
            "Epoch 70/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.3576 - accuracy: 0.8514 - val_loss: 0.3844 - val_accuracy: 0.8421\n",
            "Epoch 71/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.3442 - accuracy: 0.8503 - val_loss: 0.4019 - val_accuracy: 0.8456\n",
            "Epoch 72/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3152 - accuracy: 0.8770 - val_loss: 0.4096 - val_accuracy: 0.8456\n",
            "Epoch 73/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.3428 - accuracy: 0.8580 - val_loss: 0.4000 - val_accuracy: 0.8596\n",
            "Epoch 74/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3230 - accuracy: 0.8623 - val_loss: 0.3947 - val_accuracy: 0.8561\n",
            "Epoch 75/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3221 - accuracy: 0.8696 - val_loss: 0.3846 - val_accuracy: 0.8702\n",
            "Epoch 76/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.3523 - accuracy: 0.8536 - val_loss: 0.4867 - val_accuracy: 0.8491\n",
            "Epoch 77/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.3474 - accuracy: 0.8620 - val_loss: 0.3830 - val_accuracy: 0.8561\n",
            "Epoch 78/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3267 - accuracy: 0.8621 - val_loss: 0.3778 - val_accuracy: 0.8491\n",
            "Epoch 79/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.3279 - accuracy: 0.8623 - val_loss: 0.3953 - val_accuracy: 0.8632\n",
            "Epoch 80/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3196 - accuracy: 0.8720 - val_loss: 0.3892 - val_accuracy: 0.8632\n",
            "Epoch 81/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3223 - accuracy: 0.8674 - val_loss: 0.5335 - val_accuracy: 0.8491\n",
            "Epoch 82/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3038 - accuracy: 0.8794 - val_loss: 0.4761 - val_accuracy: 0.8526\n",
            "Epoch 83/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3222 - accuracy: 0.8757 - val_loss: 0.3744 - val_accuracy: 0.8596\n",
            "Epoch 84/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.3101 - accuracy: 0.8768 - val_loss: 0.3745 - val_accuracy: 0.8702\n",
            "Epoch 85/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3427 - accuracy: 0.8634 - val_loss: 0.4059 - val_accuracy: 0.8491\n",
            "Epoch 86/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.3188 - accuracy: 0.8717 - val_loss: 0.3691 - val_accuracy: 0.8842\n",
            "Epoch 87/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.2967 - accuracy: 0.8794 - val_loss: 0.3945 - val_accuracy: 0.8561\n",
            "Epoch 88/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3003 - accuracy: 0.8773 - val_loss: 0.4236 - val_accuracy: 0.8526\n",
            "Epoch 89/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.2856 - accuracy: 0.8790 - val_loss: 0.3726 - val_accuracy: 0.8737\n",
            "Epoch 90/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3071 - accuracy: 0.8832 - val_loss: 0.5656 - val_accuracy: 0.8316\n",
            "Epoch 91/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.3182 - accuracy: 0.8764 - val_loss: 0.4910 - val_accuracy: 0.8351\n",
            "Epoch 92/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.3098 - accuracy: 0.8763 - val_loss: 0.4339 - val_accuracy: 0.8596\n",
            "Epoch 93/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.3021 - accuracy: 0.8790 - val_loss: 0.3975 - val_accuracy: 0.8491\n",
            "Epoch 94/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3024 - accuracy: 0.8787 - val_loss: 0.4475 - val_accuracy: 0.8456\n",
            "Epoch 95/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.2764 - accuracy: 0.8816 - val_loss: 0.3991 - val_accuracy: 0.8561\n",
            "Epoch 96/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.2891 - accuracy: 0.8886 - val_loss: 0.4569 - val_accuracy: 0.8351\n",
            "Epoch 97/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.3069 - accuracy: 0.8849 - val_loss: 0.4679 - val_accuracy: 0.8386\n",
            "Epoch 98/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.2889 - accuracy: 0.8827 - val_loss: 0.4785 - val_accuracy: 0.8386\n",
            "Epoch 99/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.2826 - accuracy: 0.8901 - val_loss: 0.4127 - val_accuracy: 0.8561\n",
            "Epoch 100/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.2696 - accuracy: 0.8924 - val_loss: 0.4093 - val_accuracy: 0.8667\n",
            "Epoch 101/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.2773 - accuracy: 0.8934 - val_loss: 0.3780 - val_accuracy: 0.8702\n",
            "Epoch 102/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.2723 - accuracy: 0.8920 - val_loss: 0.5274 - val_accuracy: 0.8421\n",
            "Epoch 103/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.2709 - accuracy: 0.8940 - val_loss: 0.4070 - val_accuracy: 0.8596\n",
            "Epoch 104/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.2527 - accuracy: 0.9015 - val_loss: 0.4178 - val_accuracy: 0.8632\n",
            "Epoch 105/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.2834 - accuracy: 0.8906 - val_loss: 0.4427 - val_accuracy: 0.8632\n",
            "Epoch 106/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.2804 - accuracy: 0.8849 - val_loss: 0.5093 - val_accuracy: 0.8421\n",
            "Epoch 107/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.2473 - accuracy: 0.8982 - val_loss: 0.4344 - val_accuracy: 0.8596\n",
            "Epoch 108/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.2606 - accuracy: 0.8972 - val_loss: 0.4032 - val_accuracy: 0.8596\n",
            "Epoch 109/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.2556 - accuracy: 0.9020 - val_loss: 0.4417 - val_accuracy: 0.8456\n",
            "Epoch 110/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.2635 - accuracy: 0.8969 - val_loss: 0.4082 - val_accuracy: 0.8667\n",
            "Epoch 111/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.2644 - accuracy: 0.8971 - val_loss: 0.4721 - val_accuracy: 0.8421\n",
            "Epoch 112/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.2510 - accuracy: 0.9065 - val_loss: 0.4228 - val_accuracy: 0.8632\n",
            "Epoch 113/200\n",
            "32/32 [==============================] - 7s 204ms/step - loss: 0.2550 - accuracy: 0.8995 - val_loss: 0.4182 - val_accuracy: 0.8596\n",
            "Epoch 114/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.2374 - accuracy: 0.9108 - val_loss: 0.4207 - val_accuracy: 0.8667\n",
            "Epoch 115/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.2693 - accuracy: 0.9030 - val_loss: 0.4224 - val_accuracy: 0.8772\n",
            "Epoch 116/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.2520 - accuracy: 0.9048 - val_loss: 0.4115 - val_accuracy: 0.8526\n",
            "Epoch 117/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.2254 - accuracy: 0.9180 - val_loss: 0.4652 - val_accuracy: 0.8491\n",
            "Epoch 118/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.2393 - accuracy: 0.9107 - val_loss: 0.4594 - val_accuracy: 0.8491\n",
            "Epoch 119/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.2253 - accuracy: 0.9174 - val_loss: 0.4463 - val_accuracy: 0.8596\n",
            "Epoch 120/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.2252 - accuracy: 0.9153 - val_loss: 0.4469 - val_accuracy: 0.8632\n",
            "Epoch 121/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.2025 - accuracy: 0.9212 - val_loss: 0.3895 - val_accuracy: 0.8737\n",
            "Epoch 122/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.2173 - accuracy: 0.9143 - val_loss: 0.4175 - val_accuracy: 0.8737\n",
            "Epoch 123/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.2221 - accuracy: 0.9208 - val_loss: 0.3956 - val_accuracy: 0.8667\n",
            "Epoch 124/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.2132 - accuracy: 0.9204 - val_loss: 0.4299 - val_accuracy: 0.8596\n",
            "Epoch 125/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.2102 - accuracy: 0.9217 - val_loss: 0.4507 - val_accuracy: 0.8632\n",
            "Epoch 126/200\n",
            "32/32 [==============================] - 7s 204ms/step - loss: 0.2191 - accuracy: 0.9239 - val_loss: 0.4378 - val_accuracy: 0.8702\n",
            "Epoch 127/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.2138 - accuracy: 0.9245 - val_loss: 0.4249 - val_accuracy: 0.8632\n",
            "Epoch 128/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.2212 - accuracy: 0.9214 - val_loss: 0.4347 - val_accuracy: 0.8561\n",
            "Epoch 129/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.2037 - accuracy: 0.9233 - val_loss: 0.4534 - val_accuracy: 0.8561\n",
            "Epoch 130/200\n",
            "32/32 [==============================] - 6s 203ms/step - loss: 0.2258 - accuracy: 0.9220 - val_loss: 0.4295 - val_accuracy: 0.8702\n",
            "Epoch 131/200\n",
            "32/32 [==============================] - 6s 203ms/step - loss: 0.2022 - accuracy: 0.9222 - val_loss: 0.5041 - val_accuracy: 0.8456\n",
            "Epoch 132/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.2004 - accuracy: 0.9306 - val_loss: 0.4629 - val_accuracy: 0.8596\n",
            "Epoch 133/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.1949 - accuracy: 0.9260 - val_loss: 0.4981 - val_accuracy: 0.8632\n",
            "Epoch 134/200\n",
            "32/32 [==============================] - 7s 204ms/step - loss: 0.1919 - accuracy: 0.9269 - val_loss: 0.4359 - val_accuracy: 0.8702\n",
            "Epoch 135/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.2067 - accuracy: 0.9264 - val_loss: 0.5182 - val_accuracy: 0.8596\n",
            "Epoch 136/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.1941 - accuracy: 0.9269 - val_loss: 0.4962 - val_accuracy: 0.8561\n",
            "Epoch 137/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.1912 - accuracy: 0.9315 - val_loss: 0.6086 - val_accuracy: 0.8491\n",
            "Epoch 138/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.1721 - accuracy: 0.9426 - val_loss: 0.4602 - val_accuracy: 0.8632\n",
            "Epoch 139/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.1900 - accuracy: 0.9330 - val_loss: 0.4356 - val_accuracy: 0.8667\n",
            "Epoch 140/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.1754 - accuracy: 0.9348 - val_loss: 0.4558 - val_accuracy: 0.8737\n",
            "Epoch 141/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.1698 - accuracy: 0.9413 - val_loss: 0.4149 - val_accuracy: 0.8667\n",
            "Epoch 142/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.1823 - accuracy: 0.9387 - val_loss: 0.4638 - val_accuracy: 0.8526\n",
            "Epoch 143/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.1633 - accuracy: 0.9425 - val_loss: 0.4608 - val_accuracy: 0.8596\n",
            "Epoch 144/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.1652 - accuracy: 0.9453 - val_loss: 0.5331 - val_accuracy: 0.8491\n",
            "Epoch 145/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.1512 - accuracy: 0.9484 - val_loss: 0.5255 - val_accuracy: 0.8526\n",
            "Epoch 146/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.1508 - accuracy: 0.9428 - val_loss: 0.5048 - val_accuracy: 0.8702\n",
            "Epoch 147/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.1790 - accuracy: 0.9424 - val_loss: 0.5341 - val_accuracy: 0.8667\n",
            "Epoch 148/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.1579 - accuracy: 0.9453 - val_loss: 0.4497 - val_accuracy: 0.8596\n",
            "Epoch 149/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.1609 - accuracy: 0.9438 - val_loss: 0.5375 - val_accuracy: 0.8526\n",
            "Epoch 150/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.1612 - accuracy: 0.9389 - val_loss: 0.4960 - val_accuracy: 0.8667\n",
            "Epoch 151/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.1615 - accuracy: 0.9464 - val_loss: 0.5364 - val_accuracy: 0.8632\n",
            "Epoch 152/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.1607 - accuracy: 0.9432 - val_loss: 0.4899 - val_accuracy: 0.8632\n",
            "Epoch 153/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.1530 - accuracy: 0.9447 - val_loss: 0.5133 - val_accuracy: 0.8526\n",
            "Epoch 154/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.1398 - accuracy: 0.9550 - val_loss: 0.6574 - val_accuracy: 0.8351\n",
            "Epoch 155/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.1695 - accuracy: 0.9401 - val_loss: 0.5590 - val_accuracy: 0.8526\n",
            "Epoch 156/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.1584 - accuracy: 0.9411 - val_loss: 0.4963 - val_accuracy: 0.8632\n",
            "Epoch 157/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.1418 - accuracy: 0.9478 - val_loss: 0.5490 - val_accuracy: 0.8421\n",
            "Epoch 158/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.1435 - accuracy: 0.9486 - val_loss: 0.5265 - val_accuracy: 0.8456\n",
            "Epoch 159/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.1438 - accuracy: 0.9488 - val_loss: 0.5016 - val_accuracy: 0.8596\n",
            "Epoch 160/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.1384 - accuracy: 0.9522 - val_loss: 0.5743 - val_accuracy: 0.8596\n",
            "Epoch 161/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.1246 - accuracy: 0.9528 - val_loss: 0.5608 - val_accuracy: 0.8561\n",
            "Epoch 162/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.1247 - accuracy: 0.9530 - val_loss: 0.5541 - val_accuracy: 0.8526\n",
            "Epoch 163/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.1249 - accuracy: 0.9591 - val_loss: 0.4799 - val_accuracy: 0.8632\n",
            "Epoch 164/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.1286 - accuracy: 0.9594 - val_loss: 0.5361 - val_accuracy: 0.8561\n",
            "Epoch 165/200\n",
            "32/32 [==============================] - 7s 208ms/step - loss: 0.1385 - accuracy: 0.9513 - val_loss: 0.4996 - val_accuracy: 0.8702\n",
            "Epoch 166/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.1271 - accuracy: 0.9564 - val_loss: 0.4824 - val_accuracy: 0.8632\n",
            "Epoch 167/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.1148 - accuracy: 0.9644 - val_loss: 0.5337 - val_accuracy: 0.8702\n",
            "Epoch 168/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.1130 - accuracy: 0.9586 - val_loss: 0.4925 - val_accuracy: 0.8702\n",
            "Epoch 169/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.1098 - accuracy: 0.9604 - val_loss: 0.5238 - val_accuracy: 0.8737\n",
            "Epoch 170/200\n",
            "32/32 [==============================] - 7s 208ms/step - loss: 0.0963 - accuracy: 0.9657 - val_loss: 0.5188 - val_accuracy: 0.8667\n",
            "Epoch 171/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.1158 - accuracy: 0.9583 - val_loss: 0.6364 - val_accuracy: 0.8421\n",
            "Epoch 172/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.1279 - accuracy: 0.9525 - val_loss: 0.5705 - val_accuracy: 0.8456\n",
            "Epoch 173/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.1334 - accuracy: 0.9550 - val_loss: 0.5371 - val_accuracy: 0.8561\n",
            "Epoch 174/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.1215 - accuracy: 0.9638 - val_loss: 0.6248 - val_accuracy: 0.8386\n",
            "Epoch 175/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.1062 - accuracy: 0.9622 - val_loss: 0.6257 - val_accuracy: 0.8491\n",
            "Epoch 176/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.1176 - accuracy: 0.9606 - val_loss: 0.5865 - val_accuracy: 0.8526\n",
            "Epoch 177/200\n",
            "32/32 [==============================] - 7s 208ms/step - loss: 0.0983 - accuracy: 0.9667 - val_loss: 0.5232 - val_accuracy: 0.8526\n",
            "Epoch 178/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.0873 - accuracy: 0.9763 - val_loss: 0.6099 - val_accuracy: 0.8456\n",
            "Epoch 179/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.0912 - accuracy: 0.9683 - val_loss: 0.6209 - val_accuracy: 0.8561\n",
            "Epoch 180/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.1009 - accuracy: 0.9686 - val_loss: 0.5361 - val_accuracy: 0.8737\n",
            "Epoch 181/200\n",
            "32/32 [==============================] - 7s 204ms/step - loss: 0.0934 - accuracy: 0.9728 - val_loss: 0.5800 - val_accuracy: 0.8667\n",
            "Epoch 182/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.0875 - accuracy: 0.9721 - val_loss: 0.6180 - val_accuracy: 0.8737\n",
            "Epoch 183/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.0754 - accuracy: 0.9802 - val_loss: 0.5552 - val_accuracy: 0.8702\n",
            "Epoch 184/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.0965 - accuracy: 0.9685 - val_loss: 0.5690 - val_accuracy: 0.8632\n",
            "Epoch 185/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.1163 - accuracy: 0.9622 - val_loss: 0.5683 - val_accuracy: 0.8561\n",
            "Epoch 186/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.1027 - accuracy: 0.9635 - val_loss: 0.5887 - val_accuracy: 0.8667\n",
            "Epoch 187/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.1188 - accuracy: 0.9537 - val_loss: 0.5901 - val_accuracy: 0.8561\n",
            "Epoch 188/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.1181 - accuracy: 0.9598 - val_loss: 0.5743 - val_accuracy: 0.8912\n",
            "Epoch 189/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.1058 - accuracy: 0.9689 - val_loss: 0.5840 - val_accuracy: 0.8772\n",
            "Epoch 190/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.1102 - accuracy: 0.9677 - val_loss: 0.6050 - val_accuracy: 0.8737\n",
            "Epoch 191/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.1072 - accuracy: 0.9647 - val_loss: 0.6516 - val_accuracy: 0.8421\n",
            "Epoch 192/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.0977 - accuracy: 0.9659 - val_loss: 0.6272 - val_accuracy: 0.8596\n",
            "Epoch 193/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.1102 - accuracy: 0.9653 - val_loss: 0.6424 - val_accuracy: 0.8456\n",
            "Epoch 194/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.0932 - accuracy: 0.9662 - val_loss: 0.6949 - val_accuracy: 0.8491\n",
            "Epoch 195/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.0724 - accuracy: 0.9788 - val_loss: 0.6864 - val_accuracy: 0.8667\n",
            "Epoch 196/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.0956 - accuracy: 0.9690 - val_loss: 0.6299 - val_accuracy: 0.8632\n",
            "Epoch 197/200\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 0.0742 - accuracy: 0.9790 - val_loss: 0.6570 - val_accuracy: 0.8561\n",
            "Epoch 198/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.0707 - accuracy: 0.9780 - val_loss: 0.6291 - val_accuracy: 0.8632\n",
            "Epoch 199/200\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.0737 - accuracy: 0.9802 - val_loss: 0.6185 - val_accuracy: 0.8667\n",
            "Epoch 200/200\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.0888 - accuracy: 0.9675 - val_loss: 0.6403 - val_accuracy: 0.8702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KikcQb9ZpuLB"
      },
      "source": [
        "# model.save(dir_path + 'Models/gestures.h5')"
      ],
      "id": "KikcQb9ZpuLB",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "integrated-optimization",
        "outputId": "5d35b648-f538-449c-c7cd-4aba62b7dccf"
      },
      "source": [
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(x_test, y_test, batch_size=24)\n",
        "print(\"test loss, test acc:\", results)\n",
        "model.save(dir_path + 'Models/gestures_dynamic', save_format='tf')"
      ],
      "id": "integrated-optimization",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate on test data\n",
            "9/9 [==============================] - 1s 77ms/step - loss: 0.9536 - accuracy: 0.8121\n",
            "test loss, test acc: [0.9535993933677673, 0.8121469020843506]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./drive/MyDrive/BAKA/Models/gestures_dynamic/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./drive/MyDrive/BAKA/Models/gestures_dynamic/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFsQ_tEXw8WX"
      },
      "source": [
        "# new_model = load_model(dir_path + 'Models/gestures.h5')\n",
        "# print(\"Evaluate on test data\")\n",
        "# results = new_model.evaluate(x_test, y_test, batch_size=80)\n",
        "# print(\"test loss, test acc:\", results)"
      ],
      "id": "hFsQ_tEXw8WX",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faWZ2lOQ09Dj",
        "outputId": "996ff0f6-4e17-4809-abdd-64b9839be771"
      },
      "source": [
        "min_max_scaler.data_max_"
      ],
      "id": "faWZ2lOQ09Dj",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([180.        , 179.99999233, 180.        , 179.99998317,\n",
              "       179.99994529, 179.999     , 179.99997165, 180.        ,\n",
              "       179.99994445, 179.99998066, 404.416     , 719.193     ,\n",
              "       570.106     , 433.43      , 778.684     , 578.49      ,\n",
              "       423.871     , 791.146     , 551.114     , 433.991     ,\n",
              "       783.643     , 520.983     , 443.729     , 762.745     ,\n",
              "       508.462     , 383.431     , 723.459     , 526.573     ,\n",
              "       179.78636976, 179.25331623, 179.54454478])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOWq0Fev1RI-",
        "outputId": "51794362-e688-4132-a58e-2b11fa75e949",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.input"
      ],
      "id": "NOWq0Fev1RI-",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 100, 31) dtype=float64 (created by layer 'lstm_8_input')>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhIACF0-aQgh"
      },
      "source": [
        ""
      ],
      "id": "FhIACF0-aQgh",
      "execution_count": null,
      "outputs": []
    }
  ]
}