The goal of the thesis was to utilize LeapMotion sensors in relation to gesture recognition, creating a pre-trained model and using it to evaluate the capabilities of the MultiLeap library.

We explored publicly available ASL and SHREC datasets, discovered possible feature mislabeling in the ASL dataset, and discussed the dataset's suitability for our purposes. In relation to the discussion, we created a simple way of sampling our dataset with a simplified ability to detect moving sequences while sampling dynamic gestures. We created a dataset consisting of 7 static gestures (fist, 1-pointing, 2-peace sign, 3, 4, 5-open fist, pinch) and 2 dynamic gestures(swipe left, swipe right). Our dataset served the purpose but is not optimal for any benchmark evaluation. Furthermore, the dataset lacks complexity as well as the number of users used for sampling. We suggest expanding the dataset in future works with the engagement of more users, increasing the gesture set as well as its complexity.

ASL dataset, despite its mislabeling, was used for benchmarking and performance evaluation in the testing environment. Our dataset was then used for real-time deployment. Both datasets applied on 4-layered LSTM as well as 2-layered bidirectional LSTM. The 4-LSTM showed promising high results in the testing environment but did not have the desired behavior in real-time deployment due to the inability to learn dynamic gestures, while 2-layered bidirectional LSTM performed well on both fronts. 

We also explored the optimal number of layers and dropout rates for bidirectional LSTMs, resulting in having 2 layers in combination with the 0.6 dropout rate, which is an optimal compromise between accuracy and required training time. As a result, the 2-layered bidirectional LSTM achieved 89.07\% accuracy performing 5-fold cross-validation.

Using the pre-trained model, we created a demo application for debugging and experimental purposes in the form of a simple console application, which supports the connection of multiple Leap Motion sensors. Despite not having an optimal dataset, we achieved to create a responsive classifier for static gestures and dynamic gestures, suitable to be integrated into other applications. 

We have conducted several experiments to evaluate the model's performance in the real-time environment as well as evaluate the performance of the MultiLeap library by using multiple Leap Motion sensors. We explored several setups and the way they can affect Leap Motion detection. We have pointed out issues with the current MultiLeap library alongside its promising results in classifying hand gestures with challenging angles while using multiple sensors. We did not explore all possible setups there are, but it was enough to have a general idea of the current MultiLeap state. We will revisit our setups and explore more in future works with improved MultiLeap.

