
In 2018, developers from UltraLeap had released an experimental build for Leap Motion tracking software, which provided data from all connected LMCs at once. Despite having this feature, the provided tracking information for the same hand was different from each sensor due to different points of origin. This problem was solved by the MultiLeap library created by Tomáš Nováček et al. in \cite{tomasMultileap}, which merges the information from all sensors and returns a unified stream of data. The library works with the same data structures as Leap Motion's API.

\section{Alignment of the tracking data}

To align tracking data, we must first determine the position of LMCs to place them in the virtual world. This can be achieved by computing the sensor's positions, and rotations in relation to other LMCs \cite{tomasMultileap}.

\subsection{Data sampling}

The MultiLeap library allows a user to sample data using a semi-automatic sampling process. Each sample consists of 20 points from the hand –-- the points represent the center of each finger joint. 

The sampling is enabled manually, but data are sampled automatically per every Leap Motion frame, approximately 90 times per second. The general idea of automatic sampling is to calibrate sensors using data from already calibrated devices.

First, one sensor is marked as calibrated. The first marked sensor is either the first connected sensor or one selected by the user. Uncalibrated sensors start acquiring samples if the presented hand is in their field of view and at the same time in the field of view of any calibrated sensor. The pair of samples consists of the uncalibrated sensor's original data and fused data from all calibrated sensors, to which is the hand visible. Once the sensor collects enough samples, it begins to compute the optimal translation and rotation of the device. The sensor is then marked as calibrated. The process is repeated until all sensors are calibrated \cite{tomasMultileap}.


Hands will then align automatically, but it is up to the user, performing the calibration, to cover enough space of the tracking area. Therefore, it is best to have diverse samples for more accurate alignment \cite{tomasMultileap}.

Considering the tracking data, where the hand is completely still, it will not have the necessary diversity in its samples. The deviation between collected tracking data is too insignificant. If we were to move the hand across the tracking area, having it rotated in various ways in various positions, the deviation of rotations and positions will be more evident, and the calculation of the alignment more precise \cite{tomasMultileap}.

Another option for calibration is a fully manual setting, allowing a user to set the position and rotation of sensors. Values need to be calculated accurately for the alignment to have any use. The main advantage of this approach is having the possibility of tracking different parts of the tracked space with the sensors, for example, LMCs being back to each other \cite{tomasMultileap}.

The combined approach is also possible. First, making a rough calibration manually and eventually improved by the semi-automatic.

\subsection{Kabsch algorithm}

Kabsch algorithm \cite{kabsch} also known as Procrustes superimposition, was used to determine the rotation of sensors by calculating optimal rotation matrix minimizing the root mean squared deviation between two paired sets of points. The first set of points consists of merged tracking information from calibrated sensors. The second set of points consists of the tracking information from any other sensor. \cite{tomasMultileap}

The goal of the Kabsch algorithm is to compute the optimal translation rotation of P onto Q, where P and Q are sets of pair points that minimize the distance between the two sets. Both P and Q are represented as $N \times 3$ matrix. Each row consists of coordinates of every point \cite{tomasMultileap}.

\begin{equation}
    \begin{pmatrix}
        x_1 & y_1 & z_1\\
        x_2 & y_2 & z_2\\
        \vdots & \vdots & \vdots\\
        x_N & y_N & z_N
    \end{pmatrix}
\end{equation}

Coordinates of the first point are in the first row, the second point in the second row, and the $N$th point in the $N$th row.

The algorithm has two main steps, computing the optimal translation and computation of the optimal matrix.

The optimal translation can be easily found by being the offset between the averages of two sets of points. As for optimal rotation, we must first calculate the mean center of the points by subtracting the coordinates of the respective centroid from the point coordinates. The centroid $C_P$ for $P$ is computed as follows:

\begin{equation}
    {C_P = {\frac{\sum_{i=1}^{N}P_i}{N}}}
\end{equation}

The mean-center calculation of all points in P:

\begin{equation}
    {P_i = P_i - C_P}
\end{equation}

Then, the $3\times3$ cross-variance matrix between the points must be calculated as follows in matrix notation:

\begin{equation}
    {H = P^T Q}
\end{equation}

At last, we will extract the rotation from the covariance matrix using polar decomposition. The extraction can be done in more iterations, resulting in more accurate rotation calculation but requiring higher computation time in return.

\begin{algorithm}
    \caption{Kabsch algorithm}
    \label{alg:devicePositioning}
    \hspace*{\algorithmicindent} \textbf{Input}: 
        \begin{itemize}
            \item \textbf{sensors}: List of N collections of samples for N sensors
            \item \textbf{iterations}: The number of iterations of the Kabsch algorithm
        \end{itemize}
    \begin{algorithmic}[1]
        \For {$sensor=2,\ldots,N$}
            \State optimalTranslation = getAverage(referenceMatrix) - getAverage(sensorMatrix);
            \State covarianceMatrix = transpose(sensorMatrix) - referenceMatrix
            \For {$i=1,\ldots,iterations$}
                \State extractRotation(covarianceMatrix)
            \EndFor
            \State Translation and rotation of the sensor in the Unity scene
        \EndFor
    \end{algorithmic} 
\end{algorithm}

\section{Data fusion}

If multiple sensors detect the hand, the fusion algorithm is used. In most cases, not all sensors detect the hands properly. One of the yield information provided by MultiLeap library is a \textit{confidence}, a float value ranging from 0.3 to 1, which denotes the confidence level of the tracking data corresponding Leap Motion frame. The purpose of confidence level is to give more weight to tracking data from the sensor, which detects the hand better, making the tracking more accurate even if two out of three sensors would send inaccurate tracking data. The confidence level is of value 0.3 when the palm normal is in a 90\textdegree \xspace and 1 when in 0\textdegree \xspace or 180\textdegree \xspace angle to Y-axis. MultiLeap does not use the confidence of 0 because even with the occlusion of fingers and hand, the tracking data still carries some information about the hand. After few experiments, the value 0.3 was determined to be the most suitable confidence level for minimal tracking data when the palm normal is in 90\textdegree \xspace angle to the Y-axis of the sensor. The mentioned approach resulted in following equation for \textit{confidence} computation:

\begin{equation}
    {confidence = (0.283699 \times angle^2)-(0.891268 \times angle)+1}
\end{equation}

The function transfers the angle, in radians, between the palm normal and the sensor's normal to the corresponding confidence level \cite{tomasMultileap}.

The confidence level is used to give weight to data from the sensor, which detects the hand better, making the tracking more precise despite faulty data coming from other sensors.