Recurrent Neural Network (RNN) is distinguished by its memory, taking input sequence with no predetermined size. Its past predictions influence currently generated output. Thus for the same input, RNN could produce different results depending on previous inputs in the sequence.\cite{rnnDSmedium}.

{\color{red}
RNNs features make it commonly used in fields such as speech recognition, image captioning, natural language processing or language translation. Some of populars being for example Siri, Google Translate or Google Voice search.\cite{ibmrnn}

As previously mentioned RNN takes into consideration informations from previous inputs. Let us look at an idiom "feeling under the weahther", where for it to make sense, words have to be in a specific order. RNN needs to account positions of each word and use its information to predict the next word in the sequence. Each timestep represents a single word. In our case third timestep represents "the". Its hidden state holds informations of previous inputs, "feeling" and "under".\cite{ibmrnn}
}

SCHEMATIC

Figure 1.7b shows the network for each time step, i.e., at time $t$, the input $\vec{x_t}$ goes into the network to produce output $\hat{y}_t$, the next time step of the input is $x_{t+1}$ with additional input from the previous time step from the hidden state $h_{t}$. This way, the neural network looks at the current input and has the context from the previous inputs.
With this structure, recurrent units hold the past values, referred to as memory. Making it possible to work with a context in data.
\cite{rnnin6}

The recurrent unit is calculated as follows:

\begin{equation}
    {h_t = f(W_{x}x_t + W_{h}h_{t-1}+\vec{b_h})}
\end{equation}

$f()$ being the activation function, $W_x,W_h$ are weight matrixes, $x_t$ is the input, and $\vec{b_h}$ is the vector of bias parameters. Unit at time step $t=0$ is initialized to $(0,0,...,0)$. The output $\hat{y_t}$ is then calculated as:

\begin{equation}
    {\hat{y}_t = g(W_{y}h_t + \vec{b_y})}
\end{equation}

$g()$ also being an activation function, usually being softmax to ensure the output is in the desired class range. $W_y$ is the weight matrix and $\vec{b_y}$ being a vector of biases determined during the learning process.

Training RNNs uses a modified version of the backpropagation algorithm called backpropagation through time (BPTT), working by unrolling the RNN \cite{Goodfellow-et-al-2016}, calculating the losses across time steps, then updating the weights with the backpropagation algorithm. More on RNN in \cite{lipton2015critical} by Liton et al.





