Feed-forward network (FNN) was the first ANN to be invented and the simplest form of ANN. Its name comes from how the information flows through the network. Its data travels in one direction, oriented from the input layer to the output layer, without cycles.\cite{ffnbrilliant} 

FNN may or may not contain several hidden layers of various widths. By having no back-loops, FNN generally minimizes error in its prediction by using the backpropagation algorithm to update its weight values.\cite{mainTypesANN}

GRAPH

The input layer takes input data, vector $\vec{x}$, producing $\hat{y}$ at the output layer. The process of training weights
 consists of minimizing the loss function $\mathcal{L}(\hat{y},y)$, $y$ being the target output of input $\vec{x}$.\cite{lipton2015critical}

%=======================================================================================================================
\subsubsection{Backpropagation}
Backpropagation, short of backward propagation of errors, is a widely used algorithm in training FFN using gradient descent to update the weights. \cite{birlliantbackprop}

\begin{equation}
    {\vec{w}_{i+1} \leftarrow \vec{w}_i - \gamma \nabla F(\vec{w}_i)}
\end{equation}

{\color{red}
Its generalization is used for other ANNs, providing a way to calculate the gradient backward through the network. A gradient of the final layer of weights being calculated first, and the gradient of the first layer being calculated last. \cite{birlliantbackprop}
}
real number value expressing prediction incorrectness.\cite{Goodfellow-et-al-2016}
