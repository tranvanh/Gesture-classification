Feed-forward network (FNN) was the first ANN to be invented and the simplest form of ANN. Its name comes from how the information flows through the network. Its data travels in one direction, oriented from the input layer to the output layer, without cycles.\cite{ffnbrilliant} 

FNN may or may not contain several hidden layers of various widths. By having no back-loops, FNN generally minimizes error in its prediction by using the backpropagation algorithm to update its weight values.\cite{mainTypesANN}

GRAPH

The input layer takes input data, vector $\vec{x}$, producing $\hat{y}$ at the output layer. The process of training weights
 consists of minimizing the loss function $\mathcal{L}(\hat{y},y)$, $y$ being the target output of input $\vec{x}$.\cite{lipton2015critical}

%=======================================================================================================================

\subsubsection{Cost Function}
Cost function $C(\vec{w})$ is used in ANN's training process. It takes all weights and biases of an ANN as its input, in form of a vector $\vec{w}$ and calculates a single real number expressing ANN's incorrectness.\cite{Goodfellow-et-al-2016} The number is high whent the ANN performs poorly and gets lower when the ANN's output gets closer to the correct result. Main goal of training is then to minimize the cost function. 

%=======================================================================================================================
\subsubsection{Backpropagation}
Backpropagation, short of backward propagation of errors, is a widely used algorithm in training FFN using gradient descent to find a local minimum of a cost function and update ANN's weights.\cite{birlliantbackprop}

Gradient of a function, with multiple variables, gives us direction of steepest gradient ascent, which direction should we step to increase the output quickly and find the local maximum. Naturaly taking its negative will give the direction towards local minimum. 

A usual practice is to divide trainning samples into small batches of size $n$. For each sample we will calculate its gradient descent and for each batch we will calculate average gradient descent of its samples, using it to update network's weights. The average gradient descent tells us which weights should be adjusted for the ANN to get closer to the correct results.\cite{birlliantbackprop}

\begin{equation}
    {- \gamma \nabla C(\vec{w}_i) + \vec{w}_i \rightarrow \vec{w}_{i+1} }
\end{equation}

Here, $\vec{w_i}$ are weights of the network at the current state (batch), $\vec{w_{i+1}}$ are updated weights, $\gamma$ is the learning rate and $-\nabla C(\vec{w_i})$ is the gradient descent.
