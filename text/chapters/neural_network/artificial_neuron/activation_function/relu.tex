
The output of the rectified linear unit (ReLU) is defined as:\\

[MATH]\\

ReLU popularity is mainly for its computational efficiency.
[https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/]

ReLu's disadvantages appear when inputs approach zero or are negative. Causing the so-called dying ReLu problem, where the network is unable to learn.
There are many variations of ReLu to this date, e.g., Leaky ReLU, Parametric ReLU, ELU, ... 