An artificial neuron's activation function defines that neuron's output value for given inputs, commonly being f: R->R [11 M]. A significant trait of many activation functions is their differentiability, allowing them to be used for Backpropagation, ANN algorithm for training weights. Having derivative not saturating or exploding, heads towards 0 or inf, is necessary for activation functions.

For such reasons, the usage of step function or any linear function is unsuitable for ANN.

\setsecnumdepth{all}
\subsubsection{Sigmoid Function}
\input{chapters/neural_network/artificial_neuron/activation_function/sigmoid_function.tex}
%=======================================================================================================================
\subsubsection{Hyperbolic Tangent}
\input{chapters/neural_network/artificial_neuron/activation_function/hyperbolic_tangent.tex}
%=======================================================================================================================
\subsubsection{Rectified Linear Unit}
\input{chapters/neural_network/artificial_neuron/activation_function/relu.tex}
%=======================================================================================================================
\subsubsection{Softmax}
\input{chapters/neural_network/artificial_neuron/activation_function/softmax.tex}
%=======================================================================================================================